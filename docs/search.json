[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hello and welcome to my website! My name is Giulio Orazzo.\nI am a finance and data enthusiast with expertise in quantitative modeling, predictive analytics, and data anlysis. I am pursuing a MS in Quantitative Methods & Modeling at CUNY Baruch College- Zicklin School of Business and hold a B.S. in Finance at CUNY College of Staten Island. Currently, I work as a Position Control Associate (HR Data Science) at the Metropolitan Transportation Authority, where I analyze data, develop predictive models, and create dashboards. Proficient in Python, R, Power BI, VBA, DAX, and STATA, I am passionate about leveraging data-driven insights to solve complex financial and business challenges.\n\n\n\n\n\n\n\nLast Updated: Thursday 02 06, 2025 at 22:30PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "",
    "text": "Show the code\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(readr)\n  library(stringr)\n  library(scales)\n  library(ggplot2)\n  library(knitr)\n  library(DT)\n})\nif(!file.exists(\"data/mp01/nyc_payroll_export.csv\")){\n  dir.create(\"data/mp01\", showWarnings=FALSE, recursive=TRUE)\n  \n  ENDPOINT &lt;- \"https://data.cityofnewyork.us/resource/k397-673e.json\"\n  \n  if(!require(\"httr2\")) install.packages(\"httr2\")\n  library(httr2)\n  \n  if(!require(\"jsonlite\")) install.packages(\"jsonlite\")\n  library(jsonlite)\n  \n  if(!require(\"dplyr\")) install.packages(\"dplyr\")\n  library(dplyr)\n  \n  if(!require(\"readr\")) install.packages(\"readr\")\n  library(readr)\n  \n  if (!require(\"scales\")) install.packages(\"scales\")\n  library(scales)\n  \n  if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n  library(ggplot2)\n  \n  if (!require(\"knitr\")) install.packages(\"knitr\")\n  library(knitr)\n  \n  BATCH_SIZE &lt;- 50000\n  OFFSET     &lt;- 0\n  END_OF_EXPORT &lt;- FALSE\n  ALL_DATA &lt;- list()\n  \n  while(!END_OF_EXPORT){\n    cat(\"Requesting items\", OFFSET, \"to\", BATCH_SIZE + OFFSET, \"\\n\")\n    \n    req &lt;- request(ENDPOINT) |&gt;\n      req_url_query(`$limit`  = BATCH_SIZE, \n                    `$offset` = OFFSET)\n    \n    resp &lt;- req_perform(req)\n    \n    batch_data &lt;- fromJSON(resp_body_string(resp))\n    \n    ALL_DATA &lt;- c(ALL_DATA, list(batch_data))\n    \n    if(NROW(batch_data) != BATCH_SIZE){\n      END_OF_EXPORT &lt;- TRUE\n      \n      cat(\"End of Data Export Reached\\n\")\n    } else {\n      OFFSET &lt;- OFFSET + BATCH_SIZE\n    }\n  }\n  \n  ALL_DATA &lt;- bind_rows(ALL_DATA)\n  \n  cat(\"Data export complete:\", NROW(ALL_DATA), \"rows and\", NCOL(ALL_DATA), \"columns.\")\n  \n  write_csv(ALL_DATA, \"data/mp01/nyc_payroll_export.csv\")\n}\ndata &lt;- read_csv(\"C:/Users/orazz/OneDrive/Documents/STA9750-2025-SPRING/data/mp01/nyc_payroll_export.csv\")\n\ndata &lt;- data |&gt;\n  mutate(agency_name = str_to_title(agency_name),\n         last_name = str_to_title(last_name),\n         first_name = str_to_title(first_name),\n         work_location_borough = str_to_title(work_location_borough),\n         title_description = str_to_title(title_description),\n         leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30)\n  )\n#create unique key identifier\ndata&lt;- data|&gt;\n  mutate(primary_key =paste(first_name,\n                            coalesce(mid_init, \"\"),\n                            last_name)\n         )\n# switch Custodian Engineer from hourly to salary\ndata&lt;- data|&gt;\n  mutate(pay_basis = if_else(title_description == \"Custodian Engineer\" & pay_basis == \"per Hour\", \"per Annum\", pay_basis))\n\ndata &lt;- data|&gt;\n  mutate( total_pay =case_when(\n    pay_basis == \"per Hour\" ~ (regular_hours * base_salary ) + (ot_hours *(base_salary * 1.5)),\n    pay_basis == \"per Day\" ~ (base_salary *(regular_hours / 7.5)) + (ot_hours * (base_salary/7.5)*1.5),\n    pay_basis == \"per Annum\" ~ (base_salary) + (((base_salary )/ 1950) * ot_hours),\n    pay_basis == \"Prorated Annual\" ~ regular_gross_paid,\n    TRUE ~ NA_real_\n  ))"
  },
  {
    "objectID": "mp01.html#mayors-payroll-table",
    "href": "mp01.html#mayors-payroll-table",
    "title": "City Payroll Data Analysis",
    "section": "",
    "text": "Here is a table of the payroll data for Mayor Eric Adams:\n\nsource(\"Mini01.R\")\n\nRows: 6225611 Columns: 17\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (8): agency_name, last_name, first_name, mid_init, work_location_boroug...\ndbl  (8): fiscal_year, payroll_number, base_salary, regular_hours, regular_g...\ndttm (1): agency_start_date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(mayor_table)"
  },
  {
    "objectID": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "href": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which job title has the highest base rate of pay?",
    "text": "Which job title has the highest base rate of pay?\nAfter analyzing the city payroll data, the job title with the highest base rate of pay is identified as Chief Actuary. The average base annual salary for this position is $296,470.\n\n\nShow the code\nkable(highest_salary|&gt;\n        rename(\"TITLE DESCRIPTION\" = title_description ,\n               \"AVERAGE SALARY\" = avg_tot_pay ))\n\n\n\n\n\nTITLE DESCRIPTION\nAVERAGE SALARY\n\n\n\n\nChief Actuary\n296470.4\n\n\n\n\n\nThe job title that has instead the highest hourly rate on avarage is Member, Civilian Complaint Review Board, with an average hourly pay of $319.89.\n\n\nShow the code\nkable(highest_paid_title|&gt;\n        rename(\"TITLE DESCRIPTION\" = title_description,\n               \"AVERAGE HOURLY PAY\" = max_hourly_rate ))\n\n\n\n\n\nTITLE DESCRIPTION\nAVERAGE HOURLY PAY\n\n\n\n\nMember, Civilian Complaint Review Board\n319.8927\n\n\n\n\n\nHourly Rate Determination: The hourly rate for each job title was extracted from the payroll dataset. Standard Work Year Assumption: A standard 2000-hour work year was assumed, representing a full-time employee‚Äôs expected annual working hours."
  },
  {
    "objectID": "mp01.html#table-showing-mayor-eric-l.-adams-carreer",
    "href": "mp01.html#table-showing-mayor-eric-l.-adams-carreer",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "",
    "text": "Here you can find a table of representing the salary and the Position held by the mayor Eric L. Adams by fiscal year (2014-2024)"
  },
  {
    "objectID": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-gross-pay",
    "href": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-gross-pay",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which individual & in what year had the single highest city total gross pay?",
    "text": "Which individual & in what year had the single highest city total gross pay?\n\n\nShow the code\ndata&lt;- data|&gt; mutate(\n  gross_annual_pay = regular_gross_paid + total_ot_paid + total_other_pay\n)\nhighest_paid_employee &lt;-\n  data |&gt; group_by(fiscal_year)|&gt;\n  slice_max(gross_annual_pay , n= 1)|&gt;\n  select(fiscal_year , title_description, agency_name , first_name, mid_init, last_name, gross_annual_pay)|&gt;\n  ungroup()|&gt;\n  slice_max(gross_annual_pay , n=1)\n\n\nIn this analysis, gross annual pay was used as the basis for identifying the employee with the highest earnings. Gross annual pay includes base salary , overtime paid , additional compensation such as bonuses, overtime, and other forms of variable pay.\nThis differs from the annualized base rate pay, which only accounts for the fixed salary or base compensation, excluding additional earnings like bonuses and overtime.\nThe employee with the highest gross annual pay is Mark K. Tettonis, who holds the position of Chief Marine Engineer at the Department Of Transportation with a gross annual salary of $1,689,518 in 2024"
  },
  {
    "objectID": "mp01.html#which-individual-worked-the-most-overtime-hours",
    "href": "mp01.html#which-individual-worked-the-most-overtime-hours",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which individual worked the most overtime hours ?",
    "text": "Which individual worked the most overtime hours ?\n\n\nShow the code\novertime&lt;- data|&gt;\n  filter(!is.na(last_name))|&gt;\n  group_by(agency_name , last_name, first_name, mid_init, title_description)|&gt;\n  summarize(tot_ot = sum(ot_hours , na.rm = TRUE))|&gt;\n  ungroup()|&gt;\n  slice_max(tot_ot, n=1)\n\n\n\n\nShow the code\nkable(overtime|&gt;\n        rename(\"FIRST NAME\" = first_name,\n               \"LAST NAME\" = last_name,\n               \"MIDDLE INITIAL\" = mid_init,\n               \"AGENCY NAME\" = agency_name,\n               \"TITLE DESCRIPTION\" = title_description,\n               \"TOTAL OVERTIME HOURS\" = tot_ot\n               ))\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY NAME\nLAST NAME\nFIRST NAME\nMIDDLE INITIAL\nTITLE DESCRIPTION\nTOTAL OVERTIME HOURS\n\n\n\n\nDepartment Of Correction\nCastillo\nJohn\nNA\nCorrection Officer\n22119.74\n\n\n\n\n\nThe employee with most overtime hours is John Castillo, working for the Department Of Correction as a Correction Officer with 2.212^{4} hours or about 922 days."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll",
    "href": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which agency has the highest average total annual payroll?",
    "text": "Which agency has the highest average total annual payroll?\nHere is a list of the 5 agency that have the highest average annual payroll\n\n\nShow the code\ndata &lt;- data|&gt;mutate(aggregated_agency_name = replace(agency_name, str_detect(agency_name, \"Dept Of Ed\"), \"DEPARTMENT OF EDUCATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Community Board\"), \"COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Borough President\"), \"BOROUGH PRESIDENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"District Attorney\"), \"DISTRICT ATTORNEY\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Public Administrator\"), \"PUBLIC ADMINISTRATOR\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Sanitation\"), \"DEPARTMENT OF SANITATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Fire Department\"), \"FIRE DEPARTMENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Police Department\"), \"POLICE DEPARTMENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Health\"), \"DEPARTMENT OF HEALTH\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Transportation\"), \"DEPARTMENT OF TRANSPORTATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Public Service\"), \"PUBLIC SERVICE COMMISSION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Fire Pension Fund\"), \"FIRE PENSION FUND\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"DEPT OF ED\"), \"DEPARTMENT OF EDUCATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"PUBLIC ADMINISTRATOR\"), \"PUBLIC ADMINISTRATOR\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"STATEN ISLAND COMMUNITY BD\"), \"STATEN ISLAND COMMUNITY BD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"QUEENS COMMUNITY BOARD\"), \"QUEENS COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"MANHATTAN COMMUNITY BOARD\"), \"MANHATTAN COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"BROOKLYN COMMUNITY BOARD\"), \"BROOKLYN COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"BRONX COMMUNITY BOARD\"), \"BRONX COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"BOROUGH PRESIDENT\"), \"BOROUGH PRESIDENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, agency_name == \"BOARD OF ELECTION POLL WORKERS\", \"BOARD OF ELECTION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, agency_name == \"DEPARTMENT OF EDUCATION ADMIN\", \"DEPARTMENT OF EDUCATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, agency_name == \"DOE CUSTODIAL PAYROL\", \"DEPARTMENT OF EDUCATION\"))\nagency_payroll &lt;- data|&gt;\n  group_by(aggregated_agency_name)|&gt;\n  summarize(average_payroll = mean(gross_annual_pay , na.rm = TRUE))|&gt;\n  ungroup()|&gt;\n  slice_max(average_payroll , n = 5 )\nagency_payroll$average_payroll &lt;- dollar(agency_payroll$average_payroll)\n\n\n\n\nShow the code\nkable(agency_payroll|&gt;\n        rename(\"AGENCY NAME\" = aggregated_agency_name,\n               \"AVERAGE TOTAL PAYROLL\" = average_payroll\n               ))\n\n\n\n\n\nAGENCY NAME\nAVERAGE TOTAL PAYROLL\n\n\n\n\nOffice Of Collective Bargainin\n$105,563\n\n\nFinancial Info Svcs Agency\n$105,437\n\n\nFIRE DEPARTMENT\n$100,285\n\n\nOffice Of The Actuary\n$98,543\n\n\nMunicipal Water Fin Authority\n$92,881"
  },
  {
    "objectID": "mp01.html#which-agency-has-the-most-employees-on-payroll-each-year",
    "href": "mp01.html#which-agency-has-the-most-employees-on-payroll-each-year",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which agency has the most employees on payroll each year?",
    "text": "Which agency has the most employees on payroll each year?\n\n\nShow the code\nmost_employee &lt;- data|&gt;\n  group_by( fiscal_year , aggregated_agency_name)|&gt;\n  summarize(employee_n = n())|&gt;\n  slice_max(employee_n , n =1)|&gt;\n  arrange(desc(fiscal_year))\n\n\nThe table show the agency for each fiscal year with the most employee\n\n\nShow the code\nkable(most_employee|&gt;\n        rename(\n               \"AGENCY NAME\" = aggregated_agency_name,\n               \"FISCAL YEAR\" = fiscal_year,\n               \"NUMBER OF EMPLOYEE\" = employee_n\n               ))\n\n\n\n\n\nFISCAL YEAR\nAGENCY NAME\nNUMBER OF EMPLOYEE\n\n\n\n\n2024\nDEPARTMENT OF EDUCATION\n277082\n\n\n2023\nDEPARTMENT OF EDUCATION\n271265\n\n\n2022\nDEPARTMENT OF EDUCATION\n323651\n\n\n2021\nDEPARTMENT OF EDUCATION\n285867\n\n\n2020\nDEPARTMENT OF EDUCATION\n294765\n\n\n2019\nDEPARTMENT OF EDUCATION\n292293\n\n\n2018\nDEPARTMENT OF EDUCATION\n256953\n\n\n2017\nDEPARTMENT OF EDUCATION\n249514\n\n\n2016\nDEPARTMENT OF EDUCATION\n249465\n\n\n2015\nDEPARTMENT OF EDUCATION\n282919\n\n\n2014\nDEPARTMENT OF EDUCATION\n241224\n\n\n\n\n\nThe DEPARTMENT OF EDUCATION is the Agency with the most amount of empployee from the Years 2014-2024"
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-overtime-usage",
    "href": "mp01.html#which-agency-has-the-highest-overtime-usage",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which agency has the highest overtime usage?",
    "text": "Which agency has the highest overtime usage?\n\n\nShow the code\nagency_ot &lt;- data |&gt;\n  group_by(aggregated_agency_name) |&gt;\n  summarize(avg_ot = mean(ot_hours), avg_reg_hours = mean(regular_hours)) |&gt;\n  filter(avg_ot != 0) |&gt;\n  arrange(desc(avg_ot)) |&gt;\n  slice_max(avg_ot, n = 5) |&gt;\n  rename(\n    \"AVERAGE OVERTIME HOURS\" = avg_ot,\n    \"AVERAGE REGULAR HOURS\" = avg_reg_hours\n  )\n\n\n\n\nShow the code\nkable(agency_ot)\n\n\n\n\n\n\n\n\n\n\naggregated_agency_name\nAVERAGE OVERTIME HOURS\nAVERAGE REGULAR HOURS\n\n\n\n\nFIRE DEPARTMENT\n345.3771\n1814.047\n\n\nDepartment Of Correction\n316.5495\n1714.251\n\n\nBoard Of Election\n261.8237\n1311.668\n\n\nDEPARTMENT OF SANITATION\n223.3824\n1613.109\n\n\nPOLICE DEPARTMENT\n211.6590\n1698.734"
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs",
    "href": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "What is the average salary of employees who work outside the five boroughs?",
    "text": "What is the average salary of employees who work outside the five boroughs?\n\n\nShow the code\ntemp&lt;- data|&gt;\n  filter(!work_location_borough %in% c(\"Manhattan\", \"Queens\", \"Richmond\", \"Brooklyn\", \"Bronx\",NA))|&gt;\n  group_by(work_location_borough)|&gt;\n  summarize(avg_salary = mean(gross_annual_pay))\ntemp$avg_salary &lt;- dollar(temp$avg_salary)\n\n\n\n\nShow the code\nkable(temp|&gt; rename(\n  \"LOCATION OUTSIDE NYC\"= work_location_borough,\n  \"AVERAGE SALARY\" = avg_salary\n))\n\n\n\n\n\nLOCATION OUTSIDE NYC\nAVERAGE SALARY\n\n\n\n\nAlbany\n$86,340.55\n\n\nDelaware\n$72,016.17\n\n\nDutchess\n$98,397.71\n\n\nGreene\n$91,328.46\n\n\nNassau\n$36,998.58\n\n\nOrange\n$53,820.85\n\n\nOther\n$68,563.13\n\n\nPutnam\n$72,497.18\n\n\nSchoharie\n$75,467.09\n\n\nSullivan\n$76,914.40\n\n\nUlster\n$77,292.72\n\n\nWashington Dc\n$83,022.62\n\n\nWestchester\n$78,168.78"
  },
  {
    "objectID": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "href": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "How much has the city‚Äôs aggregate payroll grown over the past 10 years?",
    "text": "How much has the city‚Äôs aggregate payroll grown over the past 10 years?\nThe graph belows shows the trend of the city‚Äôs aggregate Payroll(in Billions $) by fiscal year.\n\n\nShow the code\naggregate_payroll &lt;- data|&gt;\n  group_by(fiscal_year)|&gt;\n  summarize(tot_payroll = sum(gross_annual_pay))\npayroll_2014 &lt;- aggregate_payroll |&gt; filter(fiscal_year == 2014)|&gt; pull(tot_payroll)\npayroll_2024 &lt;- aggregate_payroll |&gt; filter(fiscal_year == 2024)|&gt; pull(tot_payroll)\n\n\n\n\nShow the code\nggplot(aggregate_payroll, aes(x = fiscal_year, y = tot_payroll)) +\n  geom_line(color = \"darkblue\", size = 1) +  \n  geom_point(color = \"orange\", size = 3) +  \n  labs(\n    title = \"City's Aggregate Payroll Growth Over the Past 10 Years\",\n    x = \"Fiscal Year\",\n    y = \"Total Payroll $ (in Billions)\"\n  ) +\n  scale_y_continuous(labels = label_dollar(scale = 1e-9, suffix = \"B\")) +  \n  scale_x_continuous(breaks = seq(min(aggregate_payroll$fiscal_year), max(aggregate_payroll$fiscal_year), by = 1)) +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\nOver the last 10 years, we have seen an upward trend in the total payroll. The 2014 total payroll is $22,862,581,289, while the 2024 total payroll is $32,148,690,281, with an increase of $9,286,108,991 which is equal 40.62% increase since 2014."
  },
  {
    "objectID": "mp01.html#establish-a-workforce-attrition-management-strategy",
    "href": "mp01.html#establish-a-workforce-attrition-management-strategy",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Establish a Workforce Attrition Management Strategy",
    "text": "Establish a Workforce Attrition Management Strategy\n\nControl hiring to maintain a sustainable workforce size post-reduction.\nRegularly assess staffing needs to ensure that essential services are not compromised.\n\n\nEnhance Operational Efficiency Through Process Optimization:\n\nObjective: Improve service delivery and reduce operational costs.\nApproach: Adopt strategies such as streamlining procurement processes, reducing bureaucratic inefficiencies, and eliminating non-essential expenses. :contentReferenceoaicite:2\nConsiderations: Engage employees in identifying inefficiencies and encourage a culture of continuous improvement.\n\nInvest in Technology and Training:\n\nObjective: Equip the workforce with tools and skills to adapt to evolving public service demands.\nApproach: Provide training in digital literacy and data-driven decision-making, and invest in technologies that automate routine tasks.\nConsiderations: Ensure that technological advancements do not lead to significant job displacement without adequate support and retraining opportunities.\n\nMonitor and Evaluate Policy Impact:\n\nObjective: Assess the effectiveness of workforce optimization efforts.\nApproach: Establish metrics to evaluate changes in service delivery quality, employee satisfaction, and fiscal savings.\nConsiderations: Use evaluation results to make data-informed adjustments to policies and practices."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Strategic Workforce Optimization policy represents a comprehensive approach to refining government operations by thoughtfully reducing workforce size and enhancing efficiency. By implementing these recommendations, agencies can better serve the public while ensuring responsible stewardship of public resources."
  },
  {
    "objectID": "mp01.html#key-findings-1",
    "href": "mp01.html#key-findings-1",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Key Findings:",
    "text": "Key Findings:\n\nCompensation Disparities: Positions such as Chief Actuary and Member of the Civilian Complaint Review Board exhibit notably high average salaries and hourly rates, suggesting potential areas for salary standardization.\nOvertime Expenditures: Many Departments incur substantial overtime costs, indicating a possible misalignment between staffing levels and workload demands.\nAgency Payroll Growth: The consistent increase in average total payrolls across agencies over the past decade necessitates a reevaluation of budgeting and resource allocation strategies.\nEmployee Distribution: The Department of Education consistently employs the highest number of staff, underscoring its pivotal role in the city‚Äôs operations and the potential impact of workforce optimization within this sector."
  },
  {
    "objectID": "mp01.html#final-reccommendations",
    "href": "mp01.html#final-reccommendations",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Final Reccommendations",
    "text": "Final Reccommendations\n\nSalary Capping: Consider capping salaries for high-ranking positions to ensure a more equitable distribution of resources.\nReduce Overtime: Implement measures to reduce overtime by increasing staffing in critical roles, such as emergency medical technicians and police officers.\nStrategic Workforce Optimization  : refining government operations by thoughtfully reducing workforce size and enhancing efficiency."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Creating the Ultimate Playlist",
    "section": "",
    "text": "Introduction\nData Acquisition\nInitial Exploration\nIdentifying Characteristics of Popular Songs\nBuilding a Playlist\nFinding Related Songs\nFinal Playlist"
  },
  {
    "objectID": "mp03.html#table-of-contents",
    "href": "mp03.html#table-of-contents",
    "title": "Creating the Ultimate Playlist",
    "section": "",
    "text": "Introduction\nData Acquisition\nInitial Exploration\nIdentifying Characteristics of Popular Songs\nBuilding a Playlist\nFinding Related Songs\nFinal Playlist"
  },
  {
    "objectID": "mp03.html#how-many-distinct-tracks-and-artists-are-represented-in-the-playlist-data",
    "href": "mp03.html#how-many-distinct-tracks-and-artists-are-represented-in-the-playlist-data",
    "title": "Creating the Ultimate Playlist",
    "section": "How many distinct tracks and artists are represented in the playlist data?",
    "text": "How many distinct tracks and artists are represented in the playlist data?\n\n\nShow the code\ndist_artist &lt;- rectangular_tracks_df|&gt; summarise(\n  distinct_count = n_distinct(artist_name)\n)\n\ndist_track &lt;- rectangular_tracks_df|&gt; summarise(\n  distinct_count = n_distinct(track_name)\n)\n\n\n\nThere are 30049 tracks and 9722 artists in the playlist dataset."
  },
  {
    "objectID": "mp03.html#what-are-the-5-most-popular-tracks-in-the-playlist-data",
    "href": "mp03.html#what-are-the-5-most-popular-tracks-in-the-playlist-data",
    "title": "Creating the Ultimate Playlist",
    "section": "What are the 5 most popular tracks in the playlist data?",
    "text": "What are the 5 most popular tracks in the playlist data?\n\n\nShow the code\nmost_pop_tracks &lt;- rectangular_tracks_df|&gt;\n  group_by(track_name)|&gt;\n  summarize(\n    artist_name = first(artist_name),\n    album_name = first(album_name),\n    track_id = first(track_id),\n    count = n())|&gt;\n  arrange(desc(count))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrack Name\nArtist\nAlbum\n# of Appereance in Playlists\n\n\n\n\nCloser\nNe-Yo\nYear Of The Gentleman\n75\n\n\nOne Dance\nDrake\nViews\n55\n\n\nHUMBLE.\nKendrick Lamar\nDAMN.\n52\n\n\nRide\nCiara\nBasic Instinct\n52\n\n\nBroccoli (feat. Lil Yachty)\nDRAM\nBig Baby DRAM\n50\n\n\n\n\n\nThe table shows the 5 most popular songs, and how many time they appeared in different playlists.\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nShow the code\nsongs_df&lt;- songs_df|&gt;\n  rename(\"track_id\" = id)\n\n# joining the 2 datasets by the track ID \njoined_data &lt;- songs_df|&gt;\n  left_join(rectangular_tracks_df, by =  \"track_id\")\n\ncount_of_NA &lt;- joined_data|&gt;\n  summarize(na_count = sum(is.na(playlist_name)))\n\n#for(i in 1:5) {\n#  if most_pop_tracks$track_id[]\n#}\n\n\n\n\nAccording to the song characteristics data, what is the most ‚Äúdanceable‚Äù track? How often does it appear in a playlist?\n\n\nShow the code\n#get the most danceable song\nmost_danceable&lt;- joined_data|&gt;\n  group_by(name)|&gt;\n  arrange(desc(danceability))|&gt;\n  head(5)\n# counting number of appearences\ndance_count&lt;- rectangular_tracks_df|&gt;\n  filter(track_id == most_danceable$track_id[1])|&gt;\n  summarize(appereances = n())\n\n\n\nThe most danceable track in the dataset is Funky Cold Medina by Tone-Loc, which appears 1 time in the ‚ÄúVACATION‚Äù\n\n\n\nWhich playlist has the longest average track length?\n\n\nShow the code\nrectangular_tracks_df &lt;- rectangular_tracks_df|&gt;\n  group_by(playlist_name)|&gt;\n  mutate(avg_duration = mean(duration))|&gt;\n  ungroup()|&gt;\n  mutate(avg_duration = avg_duration / 1000,  # converting ms to seconds\n         avg_duration_min = avg_duration / 60)  \n\nlong_avg &lt;- rectangular_tracks_df|&gt;\n  slice_max(avg_duration)\n\n\n‚Äúclassical‚Äù is the playlist with longest average track lenght, with an average duration of 411 seconds , or about 7 minutes for each song.\n\n\nWhat is the most popular playlist on Spotify?\n\n\nShow the code\nmost_pop_play &lt;- rectangular_tracks_df|&gt;\n  slice_max(playlist_followers)\n\n\nThe most popular playlist on Spotify is Tangled with 1038 followers."
  },
  {
    "objectID": "mp03.html#what-is-the-most-popular-track-in-the-playlist-data-that-does-not-have-a-corresponding-entry-in-the-song-characteristics-data",
    "href": "mp03.html#what-is-the-most-popular-track-in-the-playlist-data-that-does-not-have-a-corresponding-entry-in-the-song-characteristics-data",
    "title": "Creating the Ultimate Playlist",
    "section": "What is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?",
    "text": "What is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nShow the code\nsongs_df&lt;- songs_df|&gt;\n  rename(\"track_id\" = id)\n\n# joining the 2 datasets by the track ID \njoined_data &lt;- songs_df|&gt;\n  left_join(rectangular_tracks_df, by =  \"track_id\")\n\ncount_of_NA &lt;- joined_data|&gt;\n  summarize(na_count = sum(is.na(playlist_name)))\n\n\n\nAccording to the song characteristics data, what is the most ‚Äúdanceable‚Äù track? How often does it appear in a playlist?\n\n\nWhich playlist has the longest average track length?"
  },
  {
    "objectID": "mp03.html#what-is-the-most-popular-playlist-on-spotify",
    "href": "mp03.html#what-is-the-most-popular-playlist-on-spotify",
    "title": "Creating the Ultimate Playlist",
    "section": "What is the most popular playlist on Spotify?",
    "text": "What is the most popular playlist on Spotify?\n\n\nShow the code\nmost_pop_play &lt;- rectangular_tracks_df|&gt;\n  slice_max(playlist_followers)\n\n\nThe most popular playlist on Spotify is Tangled with 1038 followers."
  },
  {
    "objectID": "mp03.html#is-the-popularity-column-correlated-with-the-number-of-playlist-appearances-if-so-to-what-degree",
    "href": "mp03.html#is-the-popularity-column-correlated-with-the-number-of-playlist-appearances-if-so-to-what-degree",
    "title": "Creating the Ultimate Playlist",
    "section": "Is the popularity column correlated with the number of playlist appearances? If so, to what degree?",
    "text": "Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n\nShow the code\n# counting playilist appearances in the inner_jointed dataset\ninner_joined_data &lt;- inner_joined_data|&gt;\n  group_by(track_id)|&gt;\n  mutate( playlist_appereance = n())|&gt;\n  ungroup()\n\n# getting the first occurence of each song\npopular_songs &lt;-inner_joined_data|&gt;\n  group_by(track_id)|&gt;\n  slice(1)|&gt;\n  arrange(desc(popularity))\n\n\n\nThe table below show the Top 5 most popular songs using Popularity Index\n\n\n\n\n\n\n\n\n\n\n\n\nSong\nArtist\nAlbum Name\nPopularity Index\n# of Playlist Appearence\n\n\n\n\ngoosebumps\nTravis Scott\nBirds In The Trap Sing McKnight\n92\n35\n\n\nPlay Date\nMelanie Martinez\nCry Baby\n91\n1\n\n\nJocelyn Flores\nXXXTENTACION\n17\n87\n11\n\n\nPerfect\nEd Sheeran\n√∑\n86\n7\n\n\nShape of You\nEd Sheeran\n√∑\n85\n30\n\n\n\n\n\n\n\nThe table below show the Top 5 most popular songs by Playlist Appereances\n\n\n\n\n\n\n\n\n\n\n\n\nSong\nArtist\nAlbum Name\nPopularity Index\n# of Playlist Appearence\n\n\n\n\nF**kin‚Äô Problems (feat. Drake, 2 Chainz & Kendrick Lamar)\nA\\(AP Rocky'        |LONG.LIVE.A\\)AP (Deluxe Version)\n76\n120\n\n\n\nChampions\nKanye West‚Äô\nChampions\n68\n120\n\n\nSucker for Pain (with Wiz Khalifa, Imagine Dragons, Logic & Ty Dolla $ign feat. X Ambassadors)\nLil Wayne‚Äô\nSucker For Pain (with Logic & Ty Dolla $ign feat. X Ambassadors)\n77\n102\n\n\nNo Problem (feat. Lil Wayne & 2 Chainz)\nChance the Rapper‚Äô\nColoring Book\n73\n93\n\n\nCloser\nThe Chainsmokers‚Äô\nCloser\n84\n92\n\n\n\n\n\n\n\nShow the code\npopular_songs|&gt; ggplot(\n  aes(x = popularity , \n      y = playlist_appereance)) +\n  geom_point(color = \"darkblue\", size = 2) +\n  labs(\n    title = \"Popularity VS Playlist Appereance\",\n    x = \"Popularity\",\n    y = \"Playlist Appereance\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y = element_text(margin = margin(r = 15)),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 16),\n    plot.title = element_text(size = 20, face = \"bold\")\n  ) \n\n\n\n\n\n\n\n\n\nThe scatterplot shows a wide dispersion of points without a clear linear pattern, indicating a weak or no strong correlation between the popularity index and the number of playlist appearances. While a few popular songs do appear frequently in playlists, many others have high popularity but low appearances, or vice versa. This suggests that playlist frequency alone doesn‚Äôt strongly predict popularity.\n\n\nShow the code\ncorrelation &lt;-cor(popular_songs$popularity, popular_songs$playlist_appereance, use = \"complete.obs\")\n\n\nUpon further investigation, we found that the correlation between the 2 variable is 0.38, which indicates weak correlation, and validates our original thesis."
  },
  {
    "objectID": "mp03.html#in-what-year-were-the-most-popular-songs-released",
    "href": "mp03.html#in-what-year-were-the-most-popular-songs-released",
    "title": "Creating the Ultimate Playlist",
    "section": "In what year were the most popular songs released?",
    "text": "In what year were the most popular songs released?\n\n\nShow the code\npop_song_years &lt;- inner_joined_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_pop = mean(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_pop))\n\n\n\n\nShow the code\nlibrary(patchwork)\n\n# Plot\npop_plot &lt;- ggplot(pop_song_years, aes(x = year, y = avg_pop)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"darkorange\", size = 2) +\n  labs(title = \"Average Popularity of Songs by Year\",\n       x = \"Year\", y = \"Average Popularity\") +\n  theme_bw(base_size = 14)\n\n\n# Table \ntop_years_table &lt;- pop_song_years |&gt; \n  slice_max(avg_pop, n = 5) |&gt; \n  select(Year = year, `Avg Popularity` = avg_pop)\n\ntop_years_table$`Avg Popularity`&lt;- round(top_years_table$`Avg Popularity`, 2)\n\ntable_grob &lt;- gridExtra::tableGrob(top_years_table)\n\n# Combine side-by-side\npop_plot + patchwork::wrap_elements(table_grob)+ \n  plot_layout(widths = c(3, 1))  \n\n\n\n\n\n\n\n\n\nThe graph illustrates the average popularity of songs over time, showing a general upward trend‚Äîsuggesting that songs from more recent years tend to be more popular.\nNext to the graph, the table displays the Top 5 Most Popular Year in which songs were released."
  },
  {
    "objectID": "mp03.html#in-what-year-did-danceability-peak",
    "href": "mp03.html#in-what-year-did-danceability-peak",
    "title": "Creating the Ultimate Playlist",
    "section": "In what year did danceability peak?",
    "text": "In what year did danceability peak?\n\n\nShow the code\n# calculating average of dancebility for each year\ndanceability &lt;- inner_joined_data|&gt;\n  group_by(year)|&gt;\n  mutate(avg_danceability = mean(danceability))|&gt;\n  arrange(desc(avg_danceability))\n\n\nIn 2017 we see the higher danceability score, with an index of 0.71.\n\n\nShow the code\ndanceability|&gt; ggplot(\n  aes(x = year , \n      y = avg_danceability)) +\n  geom_point(color = \"darkorange\", size = 3) +\n  labs(\n    title = \"Linear Trend of Danceability Over Time\",\n    x = \"Year\",\n    y = \"Average Danceability\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y = element_text(margin = margin(r = 15)),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 16),\n    plot.title = element_text(size = 20, face = \"bold\")\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\")+\n  scale_x_continuous(breaks = seq(min(danceability$year), max(danceability$year), by = 10)) \n\n\n\n\n\n\n\n\n\nThis plot visualizes the distribution of danceability of tracks across the years. Danceability measures how suitable a track is for dancing, and values range from 0 to 1. Higher values indicate that the track is more suitable for dancing. This plot illustrates an upward trend in danceability over the years, suggesting that more recent tracks tend to be increasingly suited for dancing."
  },
  {
    "objectID": "mp03.html#song-characteristics",
    "href": "mp03.html#song-characteristics",
    "title": "Creating the Ultimate Playlist",
    "section": "Song Characteristics",
    "text": "Song Characteristics\n\n\nShow the code\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(kableExtra)\n\nload_songs &lt;- function() {\n  library(readr)\n  library(here)\n  \n  # Define the directory and file path\n  dir_path &lt;- here(\"data\", \"mp03\")\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name) \n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # Download file only if it doesn't exist\n  if (!file.exists(file_path)) {\n    url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n    download.file(url, destfile = file_path, method = \"auto\")\n  }\n  \n  \n  library(readr)\n  songs_df &lt;- read_csv(file_path, show_col_types = FALSE)\n  \n  # Optional: clean column names if necessary\n  # library(janitor)\n  # songs_df &lt;- clean_names(songs_df)\n  \n  return(songs_df)\n}\nsongs_df &lt;- load_songs()\n\n\nThis dataset contains audio features and metadata for a wide range of tracks. It includes details such as song name, artist(s), album, release year, and attributes like danceability, energy, and popularity. The data was downloaded from a GitHub mirror and required some cleaning‚Äîespecially the artists column, which lists multiple artists in a non-standard format. The cleaned dataset was transformed into a tidy structure, with each row representing one song-artist combination.\n\n\nShow the code\nlibrary(tidyr)\nlibrary(stringr)\nclean_artist_string &lt;- function(x){\n    str_replace_all(x, \"\\\\['\", \"\") |&gt; \n        str_replace_all(\"'\\\\]\", \"\") |&gt;\n        str_replace_all(\" '\", \"\")\n}\nsongs_df &lt;- songs_df |&gt; \n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)"
  },
  {
    "objectID": "mp03.html#playlist-dataset",
    "href": "mp03.html#playlist-dataset",
    "title": "Creating the Ultimate Playlist",
    "section": "Playlist Dataset",
    "text": "Playlist Dataset\nThis dataset is a large collection of Spotify user-generated playlists, provided as multiple JSON files. A custom function was written to responsibly download and parse the files only if not already available locally. Since the raw structure is nested and complex, the data was reshaped into a flat, rectangular format. Each row represents a single track within a playlist, including attributes like playlist name, track name, artist, album, and position within the playlist.\n\n\nShow the code\nload_playlists &lt;- function(n = 10) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  dir_path &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n\n  playlists &lt;- list()\n\n  for (i in 0:(n - 1)) {\n    start &lt;- i * 1000\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_url &lt;- paste0(base_url, file_name)\n    file_path &lt;- file.path(dir_path, file_name)\n\n    if (!file.exists(file_path)) {\n      message(\"Downloading: \", file_name)\n      result &lt;- tryCatch({\n        download.file(file_url, file_path, mode = \"wb\", quiet = TRUE)\n        TRUE\n      }, error = function(e) {\n        message(\"Failed to download \", file_name)\n        FALSE\n      })\n\n      if (!result) next\n    }\n\n    if (file.exists(file_path)) {\n      json_data &lt;- tryCatch({\n        jsonlite::fromJSON(file_path)\n      }, error = function(e) {\n        message(\"Failed to parse \", file_name)\n        NULL\n      })\n\n      if (!is.null(json_data)) {\n        playlists[[length(playlists) + 1]] &lt;- json_data$playlists\n      }\n    }\n  }\n\n  return(playlists)\n}\n\nif (file.exists(\"data/processed_playlists.rds\")) {\n  playlists &lt;- readRDS(\"data/processed_playlists.rds\")\n} else {\n  playlists &lt;- load_playlists(n = 10)\n  saveRDS(playlists, \"data/processed_playlists.rds\")\n}\n\n\nplaylists &lt;- load_playlists(n = 10)  \n\n\n\n\nShow the code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\n\n# Provided function to clean Spotify URIs\nstrip_spotify_prefix &lt;- function(x){\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\n\n# building the tidy dataframe\nrectangular_tracks_df &lt;- playlists[[1]] |&gt;\n  mutate(playlist_id = pid,\n         playlist_name = name,\n         playlist_followers = num_followers) |&gt;\n  select(playlist_id, playlist_name, playlist_followers, tracks) |&gt;\n  unnest(tracks) |&gt;\n  mutate(\n    artist_id = strip_spotify_prefix(artist_uri),\n    track_id = strip_spotify_prefix(track_uri),\n    album_id = strip_spotify_prefix(album_uri),\n    playlist_position = row_number()\n  ) |&gt;\n  rename(\n    artist_name = artist_name,\n    track_name = track_name,\n    album_name = album_name,\n    duration = duration_ms\n  ) |&gt;\n  select(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    artist_id,\n    track_name,\n    track_id,\n    album_name,\n    album_id,\n    duration\n  )"
  },
  {
    "objectID": "mp03.html#combining-the-datasets",
    "href": "mp03.html#combining-the-datasets",
    "title": "Creating the Ultimate Playlist",
    "section": "Combining the Datasets",
    "text": "Combining the Datasets\nTo analyze both song characteristics and playlist behavior, we use an inner_join to merge the playlist and song datasets based on track IDs.\nThis approach ensures that only songs appearing in both datasets are included in our analysis. Although this results in the loss of some playlist data (since not every track has corresponding song characteristics), it allows for a cleaner dataset with complete information. Given the differences in timing and structure between the two data exports, this is the most practical solution.\n\n\nShow the code\n# joining the 2 datasets by the track ID  using inner_join\ninner_joined_data &lt;- songs_df|&gt;\n  inner_join(rectangular_tracks_df, by =  \"track_id\")"
  },
  {
    "objectID": "mp03.html#which-decade-is-most-represented-on-user-playlists",
    "href": "mp03.html#which-decade-is-most-represented-on-user-playlists",
    "title": "Creating the Ultimate Playlist",
    "section": "Which decade is most represented on user playlists?",
    "text": "Which decade is most represented on user playlists?\nTo find the most represented decades in the playlists, we group the playlist data by decade and count how many times songs from each decade appear.\n\n\nShow the code\n# grouping by decades\ninner_joined_data&lt;- inner_joined_data|&gt;\n   mutate(decade = paste0((year %/% 10) * 10, \"s\"))\n\n\nlibrary(scales)\n# sum of appereances in playlist by decade\nrepr_decades&lt;- inner_joined_data|&gt;\n  group_by(decade)|&gt;\n  summarize(total_appearances = sum(playlist_appereance, na.rm = TRUE)) |&gt; \n  arrange(desc(total_appearances))|&gt;\n  mutate(total_appearances = number(total_appearances, big.mark = \",\"))\n\n\n\n\n\n\n\nDecade\nTotal Appearances in Playlists\n\n\n\n\n2010s\n469,475\n\n\n2000s\n54,649\n\n\n1990s\n19,889\n\n\n1980s\n11,031\n\n\n1970s\n9,041\n\n\n1960s\n3,733\n\n\n1940s\n500\n\n\n1950s\n223\n\n\n1930s\n2"
  },
  {
    "objectID": "mp03.html#graph-description-frequency-of-musical-keys-polar-plot",
    "href": "mp03.html#graph-description-frequency-of-musical-keys-polar-plot",
    "title": "Creating the Ultimate Playlist",
    "section": "üéµ Graph Description ‚Äì Frequency of Musical Keys (Polar Plot):",
    "text": "üéµ Graph Description ‚Äì Frequency of Musical Keys (Polar Plot):\nThis polar plot illustrates the frequency of musical keys, represented as numbers from 0 to 11, where each number corresponds to a musical key (e.g., 0 = C, 1 = C‚ôØ/D‚ô≠, 2 = D, etc.). The circular layout reflects the cyclical nature of musical keys, akin to the Circle of Fifths in music theory.\nEach bar‚Äôs height indicates how often that key appears among the songs in user playlists. This visualization helps identify which keys are most common, offering insights into musical trends and preferences. Despite using numeric values, the cyclical arrangement of keys remains intuitive in this format.\n\n\nShow the code\n# Calculate the frequency of each key\nkey_frequency &lt;- inner_joined_data |&gt;\n  group_by(key) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(desc(count))\n\n# Create a polar plot\nggplot(key_frequency, aes(x = as.factor(key), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  coord_polar(start = 0) +\n  theme_bw() +\n  labs(title = \"Frequency of Musical Keys Among Songs\",\n       x = \"Musical Key\",\n       y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 16),\n        plot.title = element_text(size = 20, face = \"bold\"))"
  },
  {
    "objectID": "mp03.html#track-duration-stats",
    "href": "mp03.html#track-duration-stats",
    "title": "Creating the Ultimate Playlist",
    "section": "Track Duration Stats",
    "text": "Track Duration Stats\n\n\nShow the code\n#convert duration in minutes \ninner_joined_data &lt;- inner_joined_data |&gt; \n  mutate(duration_min = duration_ms / 60000)\n\n# calculating mean and percentiles to see avg track length\nlength_info &lt;- inner_joined_data |&gt; \n  summarize( avg_length = mean(duration_min),\n    median_length = median(duration_min),\n    shortest = min(duration_min),\n    longest = max(duration_min),\n    p25 = quantile(duration_min, 0.25),\n    p75 = quantile(duration_min, 0.75)\n  )\nlength_info$avg_length &lt;- round(length_info$avg_length, 2)\nlength_info$median_length &lt;- round(length_info$median_length, 2)\nlength_info$shortest &lt;- round(length_info$shortest, 2)\nlength_info$longest &lt;- round(length_info$longest, 2)\nlength_info$p25 &lt;- round(length_info$p25, 2)\nlength_info$p75 &lt;- round(length_info$p75, 2)\n\n\nThe table below summarizes key statistics about the distribution of track lengths (in minutes) among the songs included in user playlists. It includes the average and median track length, as well as the shortest and longest tracks in the dataset. Additionally, the table shows the 25th percentile and 75th percentile values, which define the interquartile range (IQR) ‚Äî the range that contains the middle 50% of all track lengths.\nWe can see from the table that most tracks tend to be the range 3.4 and 4.4 minutes, suggesting a preference for mid-length songs.\n\n\n\n\n\nAverage\nMedian\nShortest\nLongest\n25 Percentile\n75 Percentile\n\n\n\n\n3.97\n3.83\n0.64\n37.31\n3.4\n4.4\n\n\n\n\n\n\n\nShow the code\nggplot(inner_joined_data, aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.5, fill = \"darkorange\", color = \"white\") +\n  labs(\n    title = \"Distribution of Track Lengths in User Playlists\",\n    x = \"Track Length (minutes)\",\n    y = \"Number of Songs\"\n  ) +\n  theme_bw()+\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(size = 16),\n        plot.title = element_text(size = 20, face = \"bold\"))+\n  scale_x_continuous(limits = c(0, 10, 2))"
  },
  {
    "objectID": "mp03.html#defining-popularity",
    "href": "mp03.html#defining-popularity",
    "title": "Creating the Ultimate Playlist",
    "section": "Defining Popularity",
    "text": "Defining Popularity\n\n\nShow the code\n# defining stats using percentiles\npopularity_stats &lt;- inner_joined_data |&gt; \n  summarize(\n    min_pop = min(popularity),\n    max_pop = max(popularity),\n    mean_pop = mean(popularity),\n    median_pop = median(popularity),\n    p75 = quantile(popularity, 0.75),\n    p90 = quantile(popularity, 0.90)\n  )\n popularity_stats$mean_pop &lt;- round(popularity_stats$mean_pop, 2)\n\n\nTo explore how song popularity relates to playlist appearances, we examined the distribution of the popularity variable and set a threshold to define what counts as a ‚Äúpopular‚Äù songs.\n\n\n\n\n\nMin\nMax\nAverage\nMedian\n75 Percentile\n90 Percentile\n\n\n\n\n6\n92\n63.81\n65\n72\n77\n\n\n\n\n\nThe table above shows the popularity statistics of all the tracks in the dataset. We decide to use the 75 Percentile as a threshold to define a ‚ÄúPopular Song‚Äù, and the ‚Äú90 Percentile‚Äù as a threshold for ‚ÄúVery Popular Songs‚Äù. Any track with a popularity index above 72 will be considered Popular , while any track with a popularity index above 77 will be considered Very Popular."
  },
  {
    "objectID": "mp03.html#songs-with-similar-tempo-key",
    "href": "mp03.html#songs-with-similar-tempo-key",
    "title": "Creating the Ultimate Playlist",
    "section": "üéõÔ∏è Songs with Similar Tempo & Key",
    "text": "üéõÔ∏è Songs with Similar Tempo & Key\nThe table below displays 251 songs that share a similar musical key and tempo with the selected anchor songs. These characteristics are commonly used by DJs to create smooth transitions between tracks. Songs were filtered to match the same key and have a tempo within ¬±5 BPM of the anchor song, ensuring musical coherence. The results are presented in the interactive table below for easy exploration."
  },
  {
    "objectID": "mp03.html#songs-by-same-artists",
    "href": "mp03.html#songs-by-same-artists",
    "title": "Creating the Ultimate Playlist",
    "section": "üé§ Songs by Same Artists",
    "text": "üé§ Songs by Same Artists\n\n\nShow the code\n# get the 2 artists from anchor songs\nartist1 &lt;- inner_joined_data|&gt;\n  filter(track_name == anchor1)|&gt; select(artist_name)|&gt;\n  head(1)\n\nartist2 &lt;- inner_joined_data|&gt;\n  filter(track_name == anchor2)|&gt; select(artist_name)|&gt;\n  head(1)\n\n# songs by same artists\nsame_artists &lt;- inner_joined_data |&gt;\n  filter(!(track_name %in% c(anchor1, anchor2))) |&gt;\n  filter(artist_name %in% c(artist1, artist2)) |&gt;\n  distinct(track_name, artist_name)\n\n\nThis table displays all songs by the same artists who performed the selected anchor songs, excluding the anchor songs themselves. These tracks were identified by matching the artist names and filtering for unique song titles. This approach highlights additional songs that share stylistic elements with the anchors, making them strong candidates for inclusion in a cohesive playlist."
  },
  {
    "objectID": "mp03.html#songs-released-in-the-same-year-with-similar-characteristics",
    "href": "mp03.html#songs-released-in-the-same-year-with-similar-characteristics",
    "title": "Creating the Ultimate Playlist",
    "section": "üéôÔ∏è Songs released in the same year with similar characteristics",
    "text": "üéôÔ∏è Songs released in the same year with similar characteristics\nTo identify songs that resemble the anchor tracks in musical characteristics, we averaged the acousticness, danceability, instrumentalness, and energy of both anchor songs. We then filtered songs within ¬±25% of these average values and from the same release years. The resulting table shows songs that closely match the overall style of the selected anchors.\n\n\nShow the code\nyear1&lt;- inner_joined_data|&gt;\n  filter(track_name  == anchor1)|&gt;\n  select(year)|&gt; head(1)\n\nyear2&lt;- inner_joined_data|&gt;\n  filter(track_name  == anchor2)|&gt;\n  select(year)|&gt; head(1)\n\n# update the anchor_stats\nanchor1_stats&lt;- inner_joined_data|&gt;\n  filter(track_name == anchor1) |&gt;\n  summarize(\n    acousticness = acousticness[1],\n    danceability = danceability[1],\n    liveness = liveness[1],\n    energy = energy[1]\n  )\n\nanchor2_stats&lt;- inner_joined_data|&gt;\n  filter(track_name == anchor2) |&gt;\n  summarize(\n    acousticness = acousticness[1],\n    danceability = danceability[1],\n    liveness = liveness[1],\n    energy = energy[1]\n  )\n\navrg_stats &lt;- bind_rows(anchor1_stats, anchor2_stats) |&gt;\n  summarize(\n    acousticness = mean(acousticness, na.rm = TRUE),\n    danceability = mean(danceability, na.rm = TRUE),\n    liveness = mean(liveness, na.rm = TRUE),\n    energy = mean(energy, na.rm = TRUE))\n\nsimilar_char &lt;- inner_joined_data|&gt;\n  filter(!(track_name %in% c(anchor1, anchor2)))|&gt;\n  filter( year %in% c(year1, year2))|&gt;\n  filter(\n    abs(acousticness - avrg_stats$acousticness) &lt;= 0.25 * avrg_stats$acousticness,\n    abs(danceability - avrg_stats$danceability) &lt;= 0.25 * avrg_stats$danceability,\n    abs(liveness - avrg_stats$liveness) &lt;= 0.25 * avrg_stats$liveness,\n    abs(energy - avrg_stats$energy) &lt;= 0.25 * avrg_stats$energy\n  )|&gt;\n  select(everything())|&gt;\n  distinct(track_name, .keep_all = TRUE)"
  },
  {
    "objectID": "mp03.html#finding-related-songs",
    "href": "mp03.html#finding-related-songs",
    "title": "Creating the Ultimate Playlist",
    "section": "üîä Finding Related Songs",
    "text": "üîä Finding Related Songs\n\n\nShow the code\n#Getting relevant tracks from heuristics\nrelevant_tracks &lt;- bind_rows(\n  co_occurring_songs|&gt; select(track_name),\n  same_artists|&gt; select(track_name),\n  similar_songs |&gt; select(track_name),\n  similar_char|&gt; select(track_name)\n  ) |&gt;\n    distinct()\n\n# non_popular songs\nplaylist_nonpopular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Not Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n#popular songs\nplaylist_popular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Very Popular\" |\n           popularity_level == \"Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n# getting 8 Non popular songs, 6 popular songs, 6 Very popular songs\n\nfin_playlist &lt;- bind_rows(\n  playlist_nonpopular|&gt; sample_n(8),\n  playlist_popular|&gt; filter(popularity_level == \"Popular\")|&gt;\n    sample_n(6),\n  playlist_popular|&gt; filter(popularity_level == \"Very Popular\")|&gt;\n    sample_n(6))"
  },
  {
    "objectID": "mp03.html#final-playlist-creation-based-on-heuristics-and-popularity",
    "href": "mp03.html#final-playlist-creation-based-on-heuristics-and-popularity",
    "title": "Creating the Ultimate Playlist",
    "section": "üéß Final Playlist Creation Based on Heuristics and Popularity",
    "text": "üéß Final Playlist Creation Based on Heuristics and Popularity\nTo build a well-rounded and personalized playlist, I began by identifying relevant tracks using several heuristic methods based on my two anchor songs:\n\nCo-occurring songs that frequently appear in the same playlists.\nTracks by the same artists as the anchor songs.\nSongs with a similar musical key and tempo, which would allow for smoother DJ transitions.\nSongs with similar audio characteristics, such as acousticness, danceability, energy, and liveness.\n\nThese heuristics were combined into a single list of relevant_tracks, ensuring only distinct track names were included.\nNext, I categorized the songs by popularity level: - Not Popular - Popular - Very Popular\nFinally, I curated a final playlist by sampling: - üé∂ 8 non-popular songs\n- üé∂ 6 popular songs\n- üé∂ 6 very popular songs\nThis approach creates a balanced and diverse playlist, mixing well-known hits with lesser-known tracks that share musical and stylistic traits with the anchor songs."
  },
  {
    "objectID": "mp03.html#Finding-Related-Songs",
    "href": "mp03.html#Finding-Related-Songs",
    "title": "Creating the Ultimate Playlist",
    "section": "üîä Finding Related Songs",
    "text": "üîä Finding Related Songs\n\n\nShow the code\n#Getting relevant tracks from heuristics\nrelevant_tracks &lt;- bind_rows(\n  co_occurring_songs|&gt; select(track_name),\n  same_artists|&gt; select(track_name),\n  similar_songs |&gt; select(track_name),\n  similar_char|&gt; select(track_name)\n  ) |&gt;\n    distinct()\n\n# non_popular songs\nplaylist_nonpopular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Not Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n#popular songs\nplaylist_popular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Very Popular\" |\n           popularity_level == \"Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n# getting 8 Non popular songs, 6 popular songs, 6 Very popular songs\n\nfin_playlist &lt;- bind_rows(\n  playlist_nonpopular|&gt; sample_n(8),\n  playlist_popular|&gt; filter(popularity_level == \"Popular\")|&gt;\n    sample_n(6),\n  playlist_popular|&gt; filter(popularity_level == \"Very Popular\")|&gt;\n    sample_n(6))\n\n\nTo build a well-rounded and personalized playlist, we began by identifying relevant tracks using several heuristic methods based on my two anchor songs:\n\nCo-occurring songs that frequently appear in the same playlists.\nTracks by the same artists as the anchor songs.\nSongs with a similar musical key and tempo, which would allow for smoother DJ transitions.\nSongs with similar audio characteristics, such as acousticness, danceability, energy, and liveness.\n\nThese heuristics were combined into a single list of relevant_tracks, ensuring only distinct track names were included.\nNext, I categorized the songs by popularity level: - Not Popular - Popular - Very Popular\nFinally, I curated a final playlist by sampling:\n\nüé∂ 8 non-popular songs\n\nüé∂ 6 popular songs\n\nüé∂ 6 very popular songs\n\nThis approach creates a balanced and diverse playlist, mixing well-known hits with lesser-known tracks that share musical and stylistic traits with the anchor songs."
  },
  {
    "objectID": "mp03.html#final-playlist-based-on-heuristics-and-popularity-final-playlist",
    "href": "mp03.html#final-playlist-based-on-heuristics-and-popularity-final-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "üéß Final Playlist Based on Heuristics and Popularity {final-playlist}",
    "text": "üéß Final Playlist Based on Heuristics and Popularity {final-playlist}\n\n\nShow the code\nfin_playlist_12 &lt;- bind_rows(\n  playlist_nonpopular |&gt; sample_n(4),\n  playlist_popular |&gt; filter(popularity_level == \"Popular\") |&gt; sample_n(4),\n  playlist_popular |&gt; filter(popularity_level == \"Very Popular\") |&gt; sample_n(4)\n)\n\n# audio features for visualization\nplaylist_features &lt;- inner_join(fin_playlist_12, inner_joined_data, \n                                by = c(\"name\" = \"name\", \"artist_name\" = \"artist_name\")) |&gt;\n  distinct(name, artist_name, acousticness, energy, danceability, tempo, popularity) |&gt;\n  mutate(order = row_number())\n\n# Plotting evolution\nplaylist_features_long &lt;- playlist_features |&gt;\n  select(order, name, acousticness, energy, danceability, tempo) |&gt;\n  pivot_longer(-c(order, name), names_to = \"feature\", values_to = \"value\")"
  },
  {
    "objectID": "mp03.html#analysis-of-musical-structure",
    "href": "mp03.html#analysis-of-musical-structure",
    "title": "Creating the Ultimate Playlist",
    "section": "üìä Analysis of Musical Structure",
    "text": "üìä Analysis of Musical Structure\nTo understand the sonic journey of my playlist ‚ÄúPulse & Echo‚Äù, I analyzed the evolution of key audio features provided by Spotify. These features include:\n\nAcousticness: Likelihood that a track is acoustic.\nEnergy: A measure of intensity and activity (e.g., fast, loud, noisy).\nDanceability: How suitable a track is for dancing based on tempo, rhythm stability, and beat strength.\nTempo: The overall speed of the song (measured in BPM).\n\nThe line chart below displays how these metrics change across the 12 songs in the playlist, following the track order:\n\n\nShow the code\n# Plot 1: Acousticness, Energy, Danceability\nplot1 &lt;- playlist_features|&gt;\n  select(name, acousticness, energy, danceability) |&gt;\n  pivot_longer(-name, names_to = \"Feature\", values_to = \"Value\") |&gt;\n  mutate(name = factor(name, levels = playlist_features$name)) |&gt;\n  ggplot(aes(x = name, y = Value, color = Feature, group = Feature)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Playlist Feature Progression\",\n       y = \"Value (0 to 1)\",\n       x = \"Track Name\",\n       color = \"Feature\")\n\n\n# Plot 2: Tempo\nplot2 &lt;- playlist_features |&gt;\n  ggplot(aes(x = factor(name, levels = playlist_features$name), y = tempo)) +\n  geom_line(group = 1, color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"steelblue\", size = 2) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Tempo Progression\",\n       y = \"Tempo (BPM)\",\n       x = \"Track Name\")\nplot1 / plot2"
  },
  {
    "objectID": "mp03.html#final-playlist",
    "href": "mp03.html#final-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "üéß Final Playlist Based on Heuristics and Popularity",
    "text": "üéß Final Playlist Based on Heuristics and Popularity\n\n\nShow the code\nfin_playlist_12 &lt;- bind_rows(\n  fin_playlist|&gt; filter(popularity_level == \"Not Popular\")|&gt; sample_n(4),\n  fin_playlist|&gt; filter(popularity_level == \"Popular\")|&gt; sample_n(4),\n  fin_playlist|&gt; filter(popularity_level == \"Very Popular\")|&gt; sample_n(4)\n) \n  \n\n# audio features for visualization\nplaylist_features &lt;- inner_join(fin_playlist_12, inner_joined_data, \n                                by = c(\"name\" = \"name\", \"artist_name\" = \"artist_name\")) |&gt;\n  distinct(name, artist_name, acousticness, energy, danceability, tempo, popularity) |&gt;\n  mutate(order = row_number())\n\n# Plotting evolution\nplaylist_features_long &lt;- playlist_features |&gt;\n  select(order, name, acousticness, energy, danceability, tempo) |&gt;\n  pivot_longer(-c(order, name), names_to = \"feature\", values_to = \"value\")"
  },
  {
    "objectID": "mp03.html#pulse-echo",
    "href": "mp03.html#pulse-echo",
    "title": "Creating the Ultimate Playlist",
    "section": "üéß Pulse & Echo",
    "text": "üéß Pulse & Echo\n\nThe Ultimate Playlist\nPulse & Echo is a 12-track playlist designed to guide the listener through a dynamic musical journey. The sequence weaves through smooth transitions, emotional drops, and uplifting peaks, offering both familiar favorites and new sonic discoveries. With selections spanning synthwave, pop, indie, and Latin crossover, this playlist balances mood, tempo, and energy for moments of reflection, movement, or pure vibe."
  },
  {
    "objectID": "mp03.html#design-principles",
    "href": "mp03.html#design-principles",
    "title": "Creating the Ultimate Playlist",
    "section": "üéØ Design Principles",
    "text": "üéØ Design Principles\n\nüî¨ Quantitative Harmony\nSongs were selected based on closeness in tempo, key, and compatible values for acousticness, danceability, energy, and valence. All features were compared as a percentage deviation from the average anchor song values, ensuring smooth auditory transitions.\n\n\nüìà Emotional Trajectory\nThe playlist follows a ‚Äúrise‚Äìfall‚Äìrise‚Äù pattern. It begins upbeat, dips into more introspective and atmospheric tracks midway, and picks up again with energetic closers‚Äîcreating an arc that feels cinematic and emotionally resonant.\n\n\nüßÆ Data-Driven Selection\nTracks were selected using a multi-step filtering process based on five key heuristics:\n\nCo-occurrence with anchor tracks in user playlists\n\nSame artist connections\n\nSongs in the same year with similar Spotify features\n\nKey and tempo compatibility\n\nFeature similarity within ¬±25% of anchor song averages"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "",
    "text": "Introduction\nData Import\nInitial Analysis of SEP Data\nDatasets Transformation\nExplore NTD Service Data\nCalculate Emissions\nNormalize Emissions to Transit Usage\nAward Winners\nConclusion"
  },
  {
    "objectID": "mp02.html#table-of-contents",
    "href": "mp02.html#table-of-contents",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "",
    "text": "Introduction\nData Import\nInitial Analysis of SEP Data\nDatasets Transformation\nExplore NTD Service Data\nCalculate Emissions\nNormalize Emissions to Transit Usage\nAward Winners\nConclusion"
  },
  {
    "objectID": "mp02.html#transit-sustainability-awards",
    "href": "mp02.html#transit-sustainability-awards",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "Transit Sustainability Awards",
    "text": "Transit Sustainability Awards\n\nGreenest Transit Agency (Lowest Emissions per Mile)\nAwarded to the agency with the lowest emissions per mile, recognizing the most environmentally efficient and sustainable transit operations.\n\n\nShow the code\n#greenest awards\n\nsmall_greenest &lt;- smallest_miles|&gt;\n  head(1)\n\nmedium_greenest &lt;- medium_miles|&gt;\n  head(1)\nlarge_greenest &lt;- large_miles|&gt;\n  head(1)\n\n\n\nSMALL : ‚ÄúAdirondack Transit Lines, Inc.: Adirondack Trailways‚Äù in Hurley, New York with 0.17lbs of CO2 emitted per mile travelled\nMEDIUM : ‚ÄúCity of Seattle: Seattle Center Monorail‚Äù in Seattle, Washington with 0.08lbs of CO2 emitted per mile travelled\nLARGE : ‚ÄúTri-County Metropolitan Transportation District of Oregon: TriMet‚Äù in Portland, Oregon with 0.12lbs of CO2 emitted per mile travelled\n\n\n\nShow the code\n# Calculate the average emission per agency size\naverage_emission &lt;- total_emission |&gt;\n  group_by(agency_size) |&gt;\n  summarise(mile_emission = mean(mile_emission, na.rm = TRUE)) |&gt;\n  mutate(category = \"Average Emission\")\n\n# Extract the most efficient agencies from each size category\nefficient_agencies &lt;- bind_rows(\n  smallest_miles |&gt; slice(1),\n  medium_miles |&gt; slice(1),\n  large_miles |&gt; slice(1)\n) |&gt;\n  mutate(category = \"Most Efficient\")\n\n# Combine both datasets for plotting\ncombined_data &lt;- bind_rows(average_emission, efficient_agencies)\n\ncombined_data$first_Agency &lt;- sub(\", dba\", \"\", combined_data$first_Agency)\n\n# Horizontal bar chart\nggplot(combined_data, aes(x = mile_emission, y = agency_size, fill = category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.6) + \n  labs(title = \"Emission Comparison: Most Efficient vs Average by Agency Size\",\n       x = \"CO2 Emissions per Mile (lbs)\",\n       y = \"Agency Size\",\n       fill = \"Category\") +\n  theme_bw() +\n  scale_fill_manual(values = c(\"Most Efficient\" = \"#98df8a\", \"Average Emission\" = \"#ffbb78\")) +\n  geom_text(aes(label = round(mile_emission, 2)), \n            position = position_dodge(width = 0.6), \n            hjust = -0.2, size = 4)+\n  geom_text(data = combined_data |&gt; filter(category == \"Most Efficient\"),\n            aes(label = first_Agency), \n            position = position_dodge(width = 0.6), \n            hjust =-0.2, vjust = -1.5 ,size = 4, fontface = \"bold\", color = \"black\")+\n  theme(legend.position = \"bottom\") +\n  theme(axis.title = element_text(size = 15),\n        plot.title = element_text(size = 20, face = \"bold\"))+\n  scale_x_continuous(expand = c(0, 0), limits = c(0, max(combined_data$mile_emission) * 1.1))\n\nggsave(\"bar.png\")\n\n\n\nThis graph compares the emissions per mile for the most efficient transit agency in each size category against the average emissions for that category. Each bar represents an agency size, with two bars per group: one showing the average emissions and the other highlighting the most efficient agency in that category.\nThe most efficient agencies have significantly lower emissions than the average, which suggests that certain agencies have successfully implemented cleaner technologies or more efficient operations. The names of these top-performing agencies are also displayed, emphasizing their standout performance in reducing emissions.\n\n\nMost Emissions Avoided (Comparison with Private Cars)\nThis award highlights the agency that has prevented the most CO‚ÇÇ emissions by encouraging public transit use over private car travel.\nEmissions avoided are calculated based on private cars emitting 19.6 lbs CO‚ÇÇ per gallon of fuel and averaging 25 MPG.\n\n\nShow the code\n# Assuming 25 MPG for veichles and 19.6 lbs of CO2 per gallon of gasoline\nMPG &lt;- 25\nco2_per_gallon &lt;- 19.6\n# calculating emission if passangers used car\ntotal_emission &lt;- total_emission|&gt;\n  mutate(\n    car_emission = (first_MILES / MPG) * co2_per_gallon,\n    emission_avoided = car_emission - tot_co2\n  )\n#identify top agency for each category  \nem_avoided_awards &lt;- total_emission|&gt;\n  group_by(agency_size)|&gt;\n  slice_max(emission_avoided, n=1)|&gt;\n  ungroup()\nem_avoided_awards$emission_avoided &lt;- round(em_avoided_awards$emission_avoided , digits = 0)|&gt;\n  number( , big.mark=\",\")\n\nem_avoided_awards$first_Agency &lt;- sub(\", dba\", \"\", em_avoided_awards$first_Agency )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY\nCITY\nSTATE\nAGENCY SIZE\nPOUNDS OF CO2 AVOIDED\n\n\n\n\nMTA New York City Transit\nBrooklyn\nNew York\nLarge\n5,395,411,402\n\n\nHudson Transit Lines, Inc.: Short Line\nMahwah\nNew Jersey\nMedium\n51,309,438\n\n\nHampton Jitney, Inc.\nCalverton\nNew York\nSmall\n28,931,084\n\n\n\n\n\n\n\nShow the code\n# Summarize data by agency size\nagency_emission_data &lt;- total_emission |&gt; \n  group_by(agency_size) |&gt; \n  summarize(Car_Emissions = sum(car_emission, na.rm = TRUE),\n            Total_Emissions = sum(tot_co2, na.rm = TRUE)) |&gt; \n  pivot_longer(cols = c(Car_Emissions, Total_Emissions), \n               names_to = \"Emission_Type\", values_to = \"Value\")\n\n# Bar plot with facets\nggplot(agency_emission_data, aes(x = Emission_Type, y = Value, fill = Emission_Type)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"Car_Emissions\" = \"#98df8a\", \"Total_Emissions\" = \"#ffbb78\"),\n                    labels = c(\"Car_Emissions\" = \"Emission Avoided\", \"Total_Emissions\" = \"Total Emission\")) +\n  labs(title = \"Emissions Breakdown by Agency Size\",\n       x = \"Emission Type\",\n       y = \"Emissions (lbs CO2)\",\n       fill = \"Emission Type\") + \n  facet_wrap(~ agency_size, scales = \"free_y\", nrow = 1) +  # Separate graphs per agency size\n  theme_bw()+\n  theme(axis.text.x = element_blank(),  \n        axis.ticks.x = element_blank(),\n        panel.spacing = unit(2, \"lines\"),  # Increase spacing between facets\n        strip.text = element_text(size = 14, face = \"bold\"),  # Make facet labels more visible\n        legend.text = element_text(size = 12),\n        panel.border = element_blank()) +\n  theme(axis.title = element_text(size = 15),\n        plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\nggsave(\"horizontal.png\")\n\n\n\nThe image demonstrates how using public transportation helps reduce emissions compared to cars. It shows that larger agencies are able to avoid more emissions because they have more people using public transport. The graph compares the emissions that would have been produced by cars with the emissions that are avoided by using public transportation, highlighting the environmental benefit of shifting away from private car use.\n\n\nBest Electrified Agency (Highest Percentage of Electric Propulsion)\nRecognizing the agency with the highest share of electric propulsion in its fleet, demonstrating a strong commitment to reducing greenhouse gas emissions and advancing sustainable mobility.\n\n\nShow the code\n# Define the fuel types and their energy content in Btu\nenergy_content_BTU &lt;- c(\n  \"Bio-Diesel\" = 138700,  \n  \"Bunker Fuel\" = 144500,  \n  \"C Natural Gas\" = 1030,  \n  \"Diesel Fuel\" = 137300,  \n  \"Ethanol\" = 114500,  \n  \"Gasoline\" = 115500,  \n  \"Hydrogen\" = 119980,  \n  \"Kerosene\" = 135000,  \n  \"Liquified Nat Gas\" = 1030,  \n  \"Liquified Petroleum Gas\" = 91500,  \n  \"Methonal\" = 56000  \n)\n\n# Conversion factor from Btu to kWh\nBTU_to_kWh &lt;- 0.000293071\n\n# Convert to kWh for all fuels \nemission_data &lt;- emission_data|&gt;\n  mutate(\n    energy_in_kWh = case_when(\n      Fuel_Type == \"Electric Battery\" ~ Fuel_Consumption,  # already in kWh\n      Fuel_Type == \"Electric Propulsion\" ~ Fuel_Consumption,  # already in kWh\n      TRUE ~ energy_content_BTU[Fuel_Type] * BTU_to_kWh  # Convert other fuels from Btu to kWh\n    ))\nemission_data$energy_in_kWh &lt;- round(emission_data$energy_in_kWh , digits = 1)\n\n# dividing the electric consumption compared to total fuel consumption\nelectric_agency &lt;- emission_data |&gt;\n  group_by(`NTD ID`) |&gt;\n  mutate(\n    tot = sum(Fuel_Consumption, na.rm = TRUE),  # Total fuel consumption for the agency\n    tot_electric = sum(Fuel_Consumption[Fuel_Type %in% c(\"Electric Battery\", \"Electric Propulsion\")], na.rm = TRUE)\n  )\nelectric_agency &lt;- electric_agency |&gt;\n  mutate(perc_electric = paste0(round((tot_electric / tot) * 100, 2), \"%\"))\n\n# defining small, medium, large agency \n\nelectric_agency &lt;- electric_agency |&gt;\n  mutate(agency_size = case_when(\n    UPT &lt;= quantile(UPT, 0.33, na.rm = TRUE) ~ \"Small\",\n    UPT &lt;= quantile(UPT, 0.67, na.rm = TRUE) ~ \"Medium\",\n    TRUE ~ \"Large\"\n  ))\n\n#identify top agency for each category  \nelectric_agency_top &lt;- electric_agency |&gt;\n  filter(!is.na(Agency) & !is.na(perc_electric)) |&gt;  # Remove missing values\n  arrange(agency_size, desc(perc_electric)) |&gt;  # Sort by size and electric %\n  group_by(agency_size) |&gt;  \n  slice(1) |&gt;  # Pick the highest per agency size\n  ungroup()\nelectric_agency_top$Agency &lt;- sub(\", dba\", \"\", electric_agency_top$Agency )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY\nCITY\nSTATE\nAGENCY SIZE\n% OF ELECTRIC OUT OF TOTAL FUEL CONSUMPTION\n\n\n\n\nTri-County Metropolitan Transportation District of Oregon: TriMet\nPortland\nOregon\nSmall\n99.16%\n\n\n\n\n\n\n\nWorst Polluter (Highest Emissions per Mile)\nGiven to the agency with the highest emissions per mile, emphasizing the need for targeted improvements in energy efficiency and electrification efforts.\n\n\nShow the code\nworst_polluter &lt;- total_emission |&gt; \n  filter(!is.na(mile_emission)) |&gt;  # Remove missing values\n  arrange(desc(mile_emission)) |&gt;  # Sort highest to lowest\n  group_by(agency_size) |&gt;  \n  slice(1) |&gt;  # Pick the highest per agency size\n  ungroup()\nworst_polluter$mile_emission &lt;- round(worst_polluter$mile_emission, digits = 2)\nworst_polluter$first_Agency &lt;- sub(\", dba\", \"\", worst_polluter$first_Agency)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY\nCITY\nSTATE\nAGENCY SIZE\nPOUND OF CO2 PER MILE\n\n\n\n\nMETRO Regional Transit Authority\nAkron\nOhio\nLarge\n7.29\n\n\nErie Metropolitan Transit Authority: the e\nErie\nPennsylvania\nMedium\n15.00\n\n\nAltoona Metro Transit: AMTRAN\nAltoona\nPennsylvania\nSmall\n13.00\n\n\n\n\n\n\n\nShow the code\ntotal_emission$first_Agency &lt;- sub(\", dba\", \"\", total_emission$first_Agency)\n\nggplot(total_emission, aes(x = agency_size, y = mile_emission, color = agency_size)) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 3) +\n  labs(title = \"Pollution of Transit Agencies by Size\",\n       x = \"Agency Size\",\n       y = \"CO2 Emissions per Mile (lbs)\",\n       color = \"Agency Size\") +\n  # Label highest emission agencies for each size category\n  geom_text(data = total_emission |&gt;\n              group_by(agency_size) |&gt;\n              slice_max(mile_emission, n = 1) |&gt;\n              ungroup(), \n            aes(label = str_wrap(first_Agency, width = 20)),\n            vjust = -0.5, color = \"black\", size = 4, fontface = \"bold\") +\n  theme_bw()  +\n  theme(legend.position = \"none\") +\n  theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\"),\n        axis.title = element_text(size = 15),\n        plot.title = element_text(size = 20, face = \"bold\"))+\n  ylim(0, 18)\nggsave(\"pollution_plot.png\")\n\n\n\nThis graph shows the CO2 emissions per mile for different transit agencies, divided into small, medium, and large categories. Each point represents an agency, with the color indicating its size. The points are spread out to make it easier to see how emissions vary across agencies.\nThe agencies with the highest emissions in each category are marked with their names displayed next to the points. Lines connect these labels to the points, making it clear which agencies are the biggest polluters.\nThe purpose of this graph is to show how different agencies compare in terms of their emissions and to draw attention to the agencies that need to make the biggest improvements. It emphasizes the importance of targeting energy efficiency efforts, particularly for the agencies with the highest emissions."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData from the Environmental Protection Agency (EPA) shows that public transportation reduces greenhouse gas emissions by up to 95% compared to private car use.‚Ü©Ô∏é"
  }
]