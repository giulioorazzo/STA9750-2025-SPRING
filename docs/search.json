[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hello and welcome to my website! My name is Giulio Orazzo.\nI am a finance and data enthusiast with expertise in quantitative modeling, predictive analytics, and data anlysis. I am pursuing a MS in Quantitative Methods & Modeling at CUNY Baruch College- Zicklin School of Business and hold a B.S. in Finance at CUNY College of Staten Island. Currently, I work as a Position Control Associate (HR Data Science) at the Metropolitan Transportation Authority, where I analyze data, develop predictive models, and create dashboards. Proficient in Python, R, Power BI, VBA, DAX, and STATA, I am passionate about leveraging data-driven insights to solve complex financial and business challenges.\n\n\n\n\n\n\n\nLast Updated: Thursday 02 06, 2025 at 22:30PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "",
    "text": "Show the code\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(readr)\n  library(stringr)\n  library(scales)\n  library(ggplot2)\n  library(knitr)\n  library(DT)\n})\nif(!file.exists(\"data/mp01/nyc_payroll_export.csv\")){\n  dir.create(\"data/mp01\", showWarnings=FALSE, recursive=TRUE)\n  \n  ENDPOINT &lt;- \"https://data.cityofnewyork.us/resource/k397-673e.json\"\n  \n  if(!require(\"httr2\")) install.packages(\"httr2\")\n  library(httr2)\n  \n  if(!require(\"jsonlite\")) install.packages(\"jsonlite\")\n  library(jsonlite)\n  \n  if(!require(\"dplyr\")) install.packages(\"dplyr\")\n  library(dplyr)\n  \n  if(!require(\"readr\")) install.packages(\"readr\")\n  library(readr)\n  \n  if (!require(\"scales\")) install.packages(\"scales\")\n  library(scales)\n  \n  if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n  library(ggplot2)\n  \n  if (!require(\"knitr\")) install.packages(\"knitr\")\n  library(knitr)\n  \n  BATCH_SIZE &lt;- 50000\n  OFFSET     &lt;- 0\n  END_OF_EXPORT &lt;- FALSE\n  ALL_DATA &lt;- list()\n  \n  while(!END_OF_EXPORT){\n    cat(\"Requesting items\", OFFSET, \"to\", BATCH_SIZE + OFFSET, \"\\n\")\n    \n    req &lt;- request(ENDPOINT) |&gt;\n      req_url_query(`$limit`  = BATCH_SIZE, \n                    `$offset` = OFFSET)\n    \n    resp &lt;- req_perform(req)\n    \n    batch_data &lt;- fromJSON(resp_body_string(resp))\n    \n    ALL_DATA &lt;- c(ALL_DATA, list(batch_data))\n    \n    if(NROW(batch_data) != BATCH_SIZE){\n      END_OF_EXPORT &lt;- TRUE\n      \n      cat(\"End of Data Export Reached\\n\")\n    } else {\n      OFFSET &lt;- OFFSET + BATCH_SIZE\n    }\n  }\n  \n  ALL_DATA &lt;- bind_rows(ALL_DATA)\n  \n  cat(\"Data export complete:\", NROW(ALL_DATA), \"rows and\", NCOL(ALL_DATA), \"columns.\")\n  \n  write_csv(ALL_DATA, \"data/mp01/nyc_payroll_export.csv\")\n}\ndata &lt;- read_csv(\"C:/Users/orazz/OneDrive/Documents/STA9750-2025-SPRING/data/mp01/nyc_payroll_export.csv\")\n\ndata &lt;- data |&gt;\n  mutate(agency_name = str_to_title(agency_name),\n         last_name = str_to_title(last_name),\n         first_name = str_to_title(first_name),\n         work_location_borough = str_to_title(work_location_borough),\n         title_description = str_to_title(title_description),\n         leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30)\n  )\n#create unique key identifier\ndata&lt;- data|&gt;\n  mutate(primary_key =paste(first_name,\n                            coalesce(mid_init, \"\"),\n                            last_name)\n         )\n# switch Custodian Engineer from hourly to salary\ndata&lt;- data|&gt;\n  mutate(pay_basis = if_else(title_description == \"Custodian Engineer\" & pay_basis == \"per Hour\", \"per Annum\", pay_basis))\n\ndata &lt;- data|&gt;\n  mutate( total_pay =case_when(\n    pay_basis == \"per Hour\" ~ (regular_hours * base_salary ) + (ot_hours *(base_salary * 1.5)),\n    pay_basis == \"per Day\" ~ (base_salary *(regular_hours / 7.5)) + (ot_hours * (base_salary/7.5)*1.5),\n    pay_basis == \"per Annum\" ~ (base_salary) + (((base_salary )/ 1950) * ot_hours),\n    pay_basis == \"Prorated Annual\" ~ regular_gross_paid,\n    TRUE ~ NA_real_\n  ))"
  },
  {
    "objectID": "mp01.html#mayors-payroll-table",
    "href": "mp01.html#mayors-payroll-table",
    "title": "City Payroll Data Analysis",
    "section": "",
    "text": "Here is a table of the payroll data for Mayor Eric Adams:\n\nsource(\"Mini01.R\")\n\nRows: 6225611 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): agency_name, last_name, first_name, mid_init, work_location_boroug...\ndbl  (8): fiscal_year, payroll_number, base_salary, regular_hours, regular_g...\ndttm (1): agency_start_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(mayor_table)"
  },
  {
    "objectID": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "href": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which job title has the highest base rate of pay?",
    "text": "Which job title has the highest base rate of pay?\nAfter analyzing the city payroll data, the job title with the highest base rate of pay is identified as Chief Actuary. The average base annual salary for this position is $296,470.\n\n\nShow the code\nkable(highest_salary|&gt;\n        rename(\"TITLE DESCRIPTION\" = title_description ,\n               \"AVERAGE SALARY\" = avg_tot_pay ))\n\n\n\n\n\nTITLE DESCRIPTION\nAVERAGE SALARY\n\n\n\n\nChief Actuary\n296470.4\n\n\n\n\n\nThe job title that has instead the highest hourly rate on avarage is Member, Civilian Complaint Review Board, with an average hourly pay of $319.89.\n\n\nShow the code\nkable(highest_paid_title|&gt;\n        rename(\"TITLE DESCRIPTION\" = title_description,\n               \"AVERAGE HOURLY PAY\" = max_hourly_rate ))\n\n\n\n\n\nTITLE DESCRIPTION\nAVERAGE HOURLY PAY\n\n\n\n\nMember, Civilian Complaint Review Board\n319.8927\n\n\n\n\n\nHourly Rate Determination: The hourly rate for each job title was extracted from the payroll dataset. Standard Work Year Assumption: A standard 2000-hour work year was assumed, representing a full-time employee’s expected annual working hours."
  },
  {
    "objectID": "mp01.html#table-showing-mayor-eric-l.-adams-carreer",
    "href": "mp01.html#table-showing-mayor-eric-l.-adams-carreer",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "",
    "text": "Here you can find a table of representing the salary and the Position held by the mayor Eric L. Adams by fiscal year (2014-2024)"
  },
  {
    "objectID": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-gross-pay",
    "href": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-gross-pay",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which individual & in what year had the single highest city total gross pay?",
    "text": "Which individual & in what year had the single highest city total gross pay?\n\n\nShow the code\ndata&lt;- data|&gt; mutate(\n  gross_annual_pay = regular_gross_paid + total_ot_paid + total_other_pay\n)\nhighest_paid_employee &lt;-\n  data |&gt; group_by(fiscal_year)|&gt;\n  slice_max(gross_annual_pay , n= 1)|&gt;\n  select(fiscal_year , title_description, agency_name , first_name, mid_init, last_name, gross_annual_pay)|&gt;\n  ungroup()|&gt;\n  slice_max(gross_annual_pay , n=1)\n\n\nIn this analysis, gross annual pay was used as the basis for identifying the employee with the highest earnings. Gross annual pay includes base salary , overtime paid , additional compensation such as bonuses, overtime, and other forms of variable pay.\nThis differs from the annualized base rate pay, which only accounts for the fixed salary or base compensation, excluding additional earnings like bonuses and overtime.\nThe employee with the highest gross annual pay is Mark K. Tettonis, who holds the position of Chief Marine Engineer at the Department Of Transportation with a gross annual salary of $1,689,518 in 2024"
  },
  {
    "objectID": "mp01.html#which-individual-worked-the-most-overtime-hours",
    "href": "mp01.html#which-individual-worked-the-most-overtime-hours",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which individual worked the most overtime hours ?",
    "text": "Which individual worked the most overtime hours ?\n\n\nShow the code\novertime&lt;- data|&gt;\n  filter(!is.na(last_name))|&gt;\n  group_by(agency_name , last_name, first_name, mid_init, title_description)|&gt;\n  summarize(tot_ot = sum(ot_hours , na.rm = TRUE))|&gt;\n  ungroup()|&gt;\n  slice_max(tot_ot, n=1)\n\n\n\n\nShow the code\nkable(overtime|&gt;\n        rename(\"FIRST NAME\" = first_name,\n               \"LAST NAME\" = last_name,\n               \"MIDDLE INITIAL\" = mid_init,\n               \"AGENCY NAME\" = agency_name,\n               \"TITLE DESCRIPTION\" = title_description,\n               \"TOTAL OVERTIME HOURS\" = tot_ot\n               ))\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY NAME\nLAST NAME\nFIRST NAME\nMIDDLE INITIAL\nTITLE DESCRIPTION\nTOTAL OVERTIME HOURS\n\n\n\n\nDepartment Of Correction\nCastillo\nJohn\nNA\nCorrection Officer\n22119.74\n\n\n\n\n\nThe employee with most overtime hours is John Castillo, working for the Department Of Correction as a Correction Officer with 2.212^{4} hours or about 922 days."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll",
    "href": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which agency has the highest average total annual payroll?",
    "text": "Which agency has the highest average total annual payroll?\nHere is a list of the 5 agency that have the highest average annual payroll\n\n\nShow the code\ndata &lt;- data|&gt;mutate(aggregated_agency_name = replace(agency_name, str_detect(agency_name, \"Dept Of Ed\"), \"DEPARTMENT OF EDUCATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Community Board\"), \"COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Borough President\"), \"BOROUGH PRESIDENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"District Attorney\"), \"DISTRICT ATTORNEY\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Public Administrator\"), \"PUBLIC ADMINISTRATOR\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Sanitation\"), \"DEPARTMENT OF SANITATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Fire Department\"), \"FIRE DEPARTMENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Police Department\"), \"POLICE DEPARTMENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Health\"), \"DEPARTMENT OF HEALTH\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Transportation\"), \"DEPARTMENT OF TRANSPORTATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Public Service\"), \"PUBLIC SERVICE COMMISSION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"Fire Pension Fund\"), \"FIRE PENSION FUND\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"DEPT OF ED\"), \"DEPARTMENT OF EDUCATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"PUBLIC ADMINISTRATOR\"), \"PUBLIC ADMINISTRATOR\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"STATEN ISLAND COMMUNITY BD\"), \"STATEN ISLAND COMMUNITY BD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"QUEENS COMMUNITY BOARD\"), \"QUEENS COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"MANHATTAN COMMUNITY BOARD\"), \"MANHATTAN COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"BROOKLYN COMMUNITY BOARD\"), \"BROOKLYN COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"BRONX COMMUNITY BOARD\"), \"BRONX COMMUNITY BOARD\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, str_detect(agency_name, \"BOROUGH PRESIDENT\"), \"BOROUGH PRESIDENT\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, agency_name == \"BOARD OF ELECTION POLL WORKERS\", \"BOARD OF ELECTION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, agency_name == \"DEPARTMENT OF EDUCATION ADMIN\", \"DEPARTMENT OF EDUCATION\")) |&gt;\n  mutate(aggregated_agency_name = replace(aggregated_agency_name, agency_name == \"DOE CUSTODIAL PAYROL\", \"DEPARTMENT OF EDUCATION\"))\nagency_payroll &lt;- data|&gt;\n  group_by(aggregated_agency_name)|&gt;\n  summarize(average_payroll = mean(gross_annual_pay , na.rm = TRUE))|&gt;\n  ungroup()|&gt;\n  slice_max(average_payroll , n = 5 )\nagency_payroll$average_payroll &lt;- dollar(agency_payroll$average_payroll)\n\n\n\n\nShow the code\nkable(agency_payroll|&gt;\n        rename(\"AGENCY NAME\" = aggregated_agency_name,\n               \"AVERAGE TOTAL PAYROLL\" = average_payroll\n               ))\n\n\n\n\n\nAGENCY NAME\nAVERAGE TOTAL PAYROLL\n\n\n\n\nOffice Of Collective Bargainin\n$105,563\n\n\nFinancial Info Svcs Agency\n$105,437\n\n\nFIRE DEPARTMENT\n$100,285\n\n\nOffice Of The Actuary\n$98,543\n\n\nMunicipal Water Fin Authority\n$92,881"
  },
  {
    "objectID": "mp01.html#which-agency-has-the-most-employees-on-payroll-each-year",
    "href": "mp01.html#which-agency-has-the-most-employees-on-payroll-each-year",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which agency has the most employees on payroll each year?",
    "text": "Which agency has the most employees on payroll each year?\n\n\nShow the code\nmost_employee &lt;- data|&gt;\n  group_by( fiscal_year , aggregated_agency_name)|&gt;\n  summarize(employee_n = n())|&gt;\n  slice_max(employee_n , n =1)|&gt;\n  arrange(desc(fiscal_year))\n\n\nThe table show the agency for each fiscal year with the most employee\n\n\nShow the code\nkable(most_employee|&gt;\n        rename(\n               \"AGENCY NAME\" = aggregated_agency_name,\n               \"FISCAL YEAR\" = fiscal_year,\n               \"NUMBER OF EMPLOYEE\" = employee_n\n               ))\n\n\n\n\n\nFISCAL YEAR\nAGENCY NAME\nNUMBER OF EMPLOYEE\n\n\n\n\n2024\nDEPARTMENT OF EDUCATION\n277082\n\n\n2023\nDEPARTMENT OF EDUCATION\n271265\n\n\n2022\nDEPARTMENT OF EDUCATION\n323651\n\n\n2021\nDEPARTMENT OF EDUCATION\n285867\n\n\n2020\nDEPARTMENT OF EDUCATION\n294765\n\n\n2019\nDEPARTMENT OF EDUCATION\n292293\n\n\n2018\nDEPARTMENT OF EDUCATION\n256953\n\n\n2017\nDEPARTMENT OF EDUCATION\n249514\n\n\n2016\nDEPARTMENT OF EDUCATION\n249465\n\n\n2015\nDEPARTMENT OF EDUCATION\n282919\n\n\n2014\nDEPARTMENT OF EDUCATION\n241224\n\n\n\n\n\nThe DEPARTMENT OF EDUCATION is the Agency with the most amount of empployee from the Years 2014-2024"
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-overtime-usage",
    "href": "mp01.html#which-agency-has-the-highest-overtime-usage",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Which agency has the highest overtime usage?",
    "text": "Which agency has the highest overtime usage?\n\n\nShow the code\nagency_ot &lt;- data |&gt;\n  group_by(aggregated_agency_name) |&gt;\n  summarize(avg_ot = mean(ot_hours), avg_reg_hours = mean(regular_hours)) |&gt;\n  filter(avg_ot != 0) |&gt;\n  arrange(desc(avg_ot)) |&gt;\n  slice_max(avg_ot, n = 5) |&gt;\n  rename(\n    \"AVERAGE OVERTIME HOURS\" = avg_ot,\n    \"AVERAGE REGULAR HOURS\" = avg_reg_hours\n  )\n\n\n\n\nShow the code\nkable(agency_ot)\n\n\n\n\n\n\n\n\n\n\naggregated_agency_name\nAVERAGE OVERTIME HOURS\nAVERAGE REGULAR HOURS\n\n\n\n\nFIRE DEPARTMENT\n345.3771\n1814.047\n\n\nDepartment Of Correction\n316.5495\n1714.251\n\n\nBoard Of Election\n261.8237\n1311.668\n\n\nDEPARTMENT OF SANITATION\n223.3824\n1613.109\n\n\nPOLICE DEPARTMENT\n211.6590\n1698.734"
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs",
    "href": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "What is the average salary of employees who work outside the five boroughs?",
    "text": "What is the average salary of employees who work outside the five boroughs?\n\n\nShow the code\ntemp&lt;- data|&gt;\n  filter(!work_location_borough %in% c(\"Manhattan\", \"Queens\", \"Richmond\", \"Brooklyn\", \"Bronx\",NA))|&gt;\n  group_by(work_location_borough)|&gt;\n  summarize(avg_salary = mean(gross_annual_pay))\ntemp$avg_salary &lt;- dollar(temp$avg_salary)\n\n\n\n\nShow the code\nkable(temp|&gt; rename(\n  \"LOCATION OUTSIDE NYC\"= work_location_borough,\n  \"AVERAGE SALARY\" = avg_salary\n))\n\n\n\n\n\nLOCATION OUTSIDE NYC\nAVERAGE SALARY\n\n\n\n\nAlbany\n$86,340.55\n\n\nDelaware\n$72,016.17\n\n\nDutchess\n$98,397.71\n\n\nGreene\n$91,328.46\n\n\nNassau\n$36,998.58\n\n\nOrange\n$53,820.85\n\n\nOther\n$68,563.13\n\n\nPutnam\n$72,497.18\n\n\nSchoharie\n$75,467.09\n\n\nSullivan\n$76,914.40\n\n\nUlster\n$77,292.72\n\n\nWashington Dc\n$83,022.62\n\n\nWestchester\n$78,168.78"
  },
  {
    "objectID": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "href": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "How much has the city’s aggregate payroll grown over the past 10 years?",
    "text": "How much has the city’s aggregate payroll grown over the past 10 years?\nThe graph belows shows the trend of the city’s aggregate Payroll(in Billions $) by fiscal year.\n\n\nShow the code\naggregate_payroll &lt;- data|&gt;\n  group_by(fiscal_year)|&gt;\n  summarize(tot_payroll = sum(gross_annual_pay))\npayroll_2014 &lt;- aggregate_payroll |&gt; filter(fiscal_year == 2014)|&gt; pull(tot_payroll)\npayroll_2024 &lt;- aggregate_payroll |&gt; filter(fiscal_year == 2024)|&gt; pull(tot_payroll)\n\n\n\n\nShow the code\nggplot(aggregate_payroll, aes(x = fiscal_year, y = tot_payroll)) +\n  geom_line(color = \"darkblue\", size = 1) +  \n  geom_point(color = \"orange\", size = 3) +  \n  labs(\n    title = \"City's Aggregate Payroll Growth Over the Past 10 Years\",\n    x = \"Fiscal Year\",\n    y = \"Total Payroll $ (in Billions)\"\n  ) +\n  scale_y_continuous(labels = label_dollar(scale = 1e-9, suffix = \"B\")) +  \n  scale_x_continuous(breaks = seq(min(aggregate_payroll$fiscal_year), max(aggregate_payroll$fiscal_year), by = 1)) +  \n  theme_minimal()  \n\n\n\n\n\n\n\n\n\nOver the last 10 years, we have seen an upward trend in the total payroll. The 2014 total payroll is $22,862,581,289, while the 2024 total payroll is $32,148,690,281, with an increase of $9,286,108,991 which is equal 40.62% increase since 2014."
  },
  {
    "objectID": "mp01.html#establish-a-workforce-attrition-management-strategy",
    "href": "mp01.html#establish-a-workforce-attrition-management-strategy",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Establish a Workforce Attrition Management Strategy",
    "text": "Establish a Workforce Attrition Management Strategy\n\nControl hiring to maintain a sustainable workforce size post-reduction.\nRegularly assess staffing needs to ensure that essential services are not compromised.\n\n\nEnhance Operational Efficiency Through Process Optimization:\n\nObjective: Improve service delivery and reduce operational costs.\nApproach: Adopt strategies such as streamlining procurement processes, reducing bureaucratic inefficiencies, and eliminating non-essential expenses. :contentReferenceoaicite:2\nConsiderations: Engage employees in identifying inefficiencies and encourage a culture of continuous improvement.\n\nInvest in Technology and Training:\n\nObjective: Equip the workforce with tools and skills to adapt to evolving public service demands.\nApproach: Provide training in digital literacy and data-driven decision-making, and invest in technologies that automate routine tasks.\nConsiderations: Ensure that technological advancements do not lead to significant job displacement without adequate support and retraining opportunities.\n\nMonitor and Evaluate Policy Impact:\n\nObjective: Assess the effectiveness of workforce optimization efforts.\nApproach: Establish metrics to evaluate changes in service delivery quality, employee satisfaction, and fiscal savings.\nConsiderations: Use evaluation results to make data-informed adjustments to policies and practices."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Strategic Workforce Optimization policy represents a comprehensive approach to refining government operations by thoughtfully reducing workforce size and enhancing efficiency. By implementing these recommendations, agencies can better serve the public while ensuring responsible stewardship of public resources."
  },
  {
    "objectID": "mp01.html#key-findings-1",
    "href": "mp01.html#key-findings-1",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Key Findings:",
    "text": "Key Findings:\n\nCompensation Disparities: Positions such as Chief Actuary and Member of the Civilian Complaint Review Board exhibit notably high average salaries and hourly rates, suggesting potential areas for salary standardization.\nOvertime Expenditures: Many Departments incur substantial overtime costs, indicating a possible misalignment between staffing levels and workload demands.\nAgency Payroll Growth: The consistent increase in average total payrolls across agencies over the past decade necessitates a reevaluation of budgeting and resource allocation strategies.\nEmployee Distribution: The Department of Education consistently employs the highest number of staff, underscoring its pivotal role in the city’s operations and the potential impact of workforce optimization within this sector."
  },
  {
    "objectID": "mp01.html#final-reccommendations",
    "href": "mp01.html#final-reccommendations",
    "title": "City Payroll Data Analysis (Mini Project 1)",
    "section": "Final Reccommendations",
    "text": "Final Reccommendations\n\nSalary Capping: Consider capping salaries for high-ranking positions to ensure a more equitable distribution of resources.\nReduce Overtime: Implement measures to reduce overtime by increasing staffing in critical roles, such as emergency medical technicians and police officers.\nStrategic Workforce Optimization  : refining government operations by thoughtfully reducing workforce size and enhancing efficiency."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Creating the Ultimate Playlist",
    "section": "",
    "text": "Introduction\nData Acquisition\nInitial Exploration\nIdentifying Characteristics of Popular Songs\nBuilding a Playlist\nFinding Related Songs\nFinal Playlist"
  },
  {
    "objectID": "mp03.html#table-of-contents",
    "href": "mp03.html#table-of-contents",
    "title": "Creating the Ultimate Playlist",
    "section": "",
    "text": "Introduction\nData Acquisition\nInitial Exploration\nIdentifying Characteristics of Popular Songs\nBuilding a Playlist\nFinding Related Songs\nFinal Playlist"
  },
  {
    "objectID": "mp03.html#how-many-distinct-tracks-and-artists-are-represented-in-the-playlist-data",
    "href": "mp03.html#how-many-distinct-tracks-and-artists-are-represented-in-the-playlist-data",
    "title": "Creating the Ultimate Playlist",
    "section": "How many distinct tracks and artists are represented in the playlist data?",
    "text": "How many distinct tracks and artists are represented in the playlist data?\n\n\nShow the code\ndist_artist &lt;- rectangular_tracks_df|&gt; summarise(\n  distinct_count = n_distinct(artist_name)\n)\n\ndist_track &lt;- rectangular_tracks_df|&gt; summarise(\n  distinct_count = n_distinct(track_name)\n)\n\n\n\nThere are 30049 tracks and 9722 artists in the playlist dataset."
  },
  {
    "objectID": "mp03.html#what-are-the-5-most-popular-tracks-in-the-playlist-data",
    "href": "mp03.html#what-are-the-5-most-popular-tracks-in-the-playlist-data",
    "title": "Creating the Ultimate Playlist",
    "section": "What are the 5 most popular tracks in the playlist data?",
    "text": "What are the 5 most popular tracks in the playlist data?\n\n\nShow the code\nmost_pop_tracks &lt;- rectangular_tracks_df|&gt;\n  group_by(track_name)|&gt;\n  summarize(\n    artist_name = first(artist_name),\n    album_name = first(album_name),\n    track_id = first(track_id),\n    count = n())|&gt;\n  arrange(desc(count))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrack Name\nArtist\nAlbum\n# of Appereance in Playlists\n\n\n\n\nCloser\nNe-Yo\nYear Of The Gentleman\n75\n\n\nOne Dance\nDrake\nViews\n55\n\n\nHUMBLE.\nKendrick Lamar\nDAMN.\n52\n\n\nRide\nCiara\nBasic Instinct\n52\n\n\nBroccoli (feat. Lil Yachty)\nDRAM\nBig Baby DRAM\n50\n\n\n\n\n\nThe table shows the 5 most popular songs, and how many time they appeared in different playlists.\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nShow the code\nsongs_df&lt;- songs_df|&gt;\n  rename(\"track_id\" = id)\n\n# joining the 2 datasets by the track ID \njoined_data &lt;- songs_df|&gt;\n  left_join(rectangular_tracks_df, by =  \"track_id\")\n\ncount_of_NA &lt;- joined_data|&gt;\n  summarize(na_count = sum(is.na(playlist_name)))\n\n#for(i in 1:5) {\n#  if most_pop_tracks$track_id[]\n#}\n\n\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\nShow the code\n#get the most danceable song\nmost_danceable&lt;- joined_data|&gt;\n  group_by(name)|&gt;\n  arrange(desc(danceability))|&gt;\n  head(5)\n# counting number of appearences\ndance_count&lt;- rectangular_tracks_df|&gt;\n  filter(track_id == most_danceable$track_id[1])|&gt;\n  summarize(appereances = n())\n\n\n\nThe most danceable track in the dataset is Funky Cold Medina by Tone-Loc, which appears 1 time in the “VACATION”\n\n\n\nWhich playlist has the longest average track length?\n\n\nShow the code\nrectangular_tracks_df &lt;- rectangular_tracks_df|&gt;\n  group_by(playlist_name)|&gt;\n  mutate(avg_duration = mean(duration))|&gt;\n  ungroup()|&gt;\n  mutate(avg_duration = avg_duration / 1000,  # converting ms to seconds\n         avg_duration_min = avg_duration / 60)  \n\nlong_avg &lt;- rectangular_tracks_df|&gt;\n  slice_max(avg_duration)\n\n\n“classical” is the playlist with longest average track lenght, with an average duration of 411 seconds , or about 7 minutes for each song.\n\n\nWhat is the most popular playlist on Spotify?\n\n\nShow the code\nmost_pop_play &lt;- rectangular_tracks_df|&gt;\n  slice_max(playlist_followers)\n\n\nThe most popular playlist on Spotify is Tangled with 1038 followers."
  },
  {
    "objectID": "mp03.html#what-is-the-most-popular-track-in-the-playlist-data-that-does-not-have-a-corresponding-entry-in-the-song-characteristics-data",
    "href": "mp03.html#what-is-the-most-popular-track-in-the-playlist-data-that-does-not-have-a-corresponding-entry-in-the-song-characteristics-data",
    "title": "Creating the Ultimate Playlist",
    "section": "What is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?",
    "text": "What is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nShow the code\nsongs_df&lt;- songs_df|&gt;\n  rename(\"track_id\" = id)\n\n# joining the 2 datasets by the track ID \njoined_data &lt;- songs_df|&gt;\n  left_join(rectangular_tracks_df, by =  \"track_id\")\n\ncount_of_NA &lt;- joined_data|&gt;\n  summarize(na_count = sum(is.na(playlist_name)))\n\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\nWhich playlist has the longest average track length?"
  },
  {
    "objectID": "mp03.html#what-is-the-most-popular-playlist-on-spotify",
    "href": "mp03.html#what-is-the-most-popular-playlist-on-spotify",
    "title": "Creating the Ultimate Playlist",
    "section": "What is the most popular playlist on Spotify?",
    "text": "What is the most popular playlist on Spotify?\n\n\nShow the code\nmost_pop_play &lt;- rectangular_tracks_df|&gt;\n  slice_max(playlist_followers)\n\n\nThe most popular playlist on Spotify is Tangled with 1038 followers."
  },
  {
    "objectID": "mp03.html#is-the-popularity-column-correlated-with-the-number-of-playlist-appearances-if-so-to-what-degree",
    "href": "mp03.html#is-the-popularity-column-correlated-with-the-number-of-playlist-appearances-if-so-to-what-degree",
    "title": "Creating the Ultimate Playlist",
    "section": "Is the popularity column correlated with the number of playlist appearances? If so, to what degree?",
    "text": "Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n\nShow the code\n# counting playilist appearances in the inner_jointed dataset\ninner_joined_data &lt;- inner_joined_data|&gt;\n  group_by(track_id)|&gt;\n  mutate( playlist_appereance = n())|&gt;\n  ungroup()\n\n# getting the first occurence of each song\npopular_songs &lt;-inner_joined_data|&gt;\n  group_by(track_id)|&gt;\n  slice(1)|&gt;\n  arrange(desc(popularity))\n\n\n\nThe table below show the Top 5 most popular songs using Popularity Index\n\n\n\n\n\n\n\n\n\n\n\n\nSong\nArtist\nAlbum Name\nPopularity Index\n# of Playlist Appearence\n\n\n\n\ngoosebumps\nTravis Scott\nBirds In The Trap Sing McKnight\n92\n35\n\n\nPlay Date\nMelanie Martinez\nCry Baby\n91\n1\n\n\nJocelyn Flores\nXXXTENTACION\n17\n87\n11\n\n\nPerfect\nEd Sheeran\n÷\n86\n7\n\n\nShape of You\nEd Sheeran\n÷\n85\n30\n\n\n\n\n\n\n\nThe table below show the Top 5 most popular songs by Playlist Appereances\n\n\n\n\n\n\n\n\n\n\n\n\nSong\nArtist\nAlbum Name\nPopularity Index\n# of Playlist Appearence\n\n\n\n\nF**kin’ Problems (feat. Drake, 2 Chainz & Kendrick Lamar)\nA\\(AP Rocky'        |LONG.LIVE.A\\)AP (Deluxe Version)\n76\n120\n\n\n\nChampions\nKanye West’\nChampions\n68\n120\n\n\nSucker for Pain (with Wiz Khalifa, Imagine Dragons, Logic & Ty Dolla $ign feat. X Ambassadors)\nLil Wayne’\nSucker For Pain (with Logic & Ty Dolla $ign feat. X Ambassadors)\n77\n102\n\n\nNo Problem (feat. Lil Wayne & 2 Chainz)\nChance the Rapper’\nColoring Book\n73\n93\n\n\nCloser\nThe Chainsmokers’\nCloser\n84\n92\n\n\n\n\n\n\n\nShow the code\npopular_songs|&gt; ggplot(\n  aes(x = popularity , \n      y = playlist_appereance)) +\n  geom_point(color = \"darkblue\", size = 2) +\n  labs(\n    title = \"Popularity VS Playlist Appereance\",\n    x = \"Popularity\",\n    y = \"Playlist Appereance\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y = element_text(margin = margin(r = 15)),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 16),\n    plot.title = element_text(size = 20, face = \"bold\")\n  ) \n\n\n\n\n\n\n\n\n\nThe scatterplot shows a wide dispersion of points without a clear linear pattern, indicating a weak or no strong correlation between the popularity index and the number of playlist appearances. While a few popular songs do appear frequently in playlists, many others have high popularity but low appearances, or vice versa. This suggests that playlist frequency alone doesn’t strongly predict popularity.\n\n\nShow the code\ncorrelation &lt;-cor(popular_songs$popularity, popular_songs$playlist_appereance, use = \"complete.obs\")\n\n\nUpon further investigation, we found that the correlation between the 2 variable is 0.38, which indicates weak correlation, and validates our original thesis."
  },
  {
    "objectID": "mp03.html#in-what-year-were-the-most-popular-songs-released",
    "href": "mp03.html#in-what-year-were-the-most-popular-songs-released",
    "title": "Creating the Ultimate Playlist",
    "section": "In what year were the most popular songs released?",
    "text": "In what year were the most popular songs released?\n\n\nShow the code\npop_song_years &lt;- inner_joined_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_pop = mean(popularity, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_pop))\n\n\n\n\nShow the code\nlibrary(patchwork)\n\n# Plot\npop_plot &lt;- ggplot(pop_song_years, aes(x = year, y = avg_pop)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"darkorange\", size = 2) +\n  labs(title = \"Average Popularity of Songs by Year\",\n       x = \"Year\", y = \"Average Popularity\") +\n  theme_bw(base_size = 14)\n\n\n# Table \ntop_years_table &lt;- pop_song_years |&gt; \n  slice_max(avg_pop, n = 5) |&gt; \n  select(Year = year, `Avg Popularity` = avg_pop)\n\ntop_years_table$`Avg Popularity`&lt;- round(top_years_table$`Avg Popularity`, 2)\n\ntable_grob &lt;- gridExtra::tableGrob(top_years_table)\n\n# Combine side-by-side\npop_plot + patchwork::wrap_elements(table_grob)+ \n  plot_layout(widths = c(3, 1))  \n\n\n\n\n\n\n\n\n\nThe graph illustrates the average popularity of songs over time, showing a general upward trend—suggesting that songs from more recent years tend to be more popular.\nNext to the graph, the table displays the Top 5 Most Popular Year in which songs were released."
  },
  {
    "objectID": "mp03.html#in-what-year-did-danceability-peak",
    "href": "mp03.html#in-what-year-did-danceability-peak",
    "title": "Creating the Ultimate Playlist",
    "section": "In what year did danceability peak?",
    "text": "In what year did danceability peak?\n\n\nShow the code\n# calculating average of dancebility for each year\ndanceability &lt;- inner_joined_data|&gt;\n  group_by(year)|&gt;\n  mutate(avg_danceability = mean(danceability))|&gt;\n  arrange(desc(avg_danceability))\n\n\nIn 2017 we see the higher danceability score, with an index of 0.71.\n\n\nShow the code\ndanceability|&gt; ggplot(\n  aes(x = year , \n      y = avg_danceability)) +\n  geom_point(color = \"darkorange\", size = 3) +\n  labs(\n    title = \"Linear Trend of Danceability Over Time\",\n    x = \"Year\",\n    y = \"Average Danceability\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.title.x = element_text(margin = margin(t = 15)),\n    axis.title.y = element_text(margin = margin(r = 15)),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 16),\n    plot.title = element_text(size = 20, face = \"bold\")\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\")+\n  scale_x_continuous(breaks = seq(min(danceability$year), max(danceability$year), by = 10)) \n\n\n\n\n\n\n\n\n\nThis plot visualizes the distribution of danceability of tracks across the years. Danceability measures how suitable a track is for dancing, and values range from 0 to 1. Higher values indicate that the track is more suitable for dancing. This plot illustrates an upward trend in danceability over the years, suggesting that more recent tracks tend to be increasingly suited for dancing."
  },
  {
    "objectID": "mp03.html#song-characteristics",
    "href": "mp03.html#song-characteristics",
    "title": "Creating the Ultimate Playlist",
    "section": "Song Characteristics",
    "text": "Song Characteristics\n\n\nShow the code\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(scales)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(kableExtra)\n\nload_songs &lt;- function() {\n  library(readr)\n  library(here)\n  \n  # Define the directory and file path\n  dir_path &lt;- here(\"data\", \"mp03\")\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name) \n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # Download file only if it doesn't exist\n  if (!file.exists(file_path)) {\n    url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n    download.file(url, destfile = file_path, method = \"auto\")\n  }\n  \n  \n  library(readr)\n  songs_df &lt;- read_csv(file_path, show_col_types = FALSE)\n  \n  # Optional: clean column names if necessary\n  # library(janitor)\n  # songs_df &lt;- clean_names(songs_df)\n  \n  return(songs_df)\n}\nsongs_df &lt;- load_songs()\n\n\nThis dataset contains audio features and metadata for a wide range of tracks. It includes details such as song name, artist(s), album, release year, and attributes like danceability, energy, and popularity. The data was downloaded from a GitHub mirror and required some cleaning—especially the artists column, which lists multiple artists in a non-standard format. The cleaned dataset was transformed into a tidy structure, with each row representing one song-artist combination.\n\n\nShow the code\nlibrary(tidyr)\nlibrary(stringr)\nclean_artist_string &lt;- function(x){\n    str_replace_all(x, \"\\\\['\", \"\") |&gt; \n        str_replace_all(\"'\\\\]\", \"\") |&gt;\n        str_replace_all(\" '\", \"\")\n}\nsongs_df &lt;- songs_df |&gt; \n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)"
  },
  {
    "objectID": "mp03.html#playlist-dataset",
    "href": "mp03.html#playlist-dataset",
    "title": "Creating the Ultimate Playlist",
    "section": "Playlist Dataset",
    "text": "Playlist Dataset\nThis dataset is a large collection of Spotify user-generated playlists, provided as multiple JSON files. A custom function was written to responsibly download and parse the files only if not already available locally. Since the raw structure is nested and complex, the data was reshaped into a flat, rectangular format. Each row represents a single track within a playlist, including attributes like playlist name, track name, artist, album, and position within the playlist.\n\n\nShow the code\nload_playlists &lt;- function(n = 10) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  dir_path &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n\n  playlists &lt;- list()\n\n  for (i in 0:(n - 1)) {\n    start &lt;- i * 1000\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_url &lt;- paste0(base_url, file_name)\n    file_path &lt;- file.path(dir_path, file_name)\n\n    if (!file.exists(file_path)) {\n      message(\"Downloading: \", file_name)\n      result &lt;- tryCatch({\n        download.file(file_url, file_path, mode = \"wb\", quiet = TRUE)\n        TRUE\n      }, error = function(e) {\n        message(\"Failed to download \", file_name)\n        FALSE\n      })\n\n      if (!result) next\n    }\n\n    if (file.exists(file_path)) {\n      json_data &lt;- tryCatch({\n        jsonlite::fromJSON(file_path)\n      }, error = function(e) {\n        message(\"Failed to parse \", file_name)\n        NULL\n      })\n\n      if (!is.null(json_data)) {\n        playlists[[length(playlists) + 1]] &lt;- json_data$playlists\n      }\n    }\n  }\n\n  return(playlists)\n}\n\nif (file.exists(\"data/processed_playlists.rds\")) {\n  playlists &lt;- readRDS(\"data/processed_playlists.rds\")\n} else {\n  playlists &lt;- load_playlists(n = 10)\n  saveRDS(playlists, \"data/processed_playlists.rds\")\n}\n\n\nplaylists &lt;- load_playlists(n = 10)  \n\n\n\n\nShow the code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\n\n# Provided function to clean Spotify URIs\nstrip_spotify_prefix &lt;- function(x){\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\n\n# building the tidy dataframe\nrectangular_tracks_df &lt;- playlists[[1]] |&gt;\n  mutate(playlist_id = pid,\n         playlist_name = name,\n         playlist_followers = num_followers) |&gt;\n  select(playlist_id, playlist_name, playlist_followers, tracks) |&gt;\n  unnest(tracks) |&gt;\n  mutate(\n    artist_id = strip_spotify_prefix(artist_uri),\n    track_id = strip_spotify_prefix(track_uri),\n    album_id = strip_spotify_prefix(album_uri),\n    playlist_position = row_number()\n  ) |&gt;\n  rename(\n    artist_name = artist_name,\n    track_name = track_name,\n    album_name = album_name,\n    duration = duration_ms\n  ) |&gt;\n  select(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    artist_id,\n    track_name,\n    track_id,\n    album_name,\n    album_id,\n    duration\n  )"
  },
  {
    "objectID": "mp03.html#combining-the-datasets",
    "href": "mp03.html#combining-the-datasets",
    "title": "Creating the Ultimate Playlist",
    "section": "Combining the Datasets",
    "text": "Combining the Datasets\nTo analyze both song characteristics and playlist behavior, we use an inner_join to merge the playlist and song datasets based on track IDs.\nThis approach ensures that only songs appearing in both datasets are included in our analysis. Although this results in the loss of some playlist data (since not every track has corresponding song characteristics), it allows for a cleaner dataset with complete information. Given the differences in timing and structure between the two data exports, this is the most practical solution.\n\n\nShow the code\n# joining the 2 datasets by the track ID  using inner_join\ninner_joined_data &lt;- songs_df|&gt;\n  inner_join(rectangular_tracks_df, by =  \"track_id\")"
  },
  {
    "objectID": "mp03.html#which-decade-is-most-represented-on-user-playlists",
    "href": "mp03.html#which-decade-is-most-represented-on-user-playlists",
    "title": "Creating the Ultimate Playlist",
    "section": "Which decade is most represented on user playlists?",
    "text": "Which decade is most represented on user playlists?\nTo find the most represented decades in the playlists, we group the playlist data by decade and count how many times songs from each decade appear.\n\n\nShow the code\n# grouping by decades\ninner_joined_data&lt;- inner_joined_data|&gt;\n   mutate(decade = paste0((year %/% 10) * 10, \"s\"))\n\n\nlibrary(scales)\n# sum of appereances in playlist by decade\nrepr_decades&lt;- inner_joined_data|&gt;\n  group_by(decade)|&gt;\n  summarize(total_appearances = sum(playlist_appereance, na.rm = TRUE)) |&gt; \n  arrange(desc(total_appearances))|&gt;\n  mutate(total_appearances = number(total_appearances, big.mark = \",\"))\n\n\n\n\n\n\n\nDecade\nTotal Appearances in Playlists\n\n\n\n\n2010s\n469,475\n\n\n2000s\n54,649\n\n\n1990s\n19,889\n\n\n1980s\n11,031\n\n\n1970s\n9,041\n\n\n1960s\n3,733\n\n\n1940s\n500\n\n\n1950s\n223\n\n\n1930s\n2"
  },
  {
    "objectID": "mp03.html#graph-description-frequency-of-musical-keys-polar-plot",
    "href": "mp03.html#graph-description-frequency-of-musical-keys-polar-plot",
    "title": "Creating the Ultimate Playlist",
    "section": "🎵 Graph Description – Frequency of Musical Keys (Polar Plot):",
    "text": "🎵 Graph Description – Frequency of Musical Keys (Polar Plot):\nThis polar plot illustrates the frequency of musical keys, represented as numbers from 0 to 11, where each number corresponds to a musical key (e.g., 0 = C, 1 = C♯/D♭, 2 = D, etc.). The circular layout reflects the cyclical nature of musical keys, akin to the Circle of Fifths in music theory.\nEach bar’s height indicates how often that key appears among the songs in user playlists. This visualization helps identify which keys are most common, offering insights into musical trends and preferences. Despite using numeric values, the cyclical arrangement of keys remains intuitive in this format.\n\n\nShow the code\n# Calculate the frequency of each key\nkey_frequency &lt;- inner_joined_data |&gt;\n  group_by(key) |&gt;\n  summarize(count = n()) |&gt;\n  arrange(desc(count))\n\n# Create a polar plot\nggplot(key_frequency, aes(x = as.factor(key), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  coord_polar(start = 0) +\n  theme_bw() +\n  labs(title = \"Frequency of Musical Keys Among Songs\",\n       x = \"Musical Key\",\n       y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 16),\n        plot.title = element_text(size = 20, face = \"bold\"))"
  },
  {
    "objectID": "mp03.html#track-duration-stats",
    "href": "mp03.html#track-duration-stats",
    "title": "Creating the Ultimate Playlist",
    "section": "Track Duration Stats",
    "text": "Track Duration Stats\n\n\nShow the code\n#convert duration in minutes \ninner_joined_data &lt;- inner_joined_data |&gt; \n  mutate(duration_min = duration_ms / 60000)\n\n# calculating mean and percentiles to see avg track length\nlength_info &lt;- inner_joined_data |&gt; \n  summarize( avg_length = mean(duration_min),\n    median_length = median(duration_min),\n    shortest = min(duration_min),\n    longest = max(duration_min),\n    p25 = quantile(duration_min, 0.25),\n    p75 = quantile(duration_min, 0.75)\n  )\nlength_info$avg_length &lt;- round(length_info$avg_length, 2)\nlength_info$median_length &lt;- round(length_info$median_length, 2)\nlength_info$shortest &lt;- round(length_info$shortest, 2)\nlength_info$longest &lt;- round(length_info$longest, 2)\nlength_info$p25 &lt;- round(length_info$p25, 2)\nlength_info$p75 &lt;- round(length_info$p75, 2)\n\n\nThe table below summarizes key statistics about the distribution of track lengths (in minutes) among the songs included in user playlists. It includes the average and median track length, as well as the shortest and longest tracks in the dataset. Additionally, the table shows the 25th percentile and 75th percentile values, which define the interquartile range (IQR) — the range that contains the middle 50% of all track lengths.\nWe can see from the table that most tracks tend to be the range 3.4 and 4.4 minutes, suggesting a preference for mid-length songs.\n\n\n\n\n\nAverage\nMedian\nShortest\nLongest\n25 Percentile\n75 Percentile\n\n\n\n\n3.97\n3.83\n0.64\n37.31\n3.4\n4.4\n\n\n\n\n\n\n\nShow the code\nggplot(inner_joined_data, aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.5, fill = \"darkorange\", color = \"white\") +\n  labs(\n    title = \"Distribution of Track Lengths in User Playlists\",\n    x = \"Track Length (minutes)\",\n    y = \"Number of Songs\"\n  ) +\n  theme_bw()+\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(size = 16),\n        plot.title = element_text(size = 20, face = \"bold\"))+\n  scale_x_continuous(limits = c(0, 10, 2))"
  },
  {
    "objectID": "mp03.html#defining-popularity",
    "href": "mp03.html#defining-popularity",
    "title": "Creating the Ultimate Playlist",
    "section": "Defining Popularity",
    "text": "Defining Popularity\n\n\nShow the code\n# defining stats using percentiles\npopularity_stats &lt;- inner_joined_data |&gt; \n  summarize(\n    min_pop = min(popularity),\n    max_pop = max(popularity),\n    mean_pop = mean(popularity),\n    median_pop = median(popularity),\n    p75 = quantile(popularity, 0.75),\n    p90 = quantile(popularity, 0.90)\n  )\n popularity_stats$mean_pop &lt;- round(popularity_stats$mean_pop, 2)\n\n\nTo explore how song popularity relates to playlist appearances, we examined the distribution of the popularity variable and set a threshold to define what counts as a “popular” songs.\n\n\n\n\n\nMin\nMax\nAverage\nMedian\n75 Percentile\n90 Percentile\n\n\n\n\n6\n92\n63.81\n65\n72\n77\n\n\n\n\n\nThe table above shows the popularity statistics of all the tracks in the dataset. We decide to use the 75 Percentile as a threshold to define a “Popular Song”, and the “90 Percentile” as a threshold for “Very Popular Songs”. Any track with a popularity index above 72 will be considered Popular , while any track with a popularity index above 77 will be considered Very Popular."
  },
  {
    "objectID": "mp03.html#songs-with-similar-tempo-key",
    "href": "mp03.html#songs-with-similar-tempo-key",
    "title": "Creating the Ultimate Playlist",
    "section": "🎛️ Songs with Similar Tempo & Key",
    "text": "🎛️ Songs with Similar Tempo & Key\nThe table below displays 251 songs that share a similar musical key and tempo with the selected anchor songs. These characteristics are commonly used by DJs to create smooth transitions between tracks. Songs were filtered to match the same key and have a tempo within ±5 BPM of the anchor song, ensuring musical coherence. The results are presented in the interactive table below for easy exploration."
  },
  {
    "objectID": "mp03.html#songs-by-same-artists",
    "href": "mp03.html#songs-by-same-artists",
    "title": "Creating the Ultimate Playlist",
    "section": "🎤 Songs by Same Artists",
    "text": "🎤 Songs by Same Artists\n\n\nShow the code\n# get the 2 artists from anchor songs\nartist1 &lt;- inner_joined_data|&gt;\n  filter(track_name == anchor1)|&gt; select(artist_name)|&gt;\n  head(1)\n\nartist2 &lt;- inner_joined_data|&gt;\n  filter(track_name == anchor2)|&gt; select(artist_name)|&gt;\n  head(1)\n\n# songs by same artists\nsame_artists &lt;- inner_joined_data |&gt;\n  filter(!(track_name %in% c(anchor1, anchor2))) |&gt;\n  filter(artist_name %in% c(artist1, artist2)) |&gt;\n  distinct(track_name, artist_name)\n\n\nThis table displays all songs by the same artists who performed the selected anchor songs, excluding the anchor songs themselves. These tracks were identified by matching the artist names and filtering for unique song titles. This approach highlights additional songs that share stylistic elements with the anchors, making them strong candidates for inclusion in a cohesive playlist."
  },
  {
    "objectID": "mp03.html#songs-released-in-the-same-year-with-similar-characteristics",
    "href": "mp03.html#songs-released-in-the-same-year-with-similar-characteristics",
    "title": "Creating the Ultimate Playlist",
    "section": "🎙️ Songs released in the same year with similar characteristics",
    "text": "🎙️ Songs released in the same year with similar characteristics\nTo identify songs that resemble the anchor tracks in musical characteristics, we averaged the acousticness, danceability, instrumentalness, and energy of both anchor songs. We then filtered songs within ±25% of these average values and from the same release years. The resulting table shows songs that closely match the overall style of the selected anchors.\n\n\nShow the code\nyear1&lt;- inner_joined_data|&gt;\n  filter(track_name  == anchor1)|&gt;\n  select(year)|&gt; head(1)\n\nyear2&lt;- inner_joined_data|&gt;\n  filter(track_name  == anchor2)|&gt;\n  select(year)|&gt; head(1)\n\n# update the anchor_stats\nanchor1_stats&lt;- inner_joined_data|&gt;\n  filter(track_name == anchor1) |&gt;\n  summarize(\n    acousticness = acousticness[1],\n    danceability = danceability[1],\n    liveness = liveness[1],\n    energy = energy[1]\n  )\n\nanchor2_stats&lt;- inner_joined_data|&gt;\n  filter(track_name == anchor2) |&gt;\n  summarize(\n    acousticness = acousticness[1],\n    danceability = danceability[1],\n    liveness = liveness[1],\n    energy = energy[1]\n  )\n\navrg_stats &lt;- bind_rows(anchor1_stats, anchor2_stats) |&gt;\n  summarize(\n    acousticness = mean(acousticness, na.rm = TRUE),\n    danceability = mean(danceability, na.rm = TRUE),\n    liveness = mean(liveness, na.rm = TRUE),\n    energy = mean(energy, na.rm = TRUE))\n\nsimilar_char &lt;- inner_joined_data|&gt;\n  filter(!(track_name %in% c(anchor1, anchor2)))|&gt;\n  filter( year %in% c(year1, year2))|&gt;\n  filter(\n    abs(acousticness - avrg_stats$acousticness) &lt;= 0.25 * avrg_stats$acousticness,\n    abs(danceability - avrg_stats$danceability) &lt;= 0.25 * avrg_stats$danceability,\n    abs(liveness - avrg_stats$liveness) &lt;= 0.25 * avrg_stats$liveness,\n    abs(energy - avrg_stats$energy) &lt;= 0.25 * avrg_stats$energy\n  )|&gt;\n  select(everything())|&gt;\n  distinct(track_name, .keep_all = TRUE)"
  },
  {
    "objectID": "mp03.html#finding-related-songs",
    "href": "mp03.html#finding-related-songs",
    "title": "Creating the Ultimate Playlist",
    "section": "🔊 Finding Related Songs",
    "text": "🔊 Finding Related Songs\n\n\nShow the code\n#Getting relevant tracks from heuristics\nrelevant_tracks &lt;- bind_rows(\n  co_occurring_songs|&gt; select(track_name),\n  same_artists|&gt; select(track_name),\n  similar_songs |&gt; select(track_name),\n  similar_char|&gt; select(track_name)\n  ) |&gt;\n    distinct()\n\n# non_popular songs\nplaylist_nonpopular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Not Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n#popular songs\nplaylist_popular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Very Popular\" |\n           popularity_level == \"Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n# getting 8 Non popular songs, 6 popular songs, 6 Very popular songs\n\nfin_playlist &lt;- bind_rows(\n  playlist_nonpopular|&gt; sample_n(8),\n  playlist_popular|&gt; filter(popularity_level == \"Popular\")|&gt;\n    sample_n(6),\n  playlist_popular|&gt; filter(popularity_level == \"Very Popular\")|&gt;\n    sample_n(6))"
  },
  {
    "objectID": "mp03.html#final-playlist-creation-based-on-heuristics-and-popularity",
    "href": "mp03.html#final-playlist-creation-based-on-heuristics-and-popularity",
    "title": "Creating the Ultimate Playlist",
    "section": "🎧 Final Playlist Creation Based on Heuristics and Popularity",
    "text": "🎧 Final Playlist Creation Based on Heuristics and Popularity\nTo build a well-rounded and personalized playlist, I began by identifying relevant tracks using several heuristic methods based on my two anchor songs:\n\nCo-occurring songs that frequently appear in the same playlists.\nTracks by the same artists as the anchor songs.\nSongs with a similar musical key and tempo, which would allow for smoother DJ transitions.\nSongs with similar audio characteristics, such as acousticness, danceability, energy, and liveness.\n\nThese heuristics were combined into a single list of relevant_tracks, ensuring only distinct track names were included.\nNext, I categorized the songs by popularity level: - Not Popular - Popular - Very Popular\nFinally, I curated a final playlist by sampling: - 🎶 8 non-popular songs\n- 🎶 6 popular songs\n- 🎶 6 very popular songs\nThis approach creates a balanced and diverse playlist, mixing well-known hits with lesser-known tracks that share musical and stylistic traits with the anchor songs."
  },
  {
    "objectID": "mp03.html#Finding-Related-Songs",
    "href": "mp03.html#Finding-Related-Songs",
    "title": "Creating the Ultimate Playlist",
    "section": "🔊 Finding Related Songs",
    "text": "🔊 Finding Related Songs\n\n\nShow the code\n#Getting relevant tracks from heuristics\nrelevant_tracks &lt;- bind_rows(\n  co_occurring_songs|&gt; select(track_name),\n  same_artists|&gt; select(track_name),\n  similar_songs |&gt; select(track_name),\n  similar_char|&gt; select(track_name)\n  ) |&gt;\n    distinct()\n\n# non_popular songs\nplaylist_nonpopular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Not Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n#popular songs\nplaylist_popular &lt;- inner_joined_data |&gt;\n  filter(name %in% relevant_tracks$track_name) |&gt;\n  filter(popularity_level == \"Very Popular\" |\n           popularity_level == \"Popular\")|&gt;\n  select(name, artist_name, popularity_level)|&gt;\n  distinct()\n\n# getting 8 Non popular songs, 6 popular songs, 6 Very popular songs\n\nfin_playlist &lt;- bind_rows(\n  playlist_nonpopular|&gt; sample_n(8),\n  playlist_popular|&gt; filter(popularity_level == \"Popular\")|&gt;\n    sample_n(6),\n  playlist_popular|&gt; filter(popularity_level == \"Very Popular\")|&gt;\n    sample_n(6))\n\n\nTo build a well-rounded and personalized playlist, we began by identifying relevant tracks using several heuristic methods based on my two anchor songs:\n\nCo-occurring songs that frequently appear in the same playlists.\nTracks by the same artists as the anchor songs.\nSongs with a similar musical key and tempo, which would allow for smoother DJ transitions.\nSongs with similar audio characteristics, such as acousticness, danceability, energy, and liveness.\n\nThese heuristics were combined into a single list of relevant_tracks, ensuring only distinct track names were included.\nNext, I categorized the songs by popularity level: - Not Popular - Popular - Very Popular\nFinally, I curated a final playlist by sampling:\n\n🎶 8 non-popular songs\n\n🎶 6 popular songs\n\n🎶 6 very popular songs\n\nThis approach creates a balanced and diverse playlist, mixing well-known hits with lesser-known tracks that share musical and stylistic traits with the anchor songs."
  },
  {
    "objectID": "mp03.html#final-playlist-based-on-heuristics-and-popularity-final-playlist",
    "href": "mp03.html#final-playlist-based-on-heuristics-and-popularity-final-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "🎧 Final Playlist Based on Heuristics and Popularity {final-playlist}",
    "text": "🎧 Final Playlist Based on Heuristics and Popularity {final-playlist}\n\n\nShow the code\nfin_playlist_12 &lt;- bind_rows(\n  playlist_nonpopular |&gt; sample_n(4),\n  playlist_popular |&gt; filter(popularity_level == \"Popular\") |&gt; sample_n(4),\n  playlist_popular |&gt; filter(popularity_level == \"Very Popular\") |&gt; sample_n(4)\n)\n\n# audio features for visualization\nplaylist_features &lt;- inner_join(fin_playlist_12, inner_joined_data, \n                                by = c(\"name\" = \"name\", \"artist_name\" = \"artist_name\")) |&gt;\n  distinct(name, artist_name, acousticness, energy, danceability, tempo, popularity) |&gt;\n  mutate(order = row_number())\n\n# Plotting evolution\nplaylist_features_long &lt;- playlist_features |&gt;\n  select(order, name, acousticness, energy, danceability, tempo) |&gt;\n  pivot_longer(-c(order, name), names_to = \"feature\", values_to = \"value\")"
  },
  {
    "objectID": "mp03.html#analysis-of-musical-structure",
    "href": "mp03.html#analysis-of-musical-structure",
    "title": "Creating the Ultimate Playlist",
    "section": "📊 Analysis of Musical Structure",
    "text": "📊 Analysis of Musical Structure\nTo understand the sonic journey of my playlist “Pulse & Echo”, I analyzed the evolution of key audio features provided by Spotify. These features include:\n\nAcousticness: Likelihood that a track is acoustic.\nEnergy: A measure of intensity and activity (e.g., fast, loud, noisy).\nDanceability: How suitable a track is for dancing based on tempo, rhythm stability, and beat strength.\nTempo: The overall speed of the song (measured in BPM).\n\nThe line chart below displays how these metrics change across the 12 songs in the playlist, following the track order:\n\n\nShow the code\n# Plot 1: Acousticness, Energy, Danceability\nplot1 &lt;- playlist_features|&gt;\n  select(name, acousticness, energy, danceability) |&gt;\n  pivot_longer(-name, names_to = \"Feature\", values_to = \"Value\") |&gt;\n  mutate(name = factor(name, levels = playlist_features$name)) |&gt;\n  ggplot(aes(x = name, y = Value, color = Feature, group = Feature)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Playlist Feature Progression\",\n       y = \"Value (0 to 1)\",\n       x = \"Track Name\",\n       color = \"Feature\")\n\n\n# Plot 2: Tempo\nplot2 &lt;- playlist_features |&gt;\n  ggplot(aes(x = factor(name, levels = playlist_features$name), y = tempo)) +\n  geom_line(group = 1, color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"steelblue\", size = 2) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Tempo Progression\",\n       y = \"Tempo (BPM)\",\n       x = \"Track Name\")\nplot1 / plot2"
  },
  {
    "objectID": "mp03.html#final-playlist",
    "href": "mp03.html#final-playlist",
    "title": "Creating the Ultimate Playlist",
    "section": "🎧 Final Playlist Based on Heuristics and Popularity",
    "text": "🎧 Final Playlist Based on Heuristics and Popularity\n\n\nShow the code\nfin_playlist_12 &lt;- bind_rows(\n  fin_playlist|&gt; filter(popularity_level == \"Not Popular\")|&gt; sample_n(4),\n  fin_playlist|&gt; filter(popularity_level == \"Popular\")|&gt; sample_n(4),\n  fin_playlist|&gt; filter(popularity_level == \"Very Popular\")|&gt; sample_n(4)\n) \n  \n\n# audio features for visualization\nplaylist_features &lt;- inner_join(fin_playlist_12, inner_joined_data, \n                                by = c(\"name\" = \"name\", \"artist_name\" = \"artist_name\")) |&gt;\n  distinct(name, artist_name, acousticness, energy, danceability, tempo, popularity) |&gt;\n  mutate(order = row_number())\n\n# Plotting evolution\nplaylist_features_long &lt;- playlist_features |&gt;\n  select(order, name, acousticness, energy, danceability, tempo) |&gt;\n  pivot_longer(-c(order, name), names_to = \"feature\", values_to = \"value\")"
  },
  {
    "objectID": "mp03.html#pulse-echo",
    "href": "mp03.html#pulse-echo",
    "title": "Creating the Ultimate Playlist",
    "section": "🎧 Pulse & Echo",
    "text": "🎧 Pulse & Echo\n\nThe Ultimate Playlist\nPulse & Echo is a 12-track playlist designed to guide the listener through a dynamic musical journey. The sequence weaves through smooth transitions, emotional drops, and uplifting peaks, offering both familiar favorites and new sonic discoveries. With selections spanning synthwave, pop, indie, and Latin crossover, this playlist balances mood, tempo, and energy for moments of reflection, movement, or pure vibe."
  },
  {
    "objectID": "mp03.html#design-principles",
    "href": "mp03.html#design-principles",
    "title": "Creating the Ultimate Playlist",
    "section": "🎯 Design Principles",
    "text": "🎯 Design Principles\n\n🔬 Quantitative Harmony\nSongs were selected based on closeness in tempo, key, and compatible values for acousticness, danceability, energy, and valence. All features were compared as a percentage deviation from the average anchor song values, ensuring smooth auditory transitions.\n\n\n📈 Emotional Trajectory\nThe playlist follows a “rise–fall–rise” pattern. It begins upbeat, dips into more introspective and atmospheric tracks midway, and picks up again with energetic closers—creating an arc that feels cinematic and emotionally resonant.\n\n\n🧮 Data-Driven Selection\nTracks were selected using a multi-step filtering process based on five key heuristics:\n\nCo-occurrence with anchor tracks in user playlists\n\nSame artist connections\n\nSongs in the same year with similar Spotify features\n\nKey and tempo compatibility\n\nFeature similarity within ±25% of anchor song averages"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "",
    "text": "Introduction\nData Import\nInitial Analysis of SEP Data\nDatasets Transformation\nExplore NTD Service Data\nCalculate Emissions\nNormalize Emissions to Transit Usage\nAward Winners\nConclusion"
  },
  {
    "objectID": "mp02.html#table-of-contents",
    "href": "mp02.html#table-of-contents",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "",
    "text": "Introduction\nData Import\nInitial Analysis of SEP Data\nDatasets Transformation\nExplore NTD Service Data\nCalculate Emissions\nNormalize Emissions to Transit Usage\nAward Winners\nConclusion"
  },
  {
    "objectID": "mp02.html#transit-sustainability-awards",
    "href": "mp02.html#transit-sustainability-awards",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "Transit Sustainability Awards",
    "text": "Transit Sustainability Awards\n\nGreenest Transit Agency (Lowest Emissions per Mile)\nAwarded to the agency with the lowest emissions per mile, recognizing the most environmentally efficient and sustainable transit operations.\n\n\nShow the code\n#greenest awards\n\nsmall_greenest &lt;- smallest_miles|&gt;\n  head(1)\n\nmedium_greenest &lt;- medium_miles|&gt;\n  head(1)\nlarge_greenest &lt;- large_miles|&gt;\n  head(1)\n\n\n\nSMALL : “Adirondack Transit Lines, Inc.: Adirondack Trailways” in Hurley, New York with 0.17lbs of CO2 emitted per mile travelled\nMEDIUM : “City of Seattle: Seattle Center Monorail” in Seattle, Washington with 0.08lbs of CO2 emitted per mile travelled\nLARGE : “Tri-County Metropolitan Transportation District of Oregon: TriMet” in Portland, Oregon with 0.12lbs of CO2 emitted per mile travelled\n\n\n\nShow the code\n# Calculate the average emission per agency size\naverage_emission &lt;- total_emission |&gt;\n  group_by(agency_size) |&gt;\n  summarise(mile_emission = mean(mile_emission, na.rm = TRUE)) |&gt;\n  mutate(category = \"Average Emission\")\n\n# Extract the most efficient agencies from each size category\nefficient_agencies &lt;- bind_rows(\n  smallest_miles |&gt; slice(1),\n  medium_miles |&gt; slice(1),\n  large_miles |&gt; slice(1)\n) |&gt;\n  mutate(category = \"Most Efficient\")\n\n# Combine both datasets for plotting\ncombined_data &lt;- bind_rows(average_emission, efficient_agencies)\n\ncombined_data$first_Agency &lt;- sub(\", dba\", \"\", combined_data$first_Agency)\n\n# Horizontal bar chart\nggplot(combined_data, aes(x = mile_emission, y = agency_size, fill = category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.6) + \n  labs(title = \"Emission Comparison: Most Efficient vs Average by Agency Size\",\n       x = \"CO2 Emissions per Mile (lbs)\",\n       y = \"Agency Size\",\n       fill = \"Category\") +\n  theme_bw() +\n  scale_fill_manual(values = c(\"Most Efficient\" = \"#98df8a\", \"Average Emission\" = \"#ffbb78\")) +\n  geom_text(aes(label = round(mile_emission, 2)), \n            position = position_dodge(width = 0.6), \n            hjust = -0.2, size = 4)+\n  geom_text(data = combined_data |&gt; filter(category == \"Most Efficient\"),\n            aes(label = first_Agency), \n            position = position_dodge(width = 0.6), \n            hjust =-0.2, vjust = -1.5 ,size = 4, fontface = \"bold\", color = \"black\")+\n  theme(legend.position = \"bottom\") +\n  theme(axis.title = element_text(size = 15),\n        plot.title = element_text(size = 20, face = \"bold\"))+\n  scale_x_continuous(expand = c(0, 0), limits = c(0, max(combined_data$mile_emission) * 1.1))\n\n\n\n\n\n\n\n\n\nThis graph compares the emissions per mile for the most efficient transit agency in each size category against the average emissions for that category. Each bar represents an agency size, with two bars per group: one showing the average emissions and the other highlighting the most efficient agency in that category.\nThe most efficient agencies have significantly lower emissions than the average, which suggests that certain agencies have successfully implemented cleaner technologies or more efficient operations. The names of these top-performing agencies are also displayed, emphasizing their standout performance in reducing emissions.\n\n\nMost Emissions Avoided (Comparison with Private Cars)\nThis award highlights the agency that has prevented the most CO₂ emissions by encouraging public transit use over private car travel.\nEmissions avoided are calculated based on private cars emitting 19.6 lbs CO₂ per gallon of fuel and averaging 25 MPG.\n\n\nShow the code\n# Assuming 25 MPG for veichles and 19.6 lbs of CO2 per gallon of gasoline\nMPG &lt;- 25\nco2_per_gallon &lt;- 19.6\n# calculating emission if passangers used car\ntotal_emission &lt;- total_emission|&gt;\n  mutate(\n    car_emission = (first_MILES / MPG) * co2_per_gallon,\n    emission_avoided = car_emission - tot_co2\n  )\n#identify top agency for each category  \nem_avoided_awards &lt;- total_emission|&gt;\n  group_by(agency_size)|&gt;\n  slice_max(emission_avoided, n=1)|&gt;\n  ungroup()\nem_avoided_awards$emission_avoided &lt;- round(em_avoided_awards$emission_avoided , digits = 0)|&gt;\n  number( , big.mark=\",\")\n\nem_avoided_awards$first_Agency &lt;- sub(\", dba\", \"\", em_avoided_awards$first_Agency )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY\nCITY\nSTATE\nAGENCY SIZE\nPOUNDS OF CO2 AVOIDED\n\n\n\n\nMTA New York City Transit\nBrooklyn\nNew York\nLarge\n5,395,411,402\n\n\nHudson Transit Lines, Inc.: Short Line\nMahwah\nNew Jersey\nMedium\n51,309,438\n\n\nHampton Jitney, Inc.\nCalverton\nNew York\nSmall\n28,931,084\n\n\n\n\n\n\n\nShow the code\n# Summarize data by agency size\nagency_emission_data &lt;- total_emission |&gt; \n  group_by(agency_size) |&gt; \n  summarize(Car_Emissions = sum(car_emission, na.rm = TRUE),\n            Total_Emissions = sum(tot_co2, na.rm = TRUE)) |&gt; \n  pivot_longer(cols = c(Car_Emissions, Total_Emissions), \n               names_to = \"Emission_Type\", values_to = \"Value\")\n\n# Bar plot with facets\nggplot(agency_emission_data, aes(x = Emission_Type, y = Value, fill = Emission_Type)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"Car_Emissions\" = \"#98df8a\", \"Total_Emissions\" = \"#ffbb78\"),\n                    labels = c(\"Car_Emissions\" = \"Emission Avoided\", \"Total_Emissions\" = \"Total Emission\")) +\n  labs(title = \"Emissions Breakdown by Agency Size\",\n       x = \"Emission Type\",\n       y = \"Emissions (lbs CO2)\",\n       fill = \"Emission Type\") + \n  facet_wrap(~ agency_size, scales = \"free_y\", nrow = 1) +  # Separate graphs per agency size\n  theme_bw()+\n  theme(axis.text.x = element_blank(),  \n        axis.ticks.x = element_blank(),\n        panel.spacing = unit(2, \"lines\"),  # Increase spacing between facets\n        strip.text = element_text(size = 14, face = \"bold\"),  # Make facet labels more visible\n        legend.text = element_text(size = 12),\n        panel.border = element_blank()) +\n  theme(axis.title = element_text(size = 15),\n        plot.title = element_text(size = 20, face = \"bold\", hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThe image demonstrates how using public transportation helps reduce emissions compared to cars. It shows that larger agencies are able to avoid more emissions because they have more people using public transport. The graph compares the emissions that would have been produced by cars with the emissions that are avoided by using public transportation, highlighting the environmental benefit of shifting away from private car use.\n\n\nBest Electrified Agency (Highest Percentage of Electric Propulsion)\nRecognizing the agency with the highest share of electric propulsion in its fleet, demonstrating a strong commitment to reducing greenhouse gas emissions and advancing sustainable mobility.\n\n\nShow the code\n# Define the fuel types and their energy content in Btu\nenergy_content_BTU &lt;- c(\n  \"Bio-Diesel\" = 138700,  \n  \"Bunker Fuel\" = 144500,  \n  \"C Natural Gas\" = 1030,  \n  \"Diesel Fuel\" = 137300,  \n  \"Ethanol\" = 114500,  \n  \"Gasoline\" = 115500,  \n  \"Hydrogen\" = 119980,  \n  \"Kerosene\" = 135000,  \n  \"Liquified Nat Gas\" = 1030,  \n  \"Liquified Petroleum Gas\" = 91500,  \n  \"Methonal\" = 56000  \n)\n\n# Conversion factor from Btu to kWh\nBTU_to_kWh &lt;- 0.000293071\n\n# Convert to kWh for all fuels \nemission_data &lt;- emission_data|&gt;\n  mutate(\n    energy_in_kWh = case_when(\n      Fuel_Type == \"Electric Battery\" ~ Fuel_Consumption,  # already in kWh\n      Fuel_Type == \"Electric Propulsion\" ~ Fuel_Consumption,  # already in kWh\n      TRUE ~ energy_content_BTU[Fuel_Type] * BTU_to_kWh  # Convert other fuels from Btu to kWh\n    ))\nemission_data$energy_in_kWh &lt;- round(emission_data$energy_in_kWh , digits = 1)\n\n# dividing the electric consumption compared to total fuel consumption\nelectric_agency &lt;- emission_data |&gt;\n  group_by(`NTD ID`) |&gt;\n  mutate(\n    tot = sum(Fuel_Consumption, na.rm = TRUE),  # Total fuel consumption for the agency\n    tot_electric = sum(Fuel_Consumption[Fuel_Type %in% c(\"Electric Battery\", \"Electric Propulsion\")], na.rm = TRUE)\n  )\nelectric_agency &lt;- electric_agency |&gt;\n  mutate(perc_electric = paste0(round((tot_electric / tot) * 100, 2), \"%\"))\n\n# defining small, medium, large agency \n\nelectric_agency &lt;- electric_agency |&gt;\n  mutate(agency_size = case_when(\n    UPT &lt;= quantile(UPT, 0.33, na.rm = TRUE) ~ \"Small\",\n    UPT &lt;= quantile(UPT, 0.67, na.rm = TRUE) ~ \"Medium\",\n    TRUE ~ \"Large\"\n  ))\n\n#identify top agency for each category  \nelectric_agency_top &lt;- electric_agency |&gt;\n  filter(!is.na(Agency) & !is.na(perc_electric)) |&gt;  # Remove missing values\n  arrange(agency_size, desc(perc_electric)) |&gt;  # Sort by size and electric %\n  group_by(agency_size) |&gt;  \n  slice(1) |&gt;  # Pick the highest per agency size\n  ungroup()\nelectric_agency_top$Agency &lt;- sub(\", dba\", \"\", electric_agency_top$Agency )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY\nCITY\nSTATE\nAGENCY SIZE\n% OF ELECTRIC OUT OF TOTAL FUEL CONSUMPTION\n\n\n\n\nTri-County Metropolitan Transportation District of Oregon: TriMet\nPortland\nOregon\nSmall\n99.16%\n\n\n\n\n\n\n\nWorst Polluter (Highest Emissions per Mile)\nGiven to the agency with the highest emissions per mile, emphasizing the need for targeted improvements in energy efficiency and electrification efforts.\n\n\nShow the code\nworst_polluter &lt;- total_emission |&gt; \n  filter(!is.na(mile_emission)) |&gt;  # Remove missing values\n  arrange(desc(mile_emission)) |&gt;  # Sort highest to lowest\n  group_by(agency_size) |&gt;  \n  slice(1) |&gt;  # Pick the highest per agency size\n  ungroup()\nworst_polluter$mile_emission &lt;- round(worst_polluter$mile_emission, digits = 2)\nworst_polluter$first_Agency &lt;- sub(\", dba\", \"\", worst_polluter$first_Agency)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGENCY\nCITY\nSTATE\nAGENCY SIZE\nPOUND OF CO2 PER MILE\n\n\n\n\nMETRO Regional Transit Authority\nAkron\nOhio\nLarge\n7.29\n\n\nErie Metropolitan Transit Authority: the e\nErie\nPennsylvania\nMedium\n15.00\n\n\nAltoona Metro Transit: AMTRAN\nAltoona\nPennsylvania\nSmall\n13.00\n\n\n\n\n\n\n\nShow the code\ntotal_emission$first_Agency &lt;- sub(\", dba\", \"\", total_emission$first_Agency)\n\nggplot(total_emission, aes(x = agency_size, y = mile_emission, color = agency_size)) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 3) +\n  labs(title = \"Pollution of Transit Agencies by Size\",\n       x = \"Agency Size\",\n       y = \"CO2 Emissions per Mile (lbs)\",\n       color = \"Agency Size\") +\n  # Label highest emission agencies for each size category\n  geom_text(data = total_emission |&gt;\n              group_by(agency_size) |&gt;\n              slice_max(mile_emission, n = 1) |&gt;\n              ungroup(), \n            aes(label = str_wrap(first_Agency, width = 20)),\n            vjust = -0.5, color = \"black\", size = 4, fontface = \"bold\") +\n  theme_bw()  +\n  theme(legend.position = \"none\") +\n  theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\"),\n        axis.title = element_text(size = 15),\n        plot.title = element_text(size = 20, face = \"bold\"))+\n  ylim(0, 18)\n\n\n\n\n\n\n\n\n\nThis graph shows the CO2 emissions per mile for different transit agencies, divided into small, medium, and large categories. Each point represents an agency, with the color indicating its size. The points are spread out to make it easier to see how emissions vary across agencies.\nThe agencies with the highest emissions in each category are marked with their names displayed next to the points. Lines connect these labels to the points, making it clear which agencies are the biggest polluters.\nThe purpose of this graph is to show how different agencies compare in terms of their emissions and to draw attention to the agencies that need to make the biggest improvements. It emphasizes the importance of targeting energy efficiency efforts, particularly for the agencies with the highest emissions."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "Identifying Environmentally Responsible US Public Transit Systems (Mini Project 2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nData from the Environmental Protection Agency (EPA) shows that public transportation reduces greenhouse gas emissions by up to 95% compared to private car use.↩︎"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Exploring Recent US Political Shifts",
    "section": "",
    "text": "Introduction\nAcquiring the data\nInitial Analysis\nReproducing NYT Figure"
  },
  {
    "objectID": "mp04.html#table-of-contents",
    "href": "mp04.html#table-of-contents",
    "title": "Exploring Recent US Political Shifts",
    "section": "",
    "text": "Introduction\nAcquiring the data\nInitial Analysis\nReproducing NYT Figure"
  },
  {
    "objectID": "Individual Final Report.html",
    "href": "Individual Final Report.html",
    "title": "Final Individual Report",
    "section": "",
    "text": "Introduction\nStock Market Data\nBond Data\nCommodities Data\nFinancial Metrics\nAssets Analysis\nAdaptive Portfolio Allocation\nConclusion"
  },
  {
    "objectID": "Individual Final Report.html#accessing-fred-data",
    "href": "Individual Final Report.html#accessing-fred-data",
    "title": "Final Individual Report",
    "section": "🌐 Accessing FRED Data",
    "text": "🌐 Accessing FRED Data\nThe following code retrieves macroeconomic time series data (e.g., short-term interest rates or inflation) from the Federal Reserve’s FRED API.\n\nThe function accepts a FRED series ID.\nIt sends an HTTP request, parses the JSON response, and returns a clean data.frame with date and value.\nThis is later used to calculate real returns or include macro indicators in portfolio modeling.\n\n\n\nShow the code\nFRED_key &lt;- readLines(\"FRED_key.txt\")\nget_fred&lt;- function(id){\n  base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations?series_id=\"\n  res &lt;- GET(paste0(base_url,id,\"&api_key=\",FRED_key,\"&file_type=json\"))\n  res_content &lt;- content(res, as = \"text\", encoding = \"UTF-8\")\n  json &lt;- fromJSON(res_content)\n  data &lt;-json$observations\n  data &lt;- data |&gt; mutate(value = as.numeric(value),# immediately convert to usable format\n                         date = as.Date(date))\n  return(data)\n}"
  },
  {
    "objectID": "Individual Final Report.html#alpha-vantage-data-function-asset-prices",
    "href": "Individual Final Report.html#alpha-vantage-data-function-asset-prices",
    "title": "Final Individual Report",
    "section": "📊 Alpha Vantage Data Function – Asset Prices",
    "text": "📊 Alpha Vantage Data Function – Asset Prices\nThis function collects monthly adjusted closing prices from Alpha Vantage for a given ticker (e.g., SPY, IWM, GLD).\n\nReturns a tidy data frame with adjusted close, dividend, and volume\nThe adjusted close price accounts for splits and dividends, making it appropriate for return calculation.\n\n\n\nShow the code\nAV_key &lt;- readLines(\"Alphavantage_key.txt\")\n\n# Function to get data from Alpha Vantage for a given ticker\nGET_AV &lt;- function(ticker){\n  url &lt;- paste0(\n    \"https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY_ADJUSTED\",\n    \"&symbol=\", ticker,\n    \"&apikey=\", AV_key\n  )\n  \n  res &lt;- GET(url)\n  res_content &lt;- content(res, as = \"text\", encoding = \"UTF-8\")\n  j &lt;- fromJSON(res_content, flatten = TRUE)\n  \n  # Check if data exists\n  if (is.null(j$`Monthly Adjusted Time Series`)) {\n    print(j)  # to show what was returned\n    stop(paste(\"No data returned for\", ticker, \"- possibly API limit reached or invalid ticker.\"))\n  }\n  \n  data &lt;- j$`Monthly Adjusted Time Series`\n  \n  df &lt;- data.frame(\n    date = as.Date(names(data)),\n    close = as.numeric(sapply(data, `[[`, \"4. close\")),\n    adjusted_close = as.numeric(sapply(data, `[[`, \"5. adjusted close\")),\n    low = as.numeric(sapply(data, `[[`, \"3. low\")),\n    volume = as.numeric(sapply(data, `[[`, \"6. volume\")),\n    dividend = as.numeric(sapply(data, `[[`, \"7. dividend amount\"))\n  )\n  \n  return(df)\n}"
  },
  {
    "objectID": "Individual Final Report.html#getting-stocks-data",
    "href": "Individual Final Report.html#getting-stocks-data",
    "title": "Final Individual Report",
    "section": "",
    "text": "Show the code\n# get S&P 500 Data\nspy_data &lt;- GET_AV(\"SPY\")\n\n# For Mid-Cap (MDY) S&P MidCap 400  \nmdy_data &lt;- GET_AV(\"MDY\")\n\n# For Small-Cap (IWM) Russell 2000 \niwm_data &lt;- GET_AV(\"IWM\")\n\n# international ETF \nvxus_data &lt;- GET_AV(\"VXUS\")\n\n#Vanguard FTSE Developed Markets ETF\nvea_data&lt;- GET_AV(\"VEA\")\n\n# Vanguard FTSE Emerging Markets ETF\nvwo_data &lt;- GET_AV(\"VWO\")\n\n# calculating log returns\nspy_data &lt;- spy_data |&gt;\n  arrange(date) |&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nmdy_data &lt;- mdy_data |&gt;\n  arrange(date) |&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\niwm_data &lt;- iwm_data |&gt;\n  arrange(date) |&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nvxus_data &lt;- vxus_data|&gt;\n  arrange(date)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nvea_data &lt;- vea_data|&gt;\n  arrange(date)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nvwo_data &lt;- vwo_data|&gt;\n  arrange(date)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\n\n\n\nShow the code\n#get Bond historical data\naaa_yield &lt;- get_fred(\"BAMLCC0A1AAATRIV\")|&gt;   #ICE BofA AAA US Corporate Index Effective Yield\n  select(date , AAA = value)\ngov10y_yield &lt;- get_fred(\"GS10\")|&gt;\n  select(date , Gov10Y = value)\ngov30y_yield &lt;- get_fred(\"GS30\")|&gt;\n  select(date , Gov30Y = value)\ngov3m_yield &lt;- get_fred(\"DTB3\")|&gt;\n  select(date , Gov3m = value)\n\nresample_to_monthly &lt;- function(df, yield_col) {\n  df |&gt;\n    mutate(month = format(date, \"%Y-%m\")) |&gt;\n    group_by(month) |&gt;\n    summarise(!!yield_col := ifelse(\n      # If there is monthly data, keep it; otherwise, take the last daily data point\n      any(!is.na(!!sym(yield_col))),\n      last(!!sym(yield_col), na.rm = TRUE),  # Last value of the month, if daily data exists\n      first(!!sym(yield_col))),  # Otherwise, keep the first monthly data\n      .groups = \"drop\") |&gt;\n    mutate(date = as.Date(paste0(month, \"-01\")))|&gt; select(-month)\n}\n\n# Apply to all the datasets\naaa_yield_monthly &lt;- resample_to_monthly(aaa_yield, \"AAA\")\ngov10y_yield_monthly &lt;- resample_to_monthly(gov10y_yield, \"Gov10Y\")\ngov30y_yield_monthly &lt;- resample_to_monthly(gov30y_yield, \"Gov30Y\")\ngov3m_yield_monthly &lt;- resample_to_monthly(gov3m_yield, \"Gov3m\")\n\n# Combine all bond yield data (now on a monthly basis)\ncombined_bond_yields &lt;- reduce(\n  list(aaa_yield_monthly, gov10y_yield_monthly, gov30y_yield_monthly, gov3m_yield_monthly),\n  full_join,\n  by = \"date\"\n)\n\n\nyield_data_long &lt;- combined_bond_yields |&gt;\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"yield\") |&gt;\n  arrange(ticker, date) |&gt;\n  mutate(\n    log_return = (yield / 100),  # Monthly return approximation (annual yield divided by 100)\n    adjusted_close = yield      # adjusted_close should be the yield value for bonds\n  ) |&gt;\n  select(date, ticker, adjusted_close, log_return)\n\n\n# commodities \n\ncombined_data_commodities &lt;- bind_rows(\n  GET_AV(\"VNQ\") |&gt;\n    arrange(date) |&gt;\n    mutate(ticker = \"VNQ\"),\n  GET_AV(\"GLD\") |&gt;\n    arrange(date) |&gt;\n    mutate(ticker = \"GLD\"),\n  GET_AV(\"DBA\") |&gt;\n    arrange(date) |&gt;\n    mutate(ticker = \"DBA\")\n)\n\ncombined_data_commodities &lt;- combined_data_commodities|&gt; \n  group_by(ticker)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))|&gt;\n  ungroup()\n\n\ncombined_data_stock &lt;- bind_rows(\n  spy_data|&gt; mutate(ticker = \"SPY\"),\n  mdy_data|&gt; mutate(ticker = \"MDY\"),\n  iwm_data|&gt; mutate(ticker = \"IWM\"),\n  vxus_data|&gt; mutate(ticker = \"VXUS\"),\n  vea_data|&gt; mutate(ticker = \"VEA\"),\n  vwo_data|&gt; mutate(ticker = \"VWO\")\n)\n\n\n\ncombined_data &lt;- bind_rows(\n  combined_data_stock,\n  combined_data_commodities,\n  yield_data_long\n)\n\nstart_date &lt;- as.Date(\"2005-01-01\") # Not NECESSARY\ncombined_data &lt;- combined_data |&gt;\n  filter(date &gt;= start_date)\n\n\nyearly_returns &lt;- combined_data |&gt;\n  drop_na(log_return) |&gt;\n  group_by(ticker) |&gt;\n  arrange(date) |&gt;\n  summarise(\n    initial_price = first(adjusted_close),\n    final_price   = last(adjusted_close),\n    years         = as.numeric(difftime(last(date), first(date), units = \"days\")) / 365.25,\n    total_return_pct = (final_price / initial_price - 1) * 100,\n    CAGR_pct      = ((final_price / initial_price)^(1 / years) - 1) * 100,\n    volatility    = sd(log_return, na.rm = TRUE) * sqrt(12),\n    sharpe_ratio  = (CAGR_pct / 100 - 0.02) / (volatility / 100)\n  )\n\n\n\nget_portfolio_weights &lt;- function(age) {\n  if (age &lt; 30) {\n    weights &lt;- c(stocks = 0.90, bonds = 0.05, commodities = 0.05)\n  } else if (age &gt;= 30 & age &lt;= 34) {\n    weights &lt;- c(stocks = 0.80, bonds = 0.10, commodities = 0.10)\n  } else if (age &gt;= 35 & age &lt;= 39) {\n    weights &lt;- c(stocks = 0.75, bonds = 0.15, commodities = 0.10)\n  } else if (age &gt;= 40 & age &lt;= 44) {\n    weights &lt;- c(stocks = 0.70, bonds = 0.20, commodities = 0.10)\n  } else if (age &gt;= 45 & age &lt;= 49) {\n    weights &lt;- c(stocks = 0.65, bonds = 0.25, commodities = 0.10)\n  } else if (age &gt;= 50 & age &lt;= 54) {\n    weights &lt;- c(stocks = 0.575, bonds = 0.30, commodities = 0.125)\n  } else if (age &gt;= 55 & age &lt;= 59) {\n    weights &lt;- c(stocks = 0.50, bonds = 0.35, commodities = 0.15)\n  } else if (age &gt;= 60 & age &lt;= 64) {\n    weights &lt;- c(stocks = 0.40, bonds = 0.45, commodities = 0.15)\n  } else if (age &gt;= 65 & age &lt;= 69) {\n    weights &lt;- c(stocks = 0.30, bonds = 0.55, commodities = 0.15)\n  } else {\n    weights &lt;- c(stocks = 0.15, bonds = 0.65, commodities = 0.20)\n  }\n  return(weights)\n}\n\n# Function to assign weights based on Sharpe ratios within each class\nassign_weights_by_age &lt;- function(yearly_returns, age) {\n  allocation &lt;- get_portfolio_weights(age)\n  \n  # Define ticker groups\n  stock_tickers &lt;- c(\"SPY\", \"MDY\", \"IWM\", \"VXUS\")\n  bond_tickers &lt;- c(\"AAA\", \"Gov10Y\", \"Gov30Y\", \"Gov3m\")\n  commodity_tickers &lt;- c(\"DBA\", \"VNQ\", \"GLD\")\n  \n  # Tag each asset by class\n  yearly_returns &lt;- yearly_returns |&gt;\n    mutate(class = case_when(\n      ticker %in% stock_tickers ~ \"stocks\",\n      ticker %in% bond_tickers ~ \"bonds\",\n      ticker %in% commodity_tickers ~ \"commodities\",\n      TRUE ~ \"other\"\n    ))\n  \n  # Separate bonds to handle fallback logic\n  bonds &lt;- yearly_returns |&gt; filter(class == \"bonds\")\n  other_assets &lt;- yearly_returns |&gt; filter(class != \"bonds\")\n  \n  # Check if all bond Sharpe ratios are &lt;= 0\n  all_bond_sharpe_negative &lt;- all(bonds$sharpe_ratio &lt;= 0 | is.na(bonds$sharpe_ratio))\n  \n  if (all_bond_sharpe_negative) {\n    # Use yield-to-risk ratio for bonds if Sharpe ratios are all bad\n    bonds &lt;- bonds |&gt;\n      mutate(\n        yield_to_risk = ifelse(volatility == 0, 0, CAGR_pct / volatility),\n        raw_weight = yield_to_risk / sum(yield_to_risk, na.rm = TRUE),\n        final_weight = raw_weight * allocation[[\"bonds\"]]\n      )\n  } else {\n    # Use Sharpe as usual\n    bonds &lt;- bonds |&gt;\n      mutate(\n        adj_sharpe = ifelse(is.na(sharpe_ratio) | sharpe_ratio &lt; 0, 0, sharpe_ratio),\n        raw_weight = adj_sharpe / sum(adj_sharpe, na.rm = TRUE),\n        final_weight = raw_weight * allocation[[\"bonds\"]]\n      )\n  }\n  \n  # Handle other asset classes (stocks, commodities) with Sharpe\n  other_assets &lt;- other_assets |&gt;\n    filter(class %in% names(allocation)) |&gt;\n    group_by(class) |&gt;\n    mutate(\n      adj_sharpe = ifelse(is.na(sharpe_ratio) | sharpe_ratio &lt; 0, 0, sharpe_ratio),\n      raw_weight = adj_sharpe / sum(adj_sharpe, na.rm = TRUE),\n      final_weight = raw_weight * allocation[class]\n    ) |&gt;\n    ungroup()\n  \n  # Combine and return\n  bind_rows(bonds, other_assets) |&gt;\n    select(ticker, class, CAGR_pct, volatility, sharpe_ratio, final_weight)\n}\n\n\n\n\nShow the code\n#get inflation data\ncpi_data &lt;- get_fred(\"CPIAUCSL\")|&gt;\n  select(date , CPI = value)\n# get wage data\nwage_data &lt;- get_fred(\"CES0500000003\")|&gt;\n  select(date , WAGE = value)\n\n\nggplot(cpi_data, aes(x = date, y = CPI)) +\n  geom_line(color = \"steelblue\") +\n  labs(title = \"Consumer Price Index (CPI)\",\n       x = \"Date\", y = \"Index Value\")\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(wage_data, aes(x = date, y = WAGE)) +\n  geom_line(color = \"darkgreen\") +\n  labs(title = \"Average Hourly Earnings (Total Private)\",\n       x = \"Date\", y = \"Earnings (USD)\")"
  },
  {
    "objectID": "Individual Final Report.html#getting-stocks-data-1",
    "href": "Individual Final Report.html#getting-stocks-data-1",
    "title": "Final Individual Report",
    "section": "",
    "text": "Show the code\nget_or_load_data &lt;- function(ticker) {\n  filename &lt;- paste0(ticker, \"_data.csv\")\n  \n  if (!file.exists(filename)) {\n    message(\"Fetching data for: \", ticker)\n    data &lt;- GET_AV(ticker)\n    \n    if (!is.null(data)) {\n      write.csv(data, filename, row.names = FALSE)\n    } else {\n      warning(\"Data for \", ticker, \" could not be retrieved.\")\n      return(NULL)\n    }\n  } else {\n    message(\"Loading cached data for: \", ticker)\n    data &lt;- read.csv(filename) |&gt;\n      mutate(date = as.Date(date))  # ensure date column is in Date format\n  }\n  \n  data$ticker &lt;- ticker\n  return(data)\n}\n\n# Now apply to each ticker\nspy_data  &lt;- get_or_load_data(\"SPY\")\nmdy_data  &lt;- get_or_load_data(\"MDY\")\niwm_data  &lt;- get_or_load_data(\"IWM\")\nvxus_data &lt;- get_or_load_data(\"VXUS\")\nvea_data  &lt;- get_or_load_data(\"VEA\")\nvwo_data  &lt;- get_or_load_data(\"VWO\")\n\n\n# calculating log returns\nspy_data &lt;- spy_data |&gt;\n  arrange(date) |&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nmdy_data &lt;- mdy_data |&gt;\n  arrange(date) |&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\niwm_data &lt;- iwm_data |&gt;\n  arrange(date) |&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nvxus_data &lt;- vxus_data|&gt;\n  arrange(date)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nvea_data &lt;- vea_data|&gt;\n  arrange(date)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))\n\nvwo_data &lt;- vwo_data|&gt;\n  arrange(date)|&gt;\n  mutate(log_return = log(adjusted_close / lag(adjusted_close)))"
  },
  {
    "objectID": "Individual Final Report.html#annualized-returns-and-risk-metrics",
    "href": "Individual Final Report.html#annualized-returns-and-risk-metrics",
    "title": "Final Individual Report",
    "section": "📈 Annualized Returns and Risk Metrics",
    "text": "📈 Annualized Returns and Risk Metrics\nTo evaluate long-term performance across different asset classes—including U.S. stocks, international developed markets, and emerging markets—I calculated a set of standardized financial metrics using monthly log return data. The goal was to estimate both total and risk-adjusted returns over the observed period.\n\n\nShow the code\nyearly_returns &lt;- combined_data |&gt;\n  drop_na(log_return) |&gt;\n  group_by(ticker) |&gt;\n  arrange(date) |&gt;\n  summarise(\n    initial_price = first(adjusted_close),\n    final_price   = last(adjusted_close),\n    years         = as.numeric(difftime(last(date), first(date), units = \"days\")) / 365.25,\n    total_return_pct = (final_price / initial_price - 1) * 100,\n    CAGR_pct      = ((final_price / initial_price)^(1 / years) - 1)  ,\n    volatility    = sd(log_return, na.rm = TRUE) * sqrt(12) ,\n    sharpe_ratio  = (CAGR_pct  - 0.02) / (volatility )\n  )\n\nyearly_returns$CAGR_pct &lt;- yearly_returns$CAGR_pct * 100\nyearly_returns$volatility &lt;- yearly_returns$volatility *100\n\n\n\n🔍 Explanation of Code\n\ndrop_na(log_return): Removes missing values to ensure calculations are valid and consistent across all assets.\ngroup_by(ticker): Performs all subsequent calculations separately for each asset (ETF) in the dataset.\ninitial_price, final_price: Capture the first and last adjusted prices for each asset, forming the basis for return calculations.\nyears: Calculates the total time span (in years) of the available data for each asset.\ntotal_return_pct: Computes the total percentage return over the entire period, from the initial to the final price.\nCAGR_pct (Compound Annual Growth Rate): Reflects the mean annual growth rate of an investment over the time period, accounting for compounding.\nvolatility: Measures the annualized standard deviation of the monthly log returns, serving as a proxy for investment risk.\nsharpe_ratio: Calculates the Sharpe Ratio, which adjusts the return by its risk. It uses a 2% annual risk-free rate to assess how efficiently an asset compensates investors for the risk taken.\n\nThese metrics collectively help compare the long-term performance and risk profile of U.S. markets, developed"
  },
  {
    "objectID": "Individual Final Report.html#data-caching-function",
    "href": "Individual Final Report.html#data-caching-function",
    "title": "Final Individual Report",
    "section": "📂 Data Caching Function",
    "text": "📂 Data Caching Function\nTo avoid downloading the same data multiple times, I created a function that checks if the data for a specific ticker (like SPY or VXUS) is already saved on my computer.\n\nIf it’s not saved yet, it downloads the data and saves it as a CSV file.\nIf it’s already saved, it simply loads the data from the file.\nThis helps save time and avoid hitting limits with the API. It also makes the analysis faster and more efficient.\n\n\n\nShow the code\nget_or_load_data &lt;- function(ticker) {\n  filename &lt;- paste0(ticker, \"_data.csv\")\n  \n  if (!file.exists(filename)) {\n    message(\"Fetching data for: \", ticker)\n    data &lt;- GET_AV(ticker)\n    \n    if (!is.null(data)) {\n      write.csv(data, filename, row.names = FALSE)\n    } else {\n      warning(\"Data for \", ticker, \" could not be retrieved.\")\n      return(NULL)\n    }\n  } else {\n    message(\"Loading cached data for: \", ticker)\n    data &lt;- read.csv(filename) |&gt;\n      mutate(date = as.Date(date))  # ensure date column is in Date format\n  }\n  \n  data$ticker &lt;- ticker\n  return(data)\n}\n\n# Now apply to each ticker\nspy_data  &lt;- get_or_load_data(\"SPY\")\nmdy_data  &lt;- get_or_load_data(\"MDY\")\niwm_data  &lt;- get_or_load_data(\"IWM\")\nvxus_data &lt;- get_or_load_data(\"VXUS\")\nvea_data  &lt;- get_or_load_data(\"VEA\")\nvwo_data  &lt;- get_or_load_data(\"VWO\")"
  },
  {
    "objectID": "Individual Final Report.html#financial-metric-formulas",
    "href": "Individual Final Report.html#financial-metric-formulas",
    "title": "Final Individual Report",
    "section": "📐 Financial Metric Formulas",
    "text": "📐 Financial Metric Formulas\nThe financial formulas used in the project to calculate Risk and Returns or the assets are shown below.\n\n1. Total Return (%)\n\\[\n\\text{Total Return (\\%)} = \\left( \\frac{P_{\\text{final}} - P_{\\text{initial}}}{P_{\\text{initial}}} \\right) \\times 100\n\\]\nWhere:\n\n\\(P_{final}\\) : Final price of the asset\n\n\\(P_{initial}\\) : Initial price of the asset\n\n\n\n2. Compound Annual Growth Rate (CAGR %)\n\\[\n\\text{CAGR \\%} = \\left( \\left( \\frac{P_{\\text{final}}}{P_{\\text{initial}}} \\right)^{\\frac{1}{n}} - 1 \\right) \\times 100\n\\]\nWhere:\n\n\\(n\\) : Number of years the asset is held\n\n\n\n\n3. Annualized Volatility\n\\[\n\\text{Volatility} = \\sigma_{\\text{monthly returns}} \\times \\sqrt{12}\n\\]\nWhere:\n\n\\(\\sigma_{\\text{monthly returns}}\\) : Standard deviation of monthly log returns\n\n\n\n\n4. Sharpe Ratio\n\\[\n\\text{Sharpe Ratio} = \\frac{\\text{CAGR} - r_f}{\\text{Volatility}}\n\\]\nWhere:\n- \\({r_f}\\) : Risk-free rate (assumed to be 2% in this project)\n- Volatility: Annualized standard deviation of returns"
  },
  {
    "objectID": "Individual Final Report.html#table-of-contents",
    "href": "Individual Final Report.html#table-of-contents",
    "title": "Final Individual Report",
    "section": "",
    "text": "Introduction\nStock Market Data\nBond Data\nCommodities Data\nFinancial Metrics\nAssets Analysis\nAdaptive Portfolio Allocation\nConclusion"
  },
  {
    "objectID": "Individual Final Report.html#research-question",
    "href": "Individual Final Report.html#research-question",
    "title": "Final Individual Report",
    "section": "Research Question",
    "text": "Research Question\nThe individual Research Question that I have analyzed for this project is:\n\nAre developed international markets more effective than emerging markets for long-term portfolio growth, and how do they compare to the United States?\nTo provide a comprehensive perspective, this project goes beyond equities. It also incorporates additional asset classes—bonds, real estate investment trusts (REITs), commodities , and inflation-protected securities—to construct portfolios that are more resilient to market volatility and macroeconomic shifts. These non-equity assets serve as important diversifiers, helping to reduce overall portfolio risk while maintaining potential for steady returns.\nIn addition, the project develops an optimal allocation strategy tailored to investor age. This dynamic, lifecycle-based approach adjusts asset weights over time to match evolving risk tolerance and investment horizons. Younger investors are allocated toward higher-growth, higher-volatility assets like equities and emerging markets, while older investors transition into more stable assets like bonds and REITs. This adaptive allocation is grounded in modern portfolio theory and aims to balance growth and preservation of capital throughout different life stages."
  },
  {
    "objectID": "question 2.html",
    "href": "question 2.html",
    "title": "Analysis",
    "section": "",
    "text": "title: “Financial Rebalancing” format: html\ngetwd() ## Importing data\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.4.3\n\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(httr)\n\nWarning: package 'httr' was built under R version 4.4.3\n\nlibrary(jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.4.3\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nsetwd(\"C:/Users/orazz/OneDrive - The City University of New York (1)/MASTER/STA9750-2025-SPRING\")\n# setwd(\"C:/Users/salda/OneDrive/Documents/money team\")\nFRED_key &lt;- readLines(\"FRED_key.txt\")\n\nWarning in readLines(\"FRED_key.txt\"): incomplete final line found on\n'FRED_key.txt'\n\nget_fred&lt;- function(id){\n  base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations?series_id=\"\n  res &lt;- GET(paste0(base_url,id,\"&api_key=\",FRED_key,\"&file_type=json\"))\n  res_content &lt;- content(res, as = \"text\", encoding = \"UTF-8\")\n  json &lt;- fromJSON(res_content)\n  data &lt;-json$observations\n  data &lt;- data |&gt; mutate(value = as.numeric(value),# immediately convert to usable format\n                         date = as.Date(date))\n  return(data)\n}\n\n# Read your Alpha Vantage key\nAV_key &lt;- readLines(\"Alphavantage_key.txt\")\n\nWarning in readLines(\"Alphavantage_key.txt\"): incomplete final line found on\n'Alphavantage_key.txt'\n\n# Function to get data from Alpha Vantage for a given ticker\nGET_AV &lt;- function(ticker){\n  \n  # Build the URL for the monthly adjusted time series\n  url &lt;- paste0(\n    \"https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY_ADJUSTED\",\n    \"&symbol=\", ticker,\n    \"&apikey=\", AV_key\n  )\n  \n  # Make the GET request and parse the JSON response\n  res &lt;- GET(url)\n  res_content &lt;- content(res, as = \"text\", encoding = \"UTF-8\")\n  j &lt;- fromJSON(res_content, flatten = TRUE)\n  \n  # Extract the \"Monthly Adjusted Time Series\" data\n  data &lt;- j$`Monthly Adjusted Time Series`\n  \n  # Create empty vectors to store our data\n  close &lt;- c()\n  adjusted_close &lt;- c()\n  low &lt;- c()\n  volume &lt;- c()\n  dividend &lt;- c()\n  \n  # Loop over each element in the data to unpack the values.\n  for(i in seq_along(data)){\n    close &lt;- append(close, data[[i]][[\"4. close\"]])\n    adjusted_close &lt;- append(adjusted_close, data[[i]][[\"5. adjusted close\"]])\n    low &lt;- append(low, data[[i]][[\"3. low\"]])\n    volume &lt;- append(volume, data[[i]][[\"6. volume\"]])\n    dividend &lt;- append(dividend, data[[i]][[\"7. dividend amount\"]])\n  }\n  \n  # Build the data frame\n  df &lt;- data.frame(\n    date = as.Date(names(data)),\n    close = as.numeric(close),\n    adjusted_close = as.numeric(adjusted_close),\n    low = as.numeric(low),\n    volume = as.numeric(volume),\n    dividend = as.numeric(dividend)\n  )\n  \n  return(df)\n}"
  },
  {
    "objectID": "question 2.html#gdp-per-capita",
    "href": "question 2.html#gdp-per-capita",
    "title": "Analysis",
    "section": "GDP Per Capita",
    "text": "GDP Per Capita\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:jsonlite':\n\n    flatten\n\n# Named vector of FRED IDs (replaced Brazil with Mexico)\nseries_ids &lt;- c(\n  USA          = \"A939RC0Q052SBEA\",\n  China        = \"PCAGDPCNA646NWDB\",\n  Germany      = \"PCAGDPDEA646NWDB\",\n  India        = \"PCAGDPINA646NWDB\",\n  Japan        = \"PCAGDPJPA646NWDB\",\n  UK           = \"PCAGDPGBA646NWDB\",\n  France       = \"PCAGDPFRA646NWDB\",\n  Italy        = \"PCAGDPITA646NWDB\",\n  Mexico       = \"PCAGDPMXA646NWDB\",   # was Brazil\n  `South Korea`= \"PCAGDPKRA646NWDB\"\n)\n\n# Pull, tag, combine, and then filter to 1960-01-01 onward\ngdp_pc_long &lt;- series_ids |&gt;\n  imap_dfr(function(id, country) {\n    get_fred(id) |&gt;\n      select(date, value) |&gt;\n      mutate(\n        country = country,\n        gdp_pc  = value\n      ) |&gt;\n      select(date, country, gdp_pc)\n  }) |&gt;\n  filter(date &gt;= as.Date(\"1960-01-01\"))\n\n# Pivot to wide form (if you need it)\ngdp_pc_wide &lt;- gdp_pc_long |&gt;\n  pivot_wider(\n    names_from  = country,\n    values_from = gdp_pc\n  )\n\nhead(gdp_pc_long)\n\n        date country gdp_pc\n1 1960-01-01     USA   3026\n2 1960-04-01     USA   2999\n3 1960-07-01     USA   3009\n4 1960-10-01     USA   2968\n5 1961-01-01     USA   2984\n6 1961-04-01     USA   3030\n\n# And the plot:\ngdp_pc_long |&gt;\n  ggplot(aes(x = date, y = gdp_pc, color = country)) +\n    geom_line(size = 1) +\n    labs(\n      title = \"GDP per Capita (1960 onward)\",\n      x     = NULL,\n      y     = \"GDP per Capita (USD)\",\n      color = NULL\n    ) +\n    theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n#Inflation ::: {.cell}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(purrr)\n\n# Updated named vector of FRED IDs for inflation (Mexico instead of Brazil)\ninflation_ids &lt;- c(\n  USA           = \"FPCPITOTLZGUSA\",\n  China         = \"FPCPITOTLZGCHN\",\n  Germany       = \"FPCPITOTLZGDEU\",\n  India         = \"FPCPITOTLZGIND\",\n  Japan         = \"FPCPITOTLZGJPN\",\n  UK            = \"FPCPITOTLZGGBR\",\n  France        = \"FPCPITOTLZGFRA\",\n  Italy         = \"FPCPITOTLZGITA\",\n  Mexico        = \"FPCPITOTLZGMEX\",\n  `South Korea` = \"FPCPITOTLZGKOR\"\n)\n\n# Fetch, tag, and combine into one long tibble\ninflation_long &lt;- inflation_ids |&gt;\n  imap_dfr(function(id, country) {\n    get_fred(id) |&gt;\n      select(date, value) |&gt;\n      mutate(\n        country   = country,\n        inflation = value\n      ) |&gt;\n      select(date, country, inflation)\n  })\n\n# Quick peek\nhead(inflation_long)\n\n        date country inflation\n1 1960-01-01     USA  1.457976\n2 1961-01-01     USA  1.070724\n3 1962-01-01     USA  1.198773\n4 1963-01-01     USA  1.239669\n5 1964-01-01     USA  1.278912\n6 1965-01-01     USA  1.585169\n\n# Plot\ninflation_long |&gt;\n  ggplot(aes(x = date, y = inflation, color = country)) +\n    geom_line(size = 1) +\n    labs(\n      title    = \"Annual Consumer-Price Inflation\",\n      subtitle = \"Inflation, consumer prices (annual % change) for selected economies\",\n      x        = NULL,\n      y        = \"Inflation Rate (%)\",\n      color    = NULL\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title    = element_text(size = 16, face = \"bold\"),\n      plot.subtitle = element_text(size = 12),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "question 2.html#etfs-for-each-of-the-nations",
    "href": "question 2.html#etfs-for-each-of-the-nations",
    "title": "Analysis",
    "section": "ETFs for each of the nations",
    "text": "ETFs for each of the nations\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\n\n# Named vector of ETF tickers for each country\n# USA: SPY (S&P 500), China: FXI (iShares China Large-Cap), Germany: EWG (iShares MSCI Germany),\n# India: INDA (iShares MSCI India), Japan: EWJ (iShares MSCI Japan), UK: EWU (iShares MSCI United Kingdom),\n# France: EWQ (iShares MSCI France), Italy: EWI (iShares MSCI Italy)\netf_tickers &lt;- c(\n  USA     = \"SPY\",\n  China   = \"FXI\",\n  Germany = \"EWG\",\n  India   = \"INDA\",\n  Japan   = \"EWJ\",\n  UK      = \"EWU\",\n  France  = \"EWQ\",\n  Italy   = \"EWI\"\n)\n\n# Fetch, tag, and combine ETF price histories\netf_prices_long &lt;- etf_tickers |&gt;\n  imap_dfr(function(ticker, country) {\n    GET_AV(ticker) |&gt;\n      select(date, adjusted_close) |&gt;\n      mutate(\n        country = country,\n        price   = adjusted_close\n      ) |&gt;\n      select(date, country, price)\n  })\n\n# Quick look\nhead(etf_prices_long)\n\n        date country    price\n1 2025-05-13     USA 586.8400\n2 2025-04-30     USA 554.5400\n3 2025-03-31     USA 559.3900\n4 2025-02-28     USA 592.3990\n5 2025-01-31     USA 600.0161\n6 2024-12-31     USA 584.3233\n\n#&gt; # A tibble: 6 × 3\n#&gt;   date       country price\n#&gt;   &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 1993-01-29 USA      43.5\n#&gt; 2 1993-02-26 USA      44.1\n#&gt; 3 1993-03-31 USA      44.8\n#&gt; 4 1993-04-30 USA      45.4\n#&gt; 5 1993-05-28 USA      45.2\n#&gt; 6 1993-06-30 USA      45.5\n\n# Plot the price history for each ETF\netf_prices_long |&gt;\n  ggplot(aes(x = date, y = price, color = country)) +\n    geom_line(size = 1) +\n    labs(\n      title    = \"ETF Price History by Country\",\n      subtitle = \"Adjusted closing prices for representative country ETFs\",\n      x        = NULL,\n      y        = \"Adjusted Close Price (USD)\",\n      color    = NULL\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title     = element_text(size = 16, face = \"bold\"),\n      plot.subtitle  = element_text(size = 12),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# Pivot to wide form so each country is a column\netf_prices_wide &lt;- etf_prices_long |&gt;\n  pivot_wider(\n    names_from  = country,\n    values_from = price\n  )\n\n# Print the first 10 rows in the console\nhead(etf_prices_wide, 10)\n\n# A tibble: 10 × 9\n   date         USA China Germany India Japan    UK France Italy\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 2025-05-13  587.  35.7    40.6  54.0  72.2  38.4   42.1  46  \n 2 2025-04-30  555.  33.9    39.5  53.6  71.4  38.1   41.0  44.2\n 3 2025-03-31  559.  35.8    37.1  51.5  68.6  37.5   39.8  42.3\n 4 2025-02-28  592.  35.2    36.1  48.1  68.5  37.0   39.5  40.8\n 5 2025-01-31  600.  31.9    34.6  51.0  68.3  35.6   38.6  38.6\n 6 2024-12-31  584.  30.4    31.8  52.6  67.1  33.9   35.9  36.0\n 7 2024-11-29  599.  29.9    32.3  54.4  68.6  35.1   36.0  35.8\n 8 2024-10-31  565.  31.1    32.3  54.4  67.0  34.8   37.6  37.4\n 9 2024-09-30  570.  31.3    33.8  58.1  70.4  36.6   40.1  38.4\n10 2024-08-30  558.  26.1    32.6  57.3  70.9  36.8   39.6  38.3\n\n# (Optional) For a nicer table in R Markdown or RStudio Viewer:\n# install.packages(\"knitr\")    # if you haven’t already\nlibrary(knitr)\nkable(\n  head(etf_prices_wide, 10),\n  caption = \"ETF Adjusted Closing Prices by Country (First 10 Observations)\"\n)\n\n\nETF Adjusted Closing Prices by Country (First 10 Observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\nUSA\nChina\nGermany\nIndia\nJapan\nUK\nFrance\nItaly\n\n\n\n\n2025-05-13\n586.8400\n35.7400\n40.59\n54.0100\n72.2500\n38.4200\n42.1000\n46.0000\n\n\n2025-04-30\n554.5400\n33.8600\n39.49\n53.5700\n71.4300\n38.1000\n41.0400\n44.2100\n\n\n2025-03-31\n559.3900\n35.8400\n37.08\n51.4800\n68.5600\n37.4700\n39.8000\n42.3200\n\n\n2025-02-28\n592.3990\n35.2300\n36.07\n48.1000\n68.4700\n37.0400\n39.5400\n40.8000\n\n\n2025-01-31\n600.0161\n31.8900\n34.63\n50.9900\n68.3100\n35.6100\n38.6300\n38.5500\n\n\n2024-12-31\n584.3233\n30.4400\n31.82\n52.6400\n67.1000\n33.9000\n35.8800\n35.9700\n\n\n2024-11-29\n598.7531\n29.8707\n32.27\n54.3820\n68.6140\n35.0640\n35.9809\n35.7916\n\n\n2024-10-31\n565.0568\n31.1025\n32.32\n54.4217\n67.0190\n34.8191\n37.6285\n37.4462\n\n\n2024-09-30\n570.1445\n31.3194\n33.84\n58.1048\n70.4355\n36.6409\n40.1000\n38.3720\n\n\n2024-08-30\n558.4127\n26.0765\n32.63\n57.3006\n70.8588\n36.7584\n39.5839\n38.2637"
  },
  {
    "objectID": "question 2.html#section",
    "href": "question 2.html#section",
    "title": "Analysis",
    "section": "",
    "text": "library(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(zoo)\n\n# 4) Build GDP per capita table (added Mexico & South Korea)\nseries_ids &lt;- c(\n  USA          = \"A939RC0Q052SBEA\",\n  China        = \"PCAGDPCNA646NWDB\",\n  Germany      = \"PCAGDPDEA646NWDB\",\n  India        = \"PCAGDPINA646NWDB\",\n  Japan        = \"PCAGDPJPA646NWDB\",\n  UK           = \"PCAGDPGBA646NWDB\",\n  France       = \"PCAGDPFRA646NWDB\",\n  Italy        = \"PCAGDPITA646NWDB\",\n  Mexico       = \"PCAGDPMXA646NWDB\",\n  `South Korea`= \"PCAGDPKRA646NWDB\"\n)\n\ngdp_pc_long &lt;- series_ids |&gt;\n  imap_dfr(function(id, country) {\n    get_fred(id) |&gt;\n      rename(gdp_pc = value) |&gt;\n      mutate(country = country)\n  }) |&gt;\n  filter(date &gt;= as.Date(\"1960-01-01\"))\n\n# 5) Build inflation long table (added Mexico & South Korea)\ninflation_ids &lt;- c(\n  USA           = \"FPCPITOTLZGUSA\",\n  China         = \"FPCPITOTLZGCHN\",\n  Germany       = \"FPCPITOTLZGDEU\",\n  India         = \"FPCPITOTLZGIND\",\n  Japan         = \"FPCPITOTLZGJPN\",\n  UK            = \"FPCPITOTLZGGBR\",\n  France        = \"FPCPITOTLZGFRA\",\n  Italy         = \"FPCPITOTLZGITA\",\n  Mexico        = \"FPCPITOTLZGMEX\",\n  `South Korea` = \"FPCPITOTLZGKOR\"\n)\n\ninflation_long &lt;- inflation_ids |&gt;\n  imap_dfr(function(id, country) {\n    get_fred(id) |&gt;\n      rename(inflation = value) |&gt;\n      mutate(country = country)\n  })\n\n# 6) Fetch VEA & VWO monthly returns\nvea_ret &lt;- GET_AV(\"VEA\") |&gt;\n  arrange(date) |&gt;\n  mutate(ret_dev = adjusted_close / lag(adjusted_close) - 1) |&gt;\n  select(date, ret_dev) |&gt;\n  drop_na()\n\nvwo_ret &lt;- GET_AV(\"VWO\") |&gt;\n  arrange(date) |&gt;\n  mutate(ret_emg = adjusted_close / lag(adjusted_close) - 1) |&gt;\n  select(date, ret_emg) |&gt;\n  drop_na()\n\n# 7) Compute quarterly EM & DM macro signals\ndev_countries &lt;- c(\"USA\",\"Japan\",\"UK\",\"France\",\"Germany\")\nemg_countries &lt;- c(\"China\",\"India\",\"Mexico\",\"South Korea\")\n\ngdp_growth_country &lt;- gdp_pc_long |&gt;\n  arrange(country, date) |&gt;\n  group_by(country) |&gt;\n  mutate(\n    gdp_growth = (gdp_pc / lag(gdp_pc, 4) - 1) * 100,\n    quarter    = as.yearqtr(date)\n  ) |&gt;\n  filter(!is.na(gdp_growth)) |&gt;\n  ungroup()\n\ngdp_emg_q &lt;- gdp_growth_country |&gt;\n  filter(country %in% emg_countries) |&gt;\n  group_by(quarter) |&gt;\n  summarize(gdp_emg = mean(gdp_growth, na.rm=TRUE), .groups=\"drop\")\n\ngdp_dev_q &lt;- gdp_growth_country |&gt;\n  filter(country %in% dev_countries) |&gt;\n  group_by(quarter) |&gt;\n  summarize(gdp_dev = mean(gdp_growth, na.rm=TRUE), .groups=\"drop\")\n\ninfl_long_q &lt;- inflation_long |&gt;\n  mutate(quarter = as.yearqtr(date))\n\ninfl_emg_q &lt;- infl_long_q |&gt;\n  filter(country %in% emg_countries) |&gt;\n  group_by(quarter) |&gt;\n  slice_tail(n = 1) |&gt;\n  summarize(infl_emg = mean(inflation, na.rm=TRUE), .groups=\"drop\")\n\ninfl_dev_q &lt;- infl_long_q |&gt;\n  filter(country %in% dev_countries) |&gt;\n  group_by(quarter) |&gt;\n  slice_tail(n = 1) |&gt;\n  summarize(infl_dev = mean(inflation, na.rm=TRUE), .groups=\"drop\")\n\nsignals &lt;- gdp_emg_q |&gt;\n  left_join(infl_emg_q, by=\"quarter\") |&gt;\n  left_join(gdp_dev_q,   by=\"quarter\") |&gt;\n  left_join(infl_dev_q,  by=\"quarter\")\n\n# 8) Build monthly table with signals\nsig_monthly &lt;- vea_ret |&gt;\n  left_join(vwo_ret, by=\"date\") |&gt;\n  mutate(quarter = as.yearqtr(date)) |&gt;\n  left_join(signals, by=\"quarter\") |&gt;\n  drop_na(gdp_emg, infl_emg, gdp_dev, infl_dev)\n\n# 9) Calendar vs. Tactical strategies, starting at 75/25\ninfl_thr &lt;- 4\n\nbaseline &lt;- sig_monthly |&gt;\n  mutate(\n    wt_vea    = 0.75,\n    wt_vwo    = 0.25,\n    strat_ret = wt_vea * ret_dev + wt_vwo * ret_emg\n  )\n\ntactical &lt;- sig_monthly |&gt;\n  mutate(\n    emg_down = (gdp_emg &lt; 0) | (infl_emg &gt; infl_thr),\n    dev_down = (gdp_dev &lt; 0) | (infl_dev &gt; infl_thr),\n    emg_down = replace_na(emg_down, FALSE),\n    dev_down = replace_na(dev_down, FALSE),\n\n    wt_vwo = 0.25,\n    wt_vea = 0.75,\n\n    wt_vwo = if_else(emg_down, wt_vwo * 0.5, wt_vwo),\n    wt_vea = 1 - wt_vwo,\n\n    wt_vea = if_else(dev_down, wt_vea * 0.5, wt_vea),\n    wt_vwo = 1 - wt_vea,\n\n    strat_ret = wt_vea * ret_dev + wt_vwo * ret_emg\n  )\n\n# 10) Combine, plot & metrics\ndf &lt;- bind_rows(\n  Calendar75_25 = baseline  |&gt; select(date, strat_ret),\n  Tactical      = tactical |&gt; select(date, strat_ret),\n  .id = \"strategy\"\n)\n\ndf |&gt;\n  group_by(strategy) |&gt;\n  arrange(date) |&gt;\n  mutate(cum = cumprod(1 + strat_ret) - 1) |&gt;\n  ggplot(aes(date, cum, color = strategy)) +\n    geom_line(size = 1) +\n    labs(\n      title = \"VEA/VWO: 75/25 Constant Split vs.GDP/Inflation Corrections\",\n      x     = NULL,\n      y     = \"Cumulative Return\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\nmetrics &lt;- df |&gt;\n  group_by(strategy) |&gt;\n  summarize(\n    n_months = n(),\n    cum_ret  = prod(1 + strat_ret) - 1,\n    ann_ret  = (1 + cum_ret)^(12 / n_months) - 1,\n    ann_vol  = sd(strat_ret, na.rm=TRUE) * sqrt(12),\n    sharpe   = ann_ret / ann_vol,\n    .groups  = \"drop\"\n  ) |&gt;\n  select(strategy, ann_ret, ann_vol, sharpe)\n\nprint(metrics)\n\n# A tibble: 2 × 4\n  strategy       ann_ret ann_vol  sharpe\n  &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Calendar75_25 -0.00899   0.185 -0.0485\n2 Tactical      -0.00216   0.190 -0.0114\n\n\n#QUESTION 3 ::: {.cell}\nlibrary(dplyr)\nlibrary(zoo)\nlibrary(ggplot2)\n\n# 1) Define your tickers\nequity_tickers &lt;- c(\n  Japan        = \"EWJ\",\n  UK           = \"EWU\",\n  Canada       = \"EWC\",\n  France       = \"EWQ\",\n  China        = \"FXI\",\n  India        = \"INDA\",\n  `South Korea`= \"EWY\",\n  Mexico       = \"EWW\"\n)\nresource_tickers &lt;- c(\n  Japan        = \"REMX\",\n  `United Kingdom` = \"OIH\",\n  Canada       = \"XEG.TO\",\n  France       = \"LRMR.PA\",\n  China        = \"CHIM\",\n  India        = \"IMAT\",\n  `South Korea`= \"KRMA\",\n  Mexico       = \"MXI\"\n)\n\n# 2) Fetch & compute returns for all series\nfetch_ret &lt;- function(sym, name) {\n  GET_AV(sym) %&gt;%\n    arrange(date) %&gt;%\n    mutate(!!name := adjusted_close / lag(adjusted_close) - 1) %&gt;%\n    select(date, all_of(name)) %&gt;%\n    drop_na()\n}\n\nequity_rets &lt;- imap_dfr(equity_tickers, function(sym, country) {\n  fetch_ret(sym, \"ret_eq\") %&gt;% mutate(country = country)\n})\n\nres_rets &lt;- imap_dfr(resource_tickers, function(sym, country) {\n  fetch_ret(sym, \"ret_res\") %&gt;% mutate(country = country)\n})\n\n# 3) Merge equity vs. resource returns\npaired &lt;- inner_join(equity_rets, res_rets, by = c(\"date\",\"country\"))\n\n# 4) Compute cumulative & drawdowns (separately, no coalesce)\npaired2 &lt;- paired %&gt;%\n  group_by(country) %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    cum_eq  = cumprod(1 + ret_eq)  - 1,\n    cum_res = cumprod(1 + ret_res) - 1,\n    dd_eq   = cum_eq  - cummax(cum_eq),\n    dd_res  = cum_res - cummax(cum_res)\n  ) %&gt;%\n  pivot_longer(\n    cols          = c(cum_eq, cum_res, dd_eq, dd_res),\n    names_to      = c(\"metric\",\"type\"),\n    names_pattern = \"(cum|dd)_(eq|res)\",\n    values_to     = \"value\"\n  ) %&gt;%\n  ungroup()\n# 5) Plot cumulative returns by country, only if there’s data\ncum_data &lt;- filter(paired2, metric == \"cum\")\nif (nrow(cum_data) &gt; 0) {\n  ggplot(cum_data, aes(date, value, color = type)) +\n    geom_line() +\n    facet_wrap(~country) +\n    labs(\n      title = \"Cumulative Returns: Equity vs. Resource\",\n      y     = \"Cumulative Return\",\n      x     = NULL,\n      color = \"Series\"\n    ) +\n    theme_minimal()\n} else {\n  message(\"No cumulative-return data available to plot.\")\n}\n\n\n\n\n\n\n\n# 6) Plot drawdowns by country, only if there’s data\ndd_data &lt;- filter(paired2, metric == \"dd\")\nif (nrow(dd_data) &gt; 0) {\n  ggplot(dd_data, aes(date, value, color = type)) +\n    geom_line() +\n    facet_wrap(~country) +\n    labs(\n      title = \"Drawdowns: Equity vs. Resource\",\n      y     = \"Drawdown\",\n      x     = NULL,\n      color = \"Series\"\n    ) +\n    theme_minimal()\n} else {\n  message(\"No drawdown data available to plot.\")\n}\n\n\n\n\n\n\n\n# 7) Annualized metrics\nmetrics &lt;- paired %&gt;%\n  group_by(country) %&gt;%\n  summarize(\n    n            = n(),\n    cum_ret_eq   = prod(1 + ret_eq) - 1,\n    ann_ret_eq   = (1 + cum_ret_eq)^(12 / n) - 1,\n    ann_vol_eq   = sd(ret_eq, na.rm = TRUE) * sqrt(12),\n    cum_ret_res  = prod(1 + ret_res) - 1,\n    ann_ret_res  = (1 + cum_ret_res)^(12 / n) - 1,\n    ann_vol_res  = sd(ret_res, na.rm = TRUE) * sqrt(12),\n    .groups      = \"drop\"\n  )\n\nprint(metrics)\n\n# A tibble: 4 × 8\n  country         n cum_ret_eq ann_ret_eq ann_vol_eq cum_ret_res ann_ret_res\n  &lt;chr&gt;       &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 Canada        241      2.69      0.0672      0.199       0.903      0.0326\n2 China         167     -0.225    -0.0181      0.237      -0.292     -0.0245\n3 Japan         174      1.24      0.0572      0.141      -0.743     -0.0893\n4 South Korea   105      0.219     0.0229      0.230       1.78       0.124 \n# ℹ 1 more variable: ann_vol_res &lt;dbl&gt;\n\n:::\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(zoo)\nlibrary(ggplot2)\n\n# 1) Define resource tickers by country\nresource_tickers &lt;- c(\n  Japan         = \"REMX\",\n  `United Kingdom` = \"OIH\",\n  Canada        = \"XEG.TO\",\n  France        = \"LRMR.PA\",\n  China         = \"CHIM\",\n  India         = \"IMAT\",\n  `South Korea`= \"KRMA\",\n  Mexico        = \"MXI\"\n)\n\n# 2) Map each country to Developed vs. Emerging\ngroup_map &lt;- c(\n  Japan           = \"Developed\",\n  `United Kingdom`= \"Developed\",\n  Canada          = \"Developed\",\n  France          = \"Developed\",\n  China           = \"Emerging\",\n  India           = \"Emerging\",\n  `South Korea`   = \"Emerging\",\n  Mexico          = \"Emerging\"\n)\n\n# 3) Fetch resource returns\nfetch_ret &lt;- function(sym, col) {\n  GET_AV(sym) %&gt;%\n    arrange(date) %&gt;%\n    mutate(!!col := adjusted_close / lag(adjusted_close) - 1) %&gt;%\n    select(date, all_of(col)) %&gt;%\n    drop_na()\n}\n\nres_rets &lt;- imap_dfr(resource_tickers, function(sym, country) {\n  fetch_ret(sym, \"ret_res\") %&gt;%\n    mutate(country = country,\n           bucket  = group_map[country])\n})\n\n# 4) Fetch VEA (developed) & VWO (emerging) returns\nvea_ret &lt;- fetch_ret(\"VEA\", \"ret_bucket\") %&gt;% mutate(bucket = \"Developed\")\nvwo_ret &lt;- fetch_ret(\"VWO\", \"ret_bucket\") %&gt;% mutate(bucket = \"Emerging\")\n\nbucket_rets &lt;- bind_rows(vea_ret, vwo_ret)\n\n# 5) Join each resource series to its bucket ETF returns by date+bucket\npaired &lt;- res_rets %&gt;%\n  left_join(bucket_rets, by = c(\"date\",\"bucket\"))\n\n# 6) Compute cum returns & drawdowns\npaired2 &lt;- paired %&gt;%\n  group_by(country) %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    cum_res    = cumprod(1 + ret_res) - 1,\n    cum_bucket = cumprod(1 + ret_bucket) - 1,\n    dd_res     = cum_res    - cummax(cum_res),\n    dd_bucket  = cum_bucket - cummax(cum_bucket)\n  ) %&gt;%\n  pivot_longer(\n    cols          = c(cum_res, cum_bucket, dd_res, dd_bucket),\n    names_to      = c(\"metric\",\"series\"),\n    names_pattern = \"(cum|dd)_(res|bucket)\",\n    values_to     = \"value\"\n  ) %&gt;%\n  ungroup()\n\n# … after you compute paired2 …\n\n# restrict to non‐missing values\npaired2_clean &lt;- paired2 %&gt;%\n  filter(!is.na(value))\n# Prepare the two subsets\ncum_data &lt;- filter(paired2_clean, metric == \"cum\")\ndd_data  &lt;- filter(paired2_clean, metric == \"dd\")\n\n# 1) Cumulative returns plot\nif (nrow(cum_data) &gt; 0) {\n  ggplot(cum_data, aes(x = date, y = value, color = series)) +\n    geom_line(na.rm = TRUE) +\n    facet_wrap(~country) +\n    labs(\n      title = \"Cumulative Returns: Resource vs. VEA/VWO by Country\",\n      y     = \"Cumulative Return\",\n      x     = NULL,\n      color = \"Series\"\n    ) +\n    theme_minimal()\n} else {\n  message(\"No cumulative‐return data available to plot.\")\n}\n\nNo cumulative‐return data available to plot.\n\n# 2) Drawdowns plot\nif (nrow(dd_data) &gt; 0) {\n  ggplot(dd_data, aes(x = date, y = value, color = series)) +\n    geom_line(na.rm = TRUE) +\n    facet_wrap(~country) +\n    labs(\n      title = \"Drawdowns: Resource vs. VEA/VWO by Country\",\n      y     = \"Drawdown\",\n      x     = NULL,\n      color = \"Series\"\n    ) +\n    theme_minimal()\n} else {\n  message(\"No drawdown data available to plot.\")\n}\n\nNo drawdown data available to plot."
  },
  {
    "objectID": "Individual Final Report.html#setup-and-data-access",
    "href": "Individual Final Report.html#setup-and-data-access",
    "title": "Final Individual Report",
    "section": "Setup and Data Access:",
    "text": "Setup and Data Access:\nThis section initializes the environment and defines functions to retrieve monthly time-series data from FRED and Alpha Vantage. These data sources provide the macroeconomic and asset-level information necessary to conduct portfolio return analysis and simulation.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(kableExtra)\nlibrary(gganimate)\nlibrary(ggthemes)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(PortfolioAnalytics)\nlibrary(quantmod)\nlibrary(PerformanceAnalytics)\n\n#setwd(\"C:/Users/orazz/OneDrive - The City University of New York (1)/MASTER/STA9750-2025-SPRING\")\n\n\n\nData wrangling: dplyr, tidyr, purrr\nUsed to clean, manipulate, and reshape datasets efficiently.\nTime handling: lubridate\nSimplifies working with dates in time series data.\nVisualization: ggplot2, gganimate, ggthemes, kableExtra.\nEnables static and animated plotting, with themes to improve aesthetics.\nAPI access: httr, jsonlite\nUsed to send HTTP requests and parse JSON responses from the FRED and Alpha Vantage APIs."
  },
  {
    "objectID": "Individual Final Report.html#financial-metrics",
    "href": "Individual Final Report.html#financial-metrics",
    "title": "Final Individual Report",
    "section": "📈 Annualized Returns and Risk Metrics",
    "text": "📈 Annualized Returns and Risk Metrics\nTo evaluate long-term performance across different asset classes—including U.S. stocks, international developed markets, and emerging markets—I calculated a set of standardized financial metrics using monthly log return data. The goal was to estimate both total and risk-adjusted returns over the observed period.\n\n\nShow the code\nyearly_returns &lt;- combined_data |&gt;\n  drop_na(log_return) |&gt;\n  group_by(ticker) |&gt;\n  arrange(date) |&gt;\n  summarise(\n    initial_price = first(adjusted_close),\n    final_price   = last(adjusted_close),\n    years         = as.numeric(difftime(last(date), first(date), units = \"days\")) / 365.25,\n    total_return_pct = (final_price / initial_price - 1) * 100,\n    CAGR_pct      = ((final_price / initial_price)^(1 / years) - 1)  ,\n    volatility    = sd(log_return, na.rm = TRUE) * sqrt(12) ,\n    sharpe_ratio  = (CAGR_pct  - 0.02) / (volatility )\n  )\n\nyearly_returns$CAGR_pct &lt;- yearly_returns$CAGR_pct * 100\nyearly_returns$volatility &lt;- yearly_returns$volatility *100\n\n\n\n🔍 Explanation of Code\n\ndrop_na(log_return): Removes missing values to ensure calculations are valid and consistent across all assets.\ngroup_by(ticker): Performs all subsequent calculations separately for each asset (ETF) in the dataset.\ninitial_price, final_price: Capture the first and last adjusted prices for each asset, forming the basis for return calculations.\nyears: Calculates the total time span (in years) of the available data for each asset.\ntotal_return_pct: Computes the total percentage return over the entire period, from the initial to the final price.\nCAGR_pct (Compound Annual Growth Rate): Reflects the mean annual growth rate of an investment over the time period, accounting for compounding.\nvolatility: Measures the annualized standard deviation of the monthly log returns, serving as a proxy for investment risk.\nsharpe_ratio: Calculates the Sharpe Ratio, which adjusts the return by its risk. It uses a 2% annual risk-free rate to assess how efficiently an asset compensates investors for the risk taken.\n\nThese metrics collectively help compare the long-term performance and risk profile of U.S. markets, developed"
  },
  {
    "objectID": "Final Report.html#introduction-rethinking-u.s.-only-portfolios-in-a-global-economy",
    "href": "Final Report.html#introduction-rethinking-u.s.-only-portfolios-in-a-global-economy",
    "title": "Final Report",
    "section": "",
    "text": "In a globalized investment landscape, many individuals still rely heavily on U.S.-based equity funds to build their portfolios. But does this domestic bias limit long-term performance? This project investigates an important question:\nHow can integrating a diverse mix of international traded funds, in developed or emerging markets boost returns compared to relying solely on U.S. funds, all while meeting individual financial objectives and investment time frames?\nInvestors face different needs based on their age, income, and financial objectives. A one-size-fits-all U.S.-only portfolio might not offer the best strategy for maximizing growth or managing risk. With access to global markets now easier than ever, we explore whether including international funds offers meaningful advantages—and under what conditions."
  },
  {
    "objectID": "Final Report.html#methodology-and-analytical-focus",
    "href": "Final Report.html#methodology-and-analytical-focus",
    "title": "Final Report",
    "section": "Methodology and Analytical Focus",
    "text": "Methodology and Analytical Focus\nWe examined the role of developed and emerging market funds, commodity-linked ETFs, and rebalancing strategies based on macroeconomic indicators such as GDP per capita and inflation. Our individual questions included:\n\nCan reallocating between developed and emerging markets protect against losses during downturns?\nDo equities from resource-rich countries improve portfolio stability or performance?\nAre developed markets better than emerging ones for long-term growth, especially compared to U.S. funds?\n\nWe used historical data from 2011–2025 and constructed hypothetical portfolios using popular ETFs, including SPY, VEA, VWO, and others. Simulated performance and volatility metrics were compared under different strategies, including rebalancing based on economic signals."
  },
  {
    "objectID": "Final Report.html#data-sources-and-limitations",
    "href": "Final Report.html#data-sources-and-limitations",
    "title": "Final Report",
    "section": "Data Sources and Limitations",
    "text": "Data Sources and Limitations\nAlphaVantage: U.S. and international equity ETFs, bond ETFs, and commodity ETFs.\nFRED: Macroeconomic data including GDP per capita and inflation rates for selected countries.\nLimitations:\n\nETF data only spans ~15 years, limiting long-term insights.\nNo fees, taxes, or bid-ask spreads were factored in.\nSome countries were grouped simply by GDP/inflation metrics, without deeper macro modeling.\nResource-sector ETFs showed high volatility and varied availability across countries."
  },
  {
    "objectID": "Final Report.html#etf-breakdown-and-asset-overview",
    "href": "Final Report.html#etf-breakdown-and-asset-overview",
    "title": "Final Report",
    "section": "ETF Breakdown and Asset Overview",
    "text": "ETF Breakdown and Asset Overview\nTo evaluate global diversification strategies, we categorized our ETF selections into three major asset classes: stocks (equities), bonds (fixed income), and commodities/alternatives. Each fund offers distinct exposure, risk, and potential return profiles.\n\n\nStocks (Equity ETFs)\nThese ETFs provide exposure to U.S. and international stock markets, spanning companies of various sizes and geographies.\n\nSPY – S&P 500 ETF (U.S.)\nTracks the 500 largest U.S. companies; a benchmark for overall U.S. equity market performance.\nMDY – S&P MidCap 400 ETF (U.S.)\nRepresents medium-sized U.S. companies, offering growth-oriented exposure.\nIWM – Russell 2000 ETF (U.S.)\nCovers small-cap U.S. stocks, typically with higher risk and potential return.\nVEA – Vanguard FTSE Developed Markets ETF\nOffers access to stocks in developed markets outside the U.S. and Canada (e.g., Europe, Japan, Australia).\nVWO – Vanguard FTSE Emerging Markets ETF\nFocused on emerging economies like China, India, and Brazil, which may provide higher growth and higher volatility.\nVXUS - Vanguard Total International Stock ETF Broad exposure to all international markets outside the U.S., including both developed and emerging markets.\n\n\n\n\nBonds (Fixed Income ETFs)\nBond ETFs contribute portfolio stability and income, and help reduce overall risk.\n\nBND – Vanguard Total Bond Market ETF\nProvides broad exposure to U.S. investment-grade bonds including Treasuries, corporate bonds, and mortgage-backed securities.\nSHY – 1–3 Year Treasury Bond ETF\nInvests in short-term U.S. Treasury bonds. Less sensitive to interest rates and lower risk.\nTLT – 20+ Year Treasury Bond ETF\nTracks long-term U.S. Treasury bonds. Higher duration and more volatility, often used for economic downturn protection.\n\n\n\n\nCommodities & Alternatives\nThese assets add diversification and serve as hedges against inflation and market volatility.\n\nGLD – SPDR Gold Shares ETF\nReflects the price of gold; commonly used as a safe haven during market stress or inflationary periods.\nDBA – Invesco Agriculture ETF\nTracks a basket of agricultural commodities like wheat, corn, and soybeans, providing real asset exposure.\nVNQ – Vanguard Real Estate ETF (REITs)\nInvests in real estate investment trusts, offering access to income-producing properties in the U.S. real estate market.\n\nEconomic Data:\n\n\nInflation: (%, not seasonally adjusted)\nGDP per capita: (in U.S. Dollar, not seasonally adjusted, annual)"
  },
  {
    "objectID": "Final Report.html#financial-metric-formulas",
    "href": "Final Report.html#financial-metric-formulas",
    "title": "Final Report",
    "section": "📐 Financial Metric Formulas",
    "text": "📐 Financial Metric Formulas\nThe financial formulas used in the project to calculate Risk and Returns or the assets are shown below.\n\n1. Total Return (%)\n\\[\n\\text{Total Return (\\%)} = \\left( \\frac{P_{\\text{final}} - P_{\\text{initial}}}{P_{\\text{initial}}} \\right) \\times 100\n\\]\nWhere:\n\n\\(P_{final}\\) : Final price of the asset\n\n\\(P_{initial}\\) : Initial price of the asset\n\n\n\n2. Compound Annual Growth Rate (CAGR %)\n\\[\n\\text{CAGR \\%} = \\left( \\left( \\frac{P_{\\text{final}}}{P_{\\text{initial}}} \\right)^{\\frac{1}{n}} - 1 \\right) \\times 100\n\\]\nWhere:\n\n\\(n\\) : Number of years the asset is held\n\n\n\n\n3. Annualized Volatility\n\\[\n\\text{Volatility} = \\sigma_{\\text{monthly returns}} \\times \\sqrt{12}\n\\]\nWhere:\n\n\\(\\sigma_{\\text{monthly returns}}\\) : Standard deviation of monthly log returns\n\n\n\n\n4. Sharpe Ratio\n\\[\n\\text{Sharpe Ratio} = \\frac{\\text{CAGR} - r_f}{\\text{Volatility}}\n\\]\nWhere:\n- \\({r_f}\\) : Risk-free rate (assumed to be 2% in this project)\n- Volatility: Annualized standard deviation of returns"
  },
  {
    "objectID": "Final Report.html#connection-to-prior-work",
    "href": "Final Report.html#connection-to-prior-work",
    "title": "Final Report",
    "section": "Connection to Prior Work",
    "text": "Connection to Prior Work\nOur findings echo recent literature:\n\nAttig et al. (2023): International diversification reduces political and inflation risk—consistent with our findings that global exposure (particularly in developed markets) stabilizes long-term performance.\nMoisand & Salmon: Rebalancing improves outcomes, especially during bear markets. Our GDP/inflation-triggered rebalancing shows limited but positive impact.\nRuano & Barros (2022): Commodities offer a diversification benefit. We found gold and agriculture ETFs lowered volatility, though performance varied by period.\nOuoba (2016): Resource-rich countries can perform well, but political and fiscal instability pose serious risks—our analysis showed mixed results for resource-sector ETFs."
  },
  {
    "objectID": "Final Report.html#conclusion-and-future-directions",
    "href": "Final Report.html#conclusion-and-future-directions",
    "title": "Final Report",
    "section": "Conclusion and Future Directions",
    "text": "Conclusion and Future Directions\n\nIs International Diversification Beneficial?\nYes—but with caveats. Developed international equities (VEA) provided more reliable returns than emerging markets (VWO), especially when rebalanced based on macro trends. However, neither matched U.S. equities in cumulative growth.\nKey takeaways: - A global mix can help with risk-adjusted returns, but U.S. equities still dominate long-term growth. - Limiting international equity exposure to ~20% appears optimal based on our results. - Bonds and commodity ETFs added the most stability to portfolios. - Rebalancing based on GDP/inflation shows potential but may require longer time horizons to fully realize benefits. - Resource-focused ETFs are too volatile to recommend broadly."
  },
  {
    "objectID": "Final Report.html#future-considerations",
    "href": "Final Report.html#future-considerations",
    "title": "Final Report",
    "section": "Future Considerations",
    "text": "Future Considerations\n\nExpand the analysis to include more countries and weight by GDP size.\nExplore currency and crypto assets for broader diversification.\nInclude tax implications and expense ratios for a more realistic performance picture.\nAssess performance during multiple recession periods."
  },
  {
    "objectID": "Final Report.html#individual-report-links",
    "href": "Final Report.html#individual-report-links",
    "title": "Final Report",
    "section": "Individual Report Links",
    "text": "Individual Report Links\n\nGiulio Orazzo’s Report – Developed vs. Emerging Markets\nSalvatore Davi’s Report – Rebalancing Equities & Resource-Rich Markets\n\n\n*Prepared for STA 9750 at Baruch College – May 2025. The Money Team: Giulio Orazzo - Salvatore Davi"
  },
  {
    "objectID": "Final Report.html#question-1---developed-vs-emerging-international-markets",
    "href": "Final Report.html#question-1---developed-vs-emerging-international-markets",
    "title": "Global Diversification and Individual Investment Goals",
    "section": "QUESTION 1 - Developed vs Emerging International Markets",
    "text": "QUESTION 1 - Developed vs Emerging International Markets\nTo explore whether developed or emerging international markets offer better long-term performance, we analyzed three popular international equity ETFs:\n\nVEA: Vanguard FTSE Developed Markets ETF\nVWO: Vanguard FTSE Emerging Markets ETF\nVXUS: Vanguard Total International Stock ETF (includes both developed and emerging markets)\n\nWe compared their performance from the start of the dataset (approx. 2011) through early 2025 using key financial metrics: total return, compound annual growth rate (CAGR), volatility, and Sharpe ratio (a measure of risk-adjusted return).\n\n\nKey Findings\nVEA (developed markets) was the top performer across most metrics. It delivered the highest total return (124.25%) and the highest CAGR (5.82%) with a relatively moderate level of volatility. Its Sharpe ratio (0.25) indicates a reasonable tradeoff between risk and return. - VXUS, which includes both developed and emerging markets, had lower returns (95.13%) and a slightly lower Sharpe ratio (0.19). This suggests that including emerging markets may have diluted the overall performance. - VWO (emerging markets) had the lowest return (48.09%) and the lowest risk-adjusted performance, with a Sharpe ratio of just 0.05. Despite a higher level of volatility (17.27%), it did not offer better long-term growth.\n\n\nInterpretation\nWhile emerging markets are often associated with high growth potential, our analysis shows that over the long run (2011–2025), developed international markets significantly outperformed emerging markets in both total and risk-adjusted returns. Investors looking for global diversification may find more consistent results by prioritizing developed market exposure (e.g., via VEA), rather than heavily relying on emerging markets like those captured by VWO. This suggests that strategic international diversification should favor developed markets, especially for investors seeking stability and consistent performance over long horizons.\n\n\nU.S. vs. International Equity Performance (2011–2025)\n\n\n\nU.S. vs International Equity Performance\n\n\n\nInsight: U.S. markets (SPY) outperformed international developed (VEA) and emerging (VWO) markets over this period. However, diversification offered better downside protection during turbulent periods.\n\n\n\n\nRebalancing Based on Economic Indicators\ninsert image here\n\nInsight: A constant 75% VEA / 25% VWO allocation slightly underperformed rebalancing strategies based on GDP growth and inflation thresholds. While gains were modest (~0.004% p.a.), rebalancing avoided worse downturns during periods of emerging market underperformance.\n\n\n\n\nResource-Driven Markets vs. Broad Country ETFs\ninsert image here\n\nInsight:\nInvesting in resource-driven ETFS does not provide drastic improvement to portfolio performance as compared to investing in broad-market ETFs.\nVolatility and Metrics:\nWe measured cumulative return, annualized return, and annualized volatility for each ETF. Notably, resource-based ETFs tend to exhibit higher volatility, especially in Japan, Canada, and China—reflecting the cyclical and commodity-driven nature of these sectors.\nKey Findings:\n\nSouth Korea’s resource ETF (KRMA) stands out, with an annualized return of 12.4%, compared to just 2.4% for the country’s broad market ETF.\nIn contrast, resource-focused ETFs in China and Japan produced flatter returns, trailing their broad market counterparts."
  },
  {
    "objectID": "Final Report.html#question-2--rebalancing-equities",
    "href": "Final Report.html#question-2--rebalancing-equities",
    "title": "Final Report",
    "section": "QUESTION 2- Rebalancing Equities",
    "text": "QUESTION 2- Rebalancing Equities\nIn this analysis, we aimed to compare the performance of a traditional 75/25 equity split between developed and emerging markets against a dynamic, signal-based rebalancing strategy.\nWe used financial data from Alpha Vantage and FRED. The ETFs VEA and VWO served as proxies for developed and emerging market equities, respectively. To construct economic signals, we gathered GDP per capita and annual inflation data for ten countries—six classified as developed and four as emerging, based on MSCI definitions. Although these countries are among the world’s most influential, we limited the sample due to API restrictions and the scope of our project.\nOur baseline strategy followed a simple approach: maintain a fixed allocation of 75% to developed markets and 25% to emerging markets, reflecting common long-term investment recommendations that suggest emerging markets should make up about 20–25% of global equity portfolios.\nIn contrast, the signal-based rebalancing strategy adjusted allocations based on economic indicators. Specifically, if the average GDP growth in a group was negative or if inflation exceeded 4%, we interpreted this as a signal of economic stress. In response, we cut emerging market exposure in half and reallocated the difference to developed markets. Importantly, the portfolio always remained fully invested in equities—we did not shift any assets to cash during rebalancing.\n\n\nInsight: A constant 75% VEA / 25% VWO allocation slightly underperformed rebalancing strategies based on GDP growth and inflation thresholds. While gains were modest (~0.004% p.a.), rebalancing avoided worse downturns during periods of emerging market underperformance."
  },
  {
    "objectID": "Final Report.html#question-3---investing-in-resource-rich-markets",
    "href": "Final Report.html#question-3---investing-in-resource-rich-markets",
    "title": "Final Report",
    "section": "QUESTION 3 - Investing In Resource-Rich Markets",
    "text": "QUESTION 3 - Investing In Resource-Rich Markets\nIn this section, we investigate whether investing in resource-based ETFs delivers superior returns compared to broad market ETFs for selected resource-rich nations.\nWe focused on four countries—Canada, China, Japan, and South Korea—where we identified both: - A broad market ETF (tracking the overall national stock market), and - A resource-sector ETF (focused on industries such as mining, energy, or materials relevant to each economy).\nFor instance, Canada’s XEG.TO ETF concentrates on energy production, including oil and natural gas, which closely aligns with the country’s resource-driven export economy.\n\n\nInsight:\nInvesting in resource-driven ETFS does not provide drastic improvement to portfolio performance as compared to investing in broad-market ETFs.\nVolatility and Metrics:\nWe measured cumulative return, annualized return, and annualized volatility for each ETF. Notably, resource-based ETFs tend to exhibit higher volatility, especially in Japan, Canada, and China—reflecting the cyclical and commodity-driven nature of these sectors.\nKey Findings:\n\nSouth Korea’s resource ETF (KRMA) stands out, with an annualized return of 12.4%, compared to just 2.4% for the country’s broad market ETF.\nIn contrast, resource-focused ETFs in China and Japan produced flatter returns, trailing their broad market counterparts."
  },
  {
    "objectID": "Final Report.html",
    "href": "Final Report.html",
    "title": "Final Report",
    "section": "",
    "text": "In a globalized investment landscape, many individuals still rely heavily on U.S.-based equity funds to build their portfolios. But does this domestic bias limit long-term performance? This project investigates an important question:\nHow can integrating a diverse mix of international traded funds, in developed or emerging markets boost returns compared to relying solely on U.S. funds, all while meeting individual financial objectives and investment time frames?\nInvestors face different needs based on their age, income, and financial objectives. A one-size-fits-all U.S.-only portfolio might not offer the best strategy for maximizing growth or managing risk. With access to global markets now easier than ever, we explore whether including international funds offers meaningful advantages—and under what conditions."
  },
  {
    "objectID": "Final Report.html#question-1-developed-vs-emerging-international-markets",
    "href": "Final Report.html#question-1-developed-vs-emerging-international-markets",
    "title": "Final Report",
    "section": "QUESTION 1 – Developed vs Emerging International Markets",
    "text": "QUESTION 1 – Developed vs Emerging International Markets\nTo explore whether developed or emerging international markets offer better long-term performance, we analyzed three popular international equity ETFs:\n\nVEA: Vanguard FTSE Developed Markets ETF\n\nVWO: Vanguard FTSE Emerging Markets ETF\n\nVXUS: Vanguard Total International Stock ETF (includes both developed and emerging markets)\n\nWe compared their performance from the start of the dataset (approx. 2011) through early 2025 using key financial metrics: total return, compound annual growth rate (CAGR), volatility, and Sharpe ratio (a measure of risk-adjusted return).\n\n\n\nInternational ETF Table\n\n\n\nKey Findings\n\nVEA (developed markets) was the top performer across most metrics. It delivered the highest total return (124.25%) and the highest CAGR (5.82%) with a relatively moderate level of volatility. Its Sharpe ratio (0.25) indicates a reasonable tradeoff between risk and return.\nVXUS, which includes both developed and emerging markets, had lower returns (95.13%) and a slightly lower Sharpe ratio (0.19). This suggests that including emerging markets may have diluted the overall performance.\nVWO (emerging markets) had the lowest return (48.09%) and the lowest risk-adjusted performance, with a Sharpe ratio of just 0.05. Despite a higher level of volatility (17.27%), it did not offer better long-term growth.\n\n\n\nInterpretation\nWhile emerging markets are often associated with high growth potential, our analysis shows that over the long run (2011–2025), developed international markets significantly outperformed emerging markets in both total and risk-adjusted returns. Investors looking for global diversification may find more consistent results by prioritizing developed market exposure (e.g., via VEA), rather than heavily relying on emerging markets like those captured by VWO. This suggests that strategic international diversification should favor developed markets, especially for investors seeking stability and consistent performance over long horizons.\n\n\nU.S. vs. International Equity Performance (2011–2025)\n\n\n\nU.S. vs International Equity Performance\n\n\n\nInsight: U.S. markets (SPY) outperformed international developed (VEA) and emerging (VWO) markets over this period. However, diversification offered better downside protection during turbulent periods."
  },
  {
    "objectID": "Final Report.html#question-2---rebalancing-equities",
    "href": "Final Report.html#question-2---rebalancing-equities",
    "title": "Final Report",
    "section": "QUESTION 2 - Rebalancing Equities",
    "text": "QUESTION 2 - Rebalancing Equities\nIn this analysis, we aimed to compare the performance of a traditional 75/25 equity split between developed and emerging markets against a dynamic, signal-based rebalancing strategy.\nWe used financial data from Alpha Vantage and FRED. The ETFs VEA and VWO served as proxies for developed and emerging market equities, respectively. To construct economic signals, we gathered GDP per capita and annual inflation data for ten countries—six classified as developed and four as emerging, based on MSCI definitions. Although these countries are among the world’s most influential, we limited the sample due to API restrictions and the scope of our project.\nOur baseline strategy followed a simple approach: maintain a fixed allocation of 75% to developed markets and 25% to emerging markets, reflecting common long-term investment recommendations that suggest emerging markets should make up about 20–25% of global equity portfolios.\nIn contrast, the signal-based rebalancing strategy adjusted allocations based on economic indicators. Specifically, if the average GDP growth in a group was negative or if inflation exceeded 4%, we interpreted this as a signal of economic stress. In response, we cut emerging market exposure in half and reallocated the difference to developed markets. Importantly, the portfolio always remained fully invested in equities—we did not shift any assets to cash during rebalancing.\n\n\nInsight: A constant 75% VEA / 25% VWO allocation slightly underperformed rebalancing strategies based on GDP growth and inflation thresholds. While gains were modest (~0.004% p.a.), rebalancing avoided worse downturns during periods of emerging market underperformance."
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "STA 9713 Project",
    "section": "",
    "text": "Preparation\nAnswering the Questions"
  },
  {
    "objectID": "Project.html#table-of-contents",
    "href": "Project.html#table-of-contents",
    "title": "STA 9713 Project",
    "section": "",
    "text": "Preparation\nAnswering the Questions"
  },
  {
    "objectID": "Project.html#question-1",
    "href": "Project.html#question-1",
    "title": "STA 9713 Project",
    "section": "Question 1",
    "text": "Question 1\nThe graph below shows a comparison between the peromance and volatility of the 4 Funds analyzed, along with the S&P 500.\n\n\n\nComparison of Returns & Volatilities\n\n\n\n\n\n\n\n\n\nFund\nMonthly Return %\nMonthly Volatility %\nAnnual Return %\nAnnual Volatility %\n\n\n\n\nS&P 500\n1.27\n4.47\n16.40\n15.49\n\n\nEndive\n1.14\n4.71\n14.61\n16.33\n\n\nWell\n1.34\n5.26\n17.30\n18.23\n\n\nWin\n1.34\n7.75\n17.25\n26.83\n\n\nPinede\n1.31\n4.74\n16.88\n16.42\n\n\n\n\n\nThe graph below show a comparison of the the monthly returns and monthly volatilities.\n\n\nShow the code\nsummary_long &lt;- pivot_longer(combined_data, \n                             cols = c(\"Monthly_Return\", \"Monthly_Volatility\"),\n                             names_to = \"Metric\",\n                             values_to = \"Value\")\nggplot(summary_long, aes(x = Fund , y = Value, fill = Metric))+\n  geom_bar(stat = \"identity\", position = \"dodge\")+\n  labs( title = \"Monthly Returns vs Monthly Volatility\")+\n  theme_bw()+\n  theme(text = element_text(family = \"Helvetica\", size = 15, face = \"bold\"))"
  },
  {
    "objectID": "Project.html#question-2",
    "href": "Project.html#question-2",
    "title": "STA 9713 Project",
    "section": "Question 2",
    "text": "Question 2\nThe code below calculates the risk-return (Sharpe, Sortino)ratio for the Endive Fund\n\n\nShow the code\n# calculating the sharpe ratio with montlhy riskfree rate of 0.2%\nrf_monthly &lt;- 0.2\nendive_sharpe &lt;- (endive_mean - rf_monthly) / endive_vol\n\n#sortino ratio \nendive_downside_ret &lt;- data$Endive[data$Endive &lt; 0]\nendive_downside_dev &lt;- sd(endive_downside_ret)\nendive_sortino &lt;- (endive_mean - rf_monthly) / endive_downside_dev\n\n\nThe Sharpe Ratio for Endive is 0.2. The Sortino Ratio is 0.3"
  },
  {
    "objectID": "Project.html#question-3",
    "href": "Project.html#question-3",
    "title": "STA 9713 Project",
    "section": "Question 3",
    "text": "Question 3\nThe code below calculates the 95% Confidence Interval for the true return of Endive\n\n\nShow the code\nn_endive &lt;- length(data$Endive)\n# standard error\n\nendive_SEM &lt;- (endive_vol) / sqrt(n_endive) \nalpha &lt;- 0.05\nendive_tvalue &lt;- qt(1 - alpha /2, df = n_endive -1)\nendive_lower_bound &lt;- (endive_mean) - endive_tvalue *endive_SEM\nendive_upper_bound &lt;- (endive_mean) + endive_tvalue *endive_SEM\n\n\nThe mean monthly return for Endive is 1.14% with standard deviation of 4.71%. The 95% Confidence Interval for the true mean return: 0.25% to 2.03%"
  },
  {
    "objectID": "Project.html#question-4",
    "href": "Project.html#question-4",
    "title": "STA 9713 Project",
    "section": "Question 4",
    "text": "Question 4\nAssuming the returns of Well follow a normal distribution, the code below calculates the probability that the average return of 4 randomly selected months is greater than 2%.\n\n\nShow the code\n# standard error for n = 4\nwell_SEM &lt;- well_vol/sqrt(4)\n\n#standardizing the value with z-score 2%\n\nwell_zscore &lt;- (2- well_mean)/ well_SEM\nwell_prob2 &lt;- (1 - pnorm(well_zscore)) * 100\n\n\nThe probability that the average return of 4 randomly selected months is greater than 2% is 40.08% .\nThe code below calculates the z-score of the 20% percentile.\n\n\nShow the code\n#zscore of 20percentile\n\nz_20 &lt;- qnorm(.20)\nwell_20_perc &lt;- well_mean + z_20 * well_vol\n\n\nThe theoretical 20% percentile for Well is -3.09%. According to the fitted normal distribution, 20% of the time the monthly return is worse than -3.09%."
  },
  {
    "objectID": "Project.html#question-5",
    "href": "Project.html#question-5",
    "title": "STA 9713 Project",
    "section": "Question 5",
    "text": "Question 5\nAssuming a 3% risk-free rate and the S&P 500 as the “market”, the code below calculates the expected returns using the CAPM equation for the 4 Funds\n\n\nShow the code\n# calculating each fund linear regression\n\nwell_model &lt;- lm(data$Well ~ data$`S&P 500 Index Fund`)\nwell_beta &lt;- as.numeric(coef(well_model)[2])\n\nendive_model &lt;- lm(data$Endive ~ data$`S&P 500 Index Fund`)\nendive_beta &lt;- as.numeric(coef(endive_model)[2])\n\nwin_model&lt;- lm(data$Win ~ data$`S&P 500 Index Fund`)\nwin_beta &lt;- as.numeric(coef(win_model)[2])\n\npinede_model&lt;- lm(data$Pinede ~ data$`S&P 500 Index Fund`)\npinede_beta &lt;- as.numeric(coef(pinede_model)[2])\n\n\n# calculating Expeced Returns using CAPM assuming A 3% rf\n\nrf &lt;- 3\nendive_exp_ret &lt;- rf + endive_beta * (annualized_sp_ret - rf)\nwell_exp_ret &lt;- rf + well_beta * (annualized_sp_ret - rf)\nwin_exp_ret &lt;- rf + win_beta * (annualized_sp_ret - rf)\npinede_exp_ret &lt;- rf + pinede_beta * (annualized_sp_ret - rf)\n \n# binding the data\nfund_names &lt;- c(\"Endive\", \"Well\", \"Win\", \"Pinede\")\nbeta &lt;- c( endive_beta, well_beta, win_beta, pinede_beta)\nexpected_ret &lt;- c(endive_exp_ret, well_exp_ret, win_exp_ret,pinede_exp_ret)\nCAPM_data &lt;- data.frame(\n  Fund = fund_names,\n  Beta = beta,\n  Expected_Return = expected_ret)\n\n\n\n\n\nComparison of Annual Expected Returns(via CAPM)\n\n\nFund\nBeta\nAnnual Expected Return %\n\n\n\n\nEndive\n0.98\n16.19\n\n\nWell\n1.11\n17.93\n\n\nWin\n1.32\n20.66\n\n\nPinede\n0.91\n15.22\n\n\n\n\n\nThe beta of the Funds represent their correlation to the market(S&P500). Using the CAPM equation, we can observe the Annual Expected Return percentage for the 4 Funds."
  },
  {
    "objectID": "Project.html#question-6",
    "href": "Project.html#question-6",
    "title": "STA 9713 Project",
    "section": "Question 6",
    "text": "Question 6\nThe code below calculates the alphas(intercepts) of the 4 Funds\n\n\nShow the code\nendive_alpha &lt;- as.numeric(coef(endive_model)[1])\nwell_alpha &lt;- as.numeric(coef(well_model)[1])\nwin_alpha &lt;- as.numeric(coef(win_model)[1])\npinede_alpha &lt;- as.numeric(coef(pinede_model)[1])\n\n# adding alphas to the dataset \nalphas &lt;- c(endive_alpha, well_alpha, win_alpha, pinede_alpha)\nCAPM_data$Alpha &lt;- alphas\n\n# adding the actual returns \nCAPM_data$Actual_Return &lt;- as.numeric(c(annualized_endive_ret, annualized_well_ret,annualized_win_ret, annualized_pinede_ret))\n\n\n\n\n\nFunds’ Alphas\n\n\nFund\nAlpha\n\n\n\n\nEndive\n-0.11\n\n\nWell\n-0.08\n\n\nWin\n-0.34\n\n\nPinede\n0.07\n\n\n\n\n\nThe alphas are different than 0 for all 4 Funds."
  },
  {
    "objectID": "Project.html#question-7",
    "href": "Project.html#question-7",
    "title": "STA 9713 Project",
    "section": "Question 7",
    "text": "Question 7\nThe code below plots the 4 Funds on the Security Market Line\n\n\nShow the code\nggplot()+\n  geom_line( data = CAPM_data, aes(x = Beta, y = Expected_Return),\n             color = \"blue\", linewidth = 1)+\n  geom_point(data = CAPM_data, aes(x = Beta, y = Actual_Return),\n             color = \"red\", size = 3)+\n  geom_text(data = CAPM_data, aes(x = Beta, y = Actual_Return, label = Fund),\n            vjust = -1)+\n  labs(\n    title = \"Security Market Line for 4 Funds\",\n    x = \"Beta\",\n    y = \"Returns\",\n    caption = \"Blue line = CAPM prediction | Red dots = Actual fund performance\"\n  )+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe blue line in th graph represents the CAPM Prediction, meaning the the expected return for the Funds, taking in consideration their correlation to the market(S&P 500). The red dots represent the actual annual return.\nIt is observed that:\n\nPinede outperformed the expected annual return of 15.22% with an actual annual average return of 16.88%.\nEndive unperformed the expected annual return of 16.19% with an actual annual average return of 14.61%.\nWell unperformed the expected annual return of 17.93% with an actual annual average return of 17.3%.\nWin unperformed the expected annual return of 20.66% with an actual annual average return of 17.25%."
  },
  {
    "objectID": "Project.html#question-8-well-vs-win",
    "href": "Project.html#question-8-well-vs-win",
    "title": "STA 9713 Project",
    "section": "Question 8 Well vs Win",
    "text": "Question 8 Well vs Win\nThe graphs below show the distribution of returns for Well and Win.\n\n\nShow the code\nggplot(data = data, aes(x = data$Well))+\n  geom_histogram(bins = 25, fill = \"steelblue\", alpha = 0.7, color = \"black\")+\n  theme_bw()+\n  labs( title = \"Well Distribution of Returns\",\n        x = \"Monthly Returns\",\n        y = \"%\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data = data, aes(x = data$Win))+\n  geom_histogram(bins = 25, fill = \"steelblue\", alpha = 0.7, color = \"black\")+\n  theme_bw()+\n  labs( title = \"Win Distribution of Returns\",\n        x = \"Monthly Returns\",\n        y = \"%\")\n\n\n\n\n\n\n\n\n\n\nComparing Mean returns of Well and Win\nThe code below calculates the t-test for Well and Win\n\n\nShow the code\n# welch test\nwelch_test &lt;- t.test(data$Well, data$Win)\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  data$Well and data$Win\nt = 0.0038286, df = 191.94, p-value = 0.9969\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.757726  1.764564\nsample estimates:\nmean of x mean of y \n 1.338512  1.335094 \n\n\nThe p-value of 0.9969, fail to reject the null hypothesis H0. This indicates that the average returns for Well and Win are not statistically different."
  },
  {
    "objectID": "Project.html#question-9",
    "href": "Project.html#question-9",
    "title": "STA 9713 Project",
    "section": "Question 9",
    "text": "Question 9\nThe code below calculates the number of months that have positive return for Endive and Well.\n\n\nShow the code\nn_endive&lt;- length(data$Endive)\nn_well&lt;- length(data$Well)\n\nendive_positive &lt;- length(which(data$Endive &gt; 0 )) \nwell_positive &lt;- length(which(data$Well &gt; 0 ))\n\n\nEndive has 72 positive months out of 110 total months.\nWell has 74 positive months out of 110 total months.\nWell has the better batting average.\n\n* Prepared for STA 9713 at Baruch College - Giulio Orazzo"
  }
]